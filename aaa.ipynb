{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Cycle Labeling for 5-Minute K-Line Data\n",
    "\n",
    "## Based on Al Brooks Price Action Theory\n",
    "\n",
    "**Labels:**\n",
    "- `-1`: ä¸‹è·Œè¶‹åŠ¿ (Downtrend)\n",
    "- `0`: éœ‡è¡/äº¤æ˜“åŒºé—´ (Trading Range / Breakout Mode)\n",
    "- `+1`: ä¸Šæ¶¨è¶‹åŠ¿ (Uptrend)\n",
    "\n",
    "**Core Principles:**\n",
    "1. åªå…³å¿ƒæœ¬å‘¨æœŸå±€éƒ¨ç»“æ„ï¼Œåˆ©ç”¨å°‘é‡æœªæ¥ä¿¡æ¯ï¼ˆtriple barrier + æœªæ¥çª—å£ï¼‰æ¥æ ‡æ³¨\n",
    "2. å……åˆ†åˆ©ç”¨ KAMAï¼ˆå«ERæ•ˆç‡å› å­ï¼‰æ¥æµ‹\"è¶‹åŠ¿ vs éœ‡è¡\"\n",
    "3. ç»“åˆ Al Brooks çš„20æ ¹baräº¤æ˜“åŒºé—´ä¸­æ€§åŒ–è§„åˆ™\n",
    "4. ç”¨ triple barrier åš\"æ–¹å‘ç¡®è®¤\"\n",
    "\n",
    "**Pipeline Layers:**\n",
    "1. æ•°æ®é¢„å¤„ç†å±‚\n",
    "2. è¾…åŠ©ç‰¹å¾å±‚ (KAMA, ER, çº¿æ€§å›å½’, éœ‡è¡åº¦)\n",
    "3. Triple Barrier å±‚\n",
    "4. å¸‚åœºå‘¨æœŸæ ‡ç­¾å±‚ + å¹³æ»‘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 1: ç¯å¢ƒé…ç½® & ä¸­æ–‡å­—ä½“è®¾ç½®ï¼ˆmac ä¸“ç”¨ + æ˜¾å¼ FontPropertiesï¼‰ ==========\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ========== Matplotlib & å­—ä½“ ==========\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager as fm\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# å…¨å±€å˜é‡ï¼šæ˜¾å¼ä¸­æ–‡å­—ä½“å±æ€§ï¼ˆåé¢ç”»å›¾æ—¶ç›´æ¥ç”¨ï¼‰\n",
    "CHINESE_FONT_PROP = None\n",
    "\n",
    "def setup_chinese_font_mac():\n",
    "    \"\"\"\n",
    "    åœ¨ macOS ä¸Šè®¾ç½®ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“ï¼š\n",
    "    1ï¼‰ä¼˜å…ˆä»å¸¸è§è·¯å¾„åŠ è½½ .ttc æ–‡ä»¶ï¼Œç”Ÿæˆ FontProperties\n",
    "    2ï¼‰å…¶æ¬¡ä»ç³»ç»Ÿå­—ä½“åå­—ä¸­é€‰æ‹©\n",
    "    æœ€åæŠŠ FontProperties å­˜åˆ° CHINESE_FONT_PROPï¼Œæ–¹ä¾¿åé¢æ˜¾å¼ä½¿ç”¨\n",
    "    \"\"\"\n",
    "    global CHINESE_FONT_PROP\n",
    "\n",
    "    # 1) ä¼˜å…ˆå°è¯•å…·ä½“å­—ä½“æ–‡ä»¶è·¯å¾„\n",
    "    candidate_paths = [\n",
    "        \"/System/Library/Fonts/PingFang.ttc\",\n",
    "        \"/System/Library/Fonts/STHeiti Light.ttc\",\n",
    "        \"/System/Library/Fonts/STHeiti Medium.ttc\",\n",
    "        \"/System/Library/Fonts/STSong.ttc\",\n",
    "    ]\n",
    "\n",
    "    for p in candidate_paths:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                fm.fontManager.addfont(p)\n",
    "                CHINESE_FONT_PROP = fm.FontProperties(fname=p)\n",
    "                name = CHINESE_FONT_PROP.get_name()\n",
    "                print(f\"âœ… é€šè¿‡è·¯å¾„åŠ è½½ä¸­æ–‡å­—ä½“æ–‡ä»¶: {p}\")\n",
    "                print(f\"   FontProperties åç§°: {name}\")\n",
    "\n",
    "                # åŒæ—¶æŠŠå®ƒè®¾è¿› rcParamsï¼ˆå³ä½¿ä¸ç”Ÿæ•ˆï¼Œåé¢è¿˜æœ‰ FontProperties å…œåº•ï¼‰\n",
    "                mpl.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "                mpl.rcParams[\"font.sans-serif\"] = [name]\n",
    "                mpl.rcParams[\"axes.unicode_minus\"] = False\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ å°è¯•åŠ è½½å­—ä½“å¤±è´¥: {p} -> {e}\")\n",
    "\n",
    "    # 2) å¦‚æœæ–‡ä»¶è·¯å¾„å¤±è´¥ï¼Œå†ç”¨å­—ä½“ååŒ¹é…\n",
    "    if CHINESE_FONT_PROP is None:\n",
    "        available = {f.name for f in fm.fontManager.ttflist}\n",
    "        candidate_names = [\n",
    "            \"PingFang SC\",\n",
    "            \"Hiragino Sans GB\",\n",
    "            \"Songti SC\",\n",
    "            \"STSong\",\n",
    "            \"STHeiti\",\n",
    "            \"Heiti TC\",\n",
    "        ]\n",
    "        chosen = None\n",
    "        for name in candidate_names:\n",
    "            if name in available:\n",
    "                chosen = name\n",
    "                break\n",
    "\n",
    "        if chosen is not None:\n",
    "            CHINESE_FONT_PROP = fm.FontProperties(family=chosen)\n",
    "            mpl.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "            mpl.rcParams[\"font.sans-serif\"] = [chosen]\n",
    "            mpl.rcParams[\"axes.unicode_minus\"] = False\n",
    "            print(f\"âœ… åœ¨ç³»ç»Ÿå­—ä½“ä¸­æ‰¾åˆ°ä¸­æ–‡å­—ä½“: {chosen}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“å€™é€‰ï¼Œå¯èƒ½ä»ç„¶å‡ºç°ä¹±ç \")\n",
    "\n",
    "    print(\"\\nå½“å‰ Matplotlib å­—ä½“é…ç½®ï¼š\")\n",
    "    print(\"  font.family     :\", mpl.rcParams[\"font.family\"])\n",
    "    print(\"  font.sans-serif :\", mpl.rcParams[\"font.sans-serif\"])\n",
    "    print(\"  axes.unicode_minus:\", mpl.rcParams[\"axes.unicode_minus\"])\n",
    "\n",
    "    if CHINESE_FONT_PROP is None:\n",
    "        print(\"âš ï¸ CHINESE_FONT_PROP ä¸ºç©ºï¼Œåç»­ç”»å›¾å¯èƒ½æ— æ³•æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡\")\n",
    "    else:\n",
    "        print(\"âœ… CHINESE_FONT_PROP å·²å°±ç»ªï¼Œå¯ç”¨äº title/xlabel/ylabel çš„ fontproperties å‚æ•°\")\n",
    "\n",
    "    return CHINESE_FONT_PROP\n",
    "\n",
    "CHINESE_FONT_PROP = setup_chinese_font_mac()\n",
    "\n",
    "# ç”»å›¾é£æ ¼\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ========== Bokeh é…ç½® ==========\n",
    "from bokeh.plotting import figure, show, output_notebook, output_file\n",
    "from bokeh.models import ColumnDataSource, BoxAnnotation, HoverTool, Range1d, Span, Label\n",
    "from bokeh.layouts import column\n",
    "from bokeh.io import save\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# ========== è·¯å¾„é…ç½® ==========\n",
    "DATA_DIR_FUTURE = Path(\"data/Future/rth\")\n",
    "\n",
    "# ========== è¾“å‡ºç›®å½•é…ç½® ==========\n",
    "OUTPUT_DIR = Path(\"market_cycle\")\n",
    "OUTPUT_DIR_DATA = OUTPUT_DIR / \"data\"       # æ ‡æ³¨æ•°æ®ã€è®­ç»ƒæ•°æ®\n",
    "OUTPUT_DIR_CHARTS = OUTPUT_DIR / \"charts\"   # Bokeh å›¾è¡¨\n",
    "OUTPUT_DIR_FEATURES = OUTPUT_DIR / \"features\"  # ç‰¹å¾çŸ©é˜µ\n",
    "\n",
    "for d in [OUTPUT_DIR_DATA, OUTPUT_DIR_CHARTS, OUTPUT_DIR_FEATURES]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nDATA_DIR_FUTURE: {DATA_DIR_FUTURE.resolve()}\")\n",
    "print(f\"Kçº¿æ•°æ®ç›®å½•å­˜åœ¨: {DATA_DIR_FUTURE.exists()}\")\n",
    "print(\"\\nè¾“å‡ºç›®å½•:\")\n",
    "print(f\"  æ•°æ® : {OUTPUT_DIR_DATA.resolve()}\")\n",
    "print(f\"  å›¾è¡¨ : {OUTPUT_DIR_CHARTS.resolve()}\")\n",
    "print(f\"  ç‰¹å¾ : {OUTPUT_DIR_FEATURES.resolve()}\")\n",
    "\n",
    "print(\"\\nCell 1 å®Œæˆ: åº“å¯¼å…¥ & ä¸­æ–‡å­—ä½“é…ç½®ï¼ˆå« FontPropertiesï¼‰âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 2: æµ‹è¯•ä¸­æ–‡ç»˜å›¾ï¼ˆæ˜¾å¼ FontPropertiesï¼‰ ==========\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "print(\"å½“å‰ Matplotlib å­—ä½“ç›¸å…³é…ç½®ï¼š\")\n",
    "print(\"  font.family     :\", mpl.rcParams[\"font.family\"])\n",
    "print(\"  font.sans-serif :\", mpl.rcParams[\"font.sans-serif\"])\n",
    "print(\"  axes.unicode_minus:\", mpl.rcParams[\"axes.unicode_minus\"])\n",
    "\n",
    "from matplotlib import font_manager as fm\n",
    "\n",
    "print(\"\\nCHINESE_FONT_PROP =\", CHINESE_FONT_PROP)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.title(\"æµ‹è¯•æ ‡é¢˜ï¼šä¸­æ–‡æ˜¯å¦æ­£å¸¸æ˜¾ç¤ºï¼Ÿ\", fontproperties=CHINESE_FONT_PROP)\n",
    "plt.xlabel(\"æ¨ªè½´ï¼šæ•°é‡\", fontproperties=CHINESE_FONT_PROP)\n",
    "plt.ylabel(\"çºµè½´ï¼šæŒ‡æ ‡\", fontproperties=CHINESE_FONT_PROP)\n",
    "plt.plot([1, 2, 3])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCell 2 å®Œæˆï¼šä¸­æ–‡å­—ä½“æµ‹è¯• âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 2: å…¨å±€è¶…å‚æ•°é…ç½® ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ å‚æ•°è°ƒä¼˜æŒ‡å—:\n",
    "   - æƒ³è¦æ£€æµ‹åˆ°æ›´å¤šè¶‹åŠ¿ â†’ é™ä½ THR_ER_LOW, THR_CHOP_HIGH, æé«˜ THR_ER_HIGH\n",
    "   - æƒ³è¦æ›´ä¸¥æ ¼çš„è¶‹åŠ¿åˆ¤å®š â†’ æé«˜ THR_ER_HIGH, é™ä½ THR_CHOP_LOW\n",
    "   - Al Brooks 80/20 è§„åˆ™: çº¦80%æ—¶é—´åœ¨éœ‡è¡åŒºé—´ï¼Œ20%æ—¶é—´åœ¨è¶‹åŠ¿ä¸­\n",
    "   \n",
    "âš¡ å¿«é€Ÿè°ƒå‚: åªä¿®æ”¹è¿™ä¸ª Cellï¼Œç„¶åé‡æ–°è¿è¡Œåç»­æ‰€æœ‰ Cell å³å¯\n",
    "\"\"\"\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Š åŸºç¡€è®¡ç®—å‚æ•°\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "N_ATR = 20          # ATR è®¡ç®—å‘¨æœŸ (5min Kçº¿, 20æ ¹â‰ˆ100åˆ†é’Ÿ)\n",
    "                    #   â†“ å‡å°: ATR æ›´æ•æ„Ÿï¼Œæ³¢åŠ¨ä¼°è®¡æ›´åŠæ—¶ä½†æ›´noisy\n",
    "                    #   â†‘ å¢å¤§: ATR æ›´ç¨³å®šï¼Œä½†å¯¹çªå‘æ³¢åŠ¨ååº”æ…¢\n",
    "\n",
    "N_ER = 20           # ER æ•ˆç‡å› å­çª—å£ (ä¸ Brooks 20-bar è§„åˆ™ä¸€è‡´)\n",
    "                    #   â†“ å‡å°: æ›´å¿«æ£€æµ‹åˆ°è¶‹åŠ¿å¼€å§‹ï¼Œä½†å¯èƒ½äº§ç”Ÿæ›´å¤šå‡ä¿¡å·\n",
    "                    #   â†‘ å¢å¤§: è¶‹åŠ¿åˆ¤æ–­æ›´å¯é ï¼Œä½†ä¼šé”™è¿‡è¶‹åŠ¿åˆæœŸ\n",
    "\n",
    "L_BACK = 20         # å›çœ‹çª—å£ (ç”¨äºçº¿æ€§å›å½’ã€éœ‡è¡åº¦ç­‰)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“ˆ KAMA (Kaufman Adaptive Moving Average) å‚æ•°\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "KAMA_N_ER = 10      # KAMA å†…éƒ¨çš„ ER è®¡ç®—çª—å£\n",
    "                    #   â†“ å‡å°: KAMA å¯¹è¶‹åŠ¿å˜åŒ–æ›´æ•æ„Ÿ\n",
    "                    #   â†‘ å¢å¤§: KAMA æ›´å¹³æ»‘\n",
    "\n",
    "KAMA_FAST = 2       # å¿«é€Ÿ EMA å‘¨æœŸ (è¶‹åŠ¿å¼ºæ—¶ä½¿ç”¨)\n",
    "KAMA_SLOW = 30      # æ…¢é€Ÿ EMA å‘¨æœŸ (éœ‡è¡æ—¶ä½¿ç”¨)\n",
    "KAMA_SLOPE_LAG = 5  # KAMA æ–œç‡è®¡ç®—æ»å (ç”¨å‡ æ ¹barè®¡ç®—æ–œç‡)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ¯ Triple Barrier å‚æ•° (æ–¹å‘ç¡®è®¤) æœªæ¥ä¿¡æ¯ç‰¹å¾æ ‡ç­¾æœºåˆ¶\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "T_VERT = 12         # å‚ç›´æ—¶é—´éšœç¢ (12æ ¹5minâ‰ˆ1å°æ—¶)\n",
    "                    #   â†“ å‡å°: åªçœ‹çŸ­æœŸæ–¹å‘ï¼Œæ›´é€‚åˆçŸ­çº¿\n",
    "                    #   â†‘ å¢å¤§: çœ‹æ›´é•¿æœŸæ–¹å‘ï¼Œè¶‹åŠ¿åˆ¤æ–­æ›´ç¨³å¥\n",
    "\n",
    "PT_MULT = 1.5       # æ­¢ç›ˆå€æ•° (ç›¸å¯¹ATR)\n",
    "                    #   â†“ å‡å°: æ›´å®¹æ˜“è§¦å‘æ–¹å‘æ ‡ç­¾ï¼Œæ£€æµ‹åˆ°æ›´å¤šè¶‹åŠ¿\n",
    "                    #   â†‘ å¢å¤§: åªæœ‰å¼ºåŠ¿è¡Œæƒ…æ‰ç»™æ–¹å‘æ ‡ç­¾\n",
    "\n",
    "SL_MULT = 1.5       # æ­¢æŸå€æ•° (ç›¸å¯¹ATR)ï¼Œé€šå¸¸ä¸ PT_MULT ç›¸åŒ\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ”´ éœ‡è¡åŒºé—´åˆ¤å®šå‚æ•° (ä¸¥æ ¼æ¡ä»¶ï¼Œéœ€åŒæ—¶æ»¡è¶³å¤šä¸ª)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "L_RANGE = 10        # åŒºé—´çª—å£ (å‰åå„å¤šå°‘æ ¹bar)\n",
    "L_RANGE_FWD = 10    # æœªæ¥çª—å£ (ç”¨äºåˆ¤æ–­æ˜¯å¦ä»åœ¨åŒºé—´) æœªæ¥ä¿¡æ¯\n",
    "\n",
    "THR_ER_LOW = 0.25   # ã€å…³é”®ã€‘ER < æ­¤å€¼è§†ä¸ºéœ‡è¡ ä½ERè§†ä¸ºéœ‡è¡\n",
    "                    #   â†“ å‡å°(å¦‚0.2): æ›´ä¸¥æ ¼ï¼Œåªæœ‰æåº¦éœ‡è¡æ‰æ ‡ä¸ºåŒºé—´ â†’ è¶‹åŠ¿æ¯”ä¾‹â†‘\n",
    "                    #   â†‘ å¢å¤§(å¦‚0.4): æ›´å®½æ¾ï¼Œæ›´å¤šbaræ ‡ä¸ºéœ‡è¡åŒºé—´ â†’ éœ‡è¡æ¯”ä¾‹â†‘\n",
    "\n",
    "THR_CHOP_HIGH = 0.5  # ã€å…³é”®ã€‘chop > æ­¤å€¼è§†ä¸ºéœ‡è¡ é«˜chopè§†ä¸ºéœ‡è¡\n",
    "                    \n",
    "\n",
    "THR_RANGE_ATR = 3.5 # range/ATR < æ­¤å€¼è§†ä¸ºçª„åŒºé—´\n",
    "                    #   â†“ å‡å°: éœ€è¦æ›´çª„çš„åŒºé—´æ‰ç®—éœ‡è¡\n",
    "                    #   â†‘ å¢å¤§: æ›´å®½çš„åŒºé—´ä¹Ÿå¯ä»¥ç®—éœ‡è¡\n",
    "\n",
    "THR_R2_LOW = 0.25   # RÂ² < æ­¤å€¼è§†ä¸ºæ— æ˜æ˜¾çº¿æ€§è¶‹åŠ¿ çº¿å‹å›å½’æ‹Ÿåˆåº¦ä½\n",
    "                    #   â†“ å‡å°: æ›´å®½æ¾ï¼Œå…è®¸ç¨æœ‰è¶‹åŠ¿æ€§çš„åŒºé—´\n",
    "                    #   â†‘ å¢å¤§: æ›´ä¸¥æ ¼\n",
    "\n",
    "THR_FUTURE_MOVE = 1.2  # æœªæ¥ç§»åŠ¨ < æ­¤å€¼*ATR è§†ä¸ºä»åœ¨åŒºé—´ æœªæ¥ä¿¡æ¯\n",
    "                    #   â†“ å‡å°: æ›´ä¸¥æ ¼ï¼Œéœ€è¦ä»·æ ¼å‡ ä¹ä¸åŠ¨\n",
    "                    #   â†‘ å¢å¤§: å…è®¸æ›´å¤§çš„æœªæ¥ç§»åŠ¨ä»ç®—åŒºé—´\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸŸ¢ è¶‹åŠ¿åˆ¤å®šå‚æ•° (å¯¹ééœ‡è¡åŒºé—´çš„barè¿›è¡Œè¶‹åŠ¿åˆ¤å®š)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "THR_ER_HIGH = 0.35  # ã€å…³é”®ã€‘ER > æ­¤å€¼å¼€å§‹è€ƒè™‘è¶‹åŠ¿\n",
    "                    #   â†“ å‡å°(å¦‚0.3): æ›´å®¹æ˜“è¢«åˆ¤å®šä¸ºè¶‹åŠ¿ â†’ è¶‹åŠ¿æ¯”ä¾‹â†‘\n",
    "                    #   â†‘ å¢å¤§(å¦‚0.5): éœ€è¦æ›´å¼ºçš„æ•ˆç‡æ‰ç»™è¶‹åŠ¿æ ‡ç­¾ â†’ è¶‹åŠ¿æ¯”ä¾‹â†“\n",
    "\n",
    "THR_CHOP_LOW = 0.33 # ã€å…³é”®ã€‘chop < æ­¤å€¼å¼€å§‹è€ƒè™‘è¶‹åŠ¿\n",
    "              \n",
    "\n",
    "THR_SLOPE_NORM = 0.015  # KAMAæ–œç‡æ ‡å‡†åŒ–é˜ˆå€¼ (æ¯barç§»åŠ¨å¤šå°‘ATR)\n",
    "                    #   â†“ å‡å°: æ›´å®¹æ˜“æ»¡è¶³æ–œç‡æ¡ä»¶ â†’ è¶‹åŠ¿æ¯”ä¾‹â†‘\n",
    "                    #   â†‘ å¢å¤§: éœ€è¦æ›´é™¡çš„æ–œç‡æ‰ç®—è¶‹åŠ¿\n",
    "\n",
    "THR_DIST_NORM = 0.4 # ä»·æ ¼åç¦»KAMAçš„æ ‡å‡†åŒ–é˜ˆå€¼\n",
    "                    #   â†“ å‡å°: æ›´å®¹æ˜“æ»¡è¶³åç¦»æ¡ä»¶ â†’ è¶‹åŠ¿æ¯”ä¾‹â†‘\n",
    "                    #   â†‘ å¢å¤§: éœ€è¦æ›´å¤§çš„åç¦»æ‰ç®—è¶‹åŠ¿\n",
    "\n",
    "L_FWD = 12          # è¶‹åŠ¿ç¡®è®¤çš„æœªæ¥çª—å£ (çº¦1å°æ—¶)\n",
    "                    #   ç”¨äºéªŒè¯å½“å‰æ–œç‡æ–¹å‘ä¸æœªæ¥èµ°åŠ¿æ˜¯å¦ä¸€è‡´\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ”§ æ ‡ç­¾å¹³æ»‘å‚æ•°\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "MIN_TREND_LEN = 4   # æœ€å°è¶‹åŠ¿æ®µé•¿åº¦ (å¤ªçŸ­çš„è¶‹åŠ¿æ®µæ ‡ä¸º0)\n",
    "                    #   â†“ å‡å°: ä¿ç•™æ›´çŸ­çš„è¶‹åŠ¿æ®µ â†’ è¶‹åŠ¿æ¯”ä¾‹â†‘\n",
    "                    #   â†‘ å¢å¤§: è¿‡æ»¤æ‰çŸ­è¶‹åŠ¿ â†’ éœ‡è¡æ¯”ä¾‹â†‘\n",
    "\n",
    "MIN_RANGE_LEN = 15  # æœ€å°éœ‡è¡åŒºé—´é•¿åº¦ (BrooksåŸå§‹ä¸º20)\n",
    "                    #   â†“ å‡å°: æ›´çŸ­çš„éœ‡è¡ä¹Ÿä¿ç•™ â†’ éœ‡è¡æ¯”ä¾‹â†‘\n",
    "                    #   â†‘ å¢å¤§: åªä¿ç•™é•¿éœ‡è¡ â†’ è¶‹åŠ¿æ¯”ä¾‹â†‘\n",
    "\n",
    "SMOOTH_WINDOW = 5   # å¹³æ»‘çª—å£ (å¤šæ•°æŠ•ç¥¨)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸŒŠ å¾®é€šé“ & å¸‚åœºçŠ¶æ€å¢å¼ºå‚æ•°ï¼ˆ10+2 æŠ½è±¡ç›¸å…³ï¼‰\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# â€”â€” å•ä¸ªå¾®é€šé“ç»“æ„å‚æ•° â€”â€”\n",
    "MICRO_WIN = 3          # å¾®é€šé“é•¿åº¦ï¼ˆ3æ ¹Kçº¿ï¼‰\n",
    "MICRO_TICK_SIZE = 0.25 # åˆçº¦æœ€å°è·³åŠ¨å•ä½ï¼Œç”¨äºåˆ¤æ–­ç¼ºå£/å…¥åœºä»·\n",
    "\n",
    "# â€”â€” å¾®é€šé“è´¨é‡ / æœªæ¥å¯äº¤æ˜“æ€§ç›¸å…³ â€”â€”\n",
    "MC_QUALITY_MIN = 0.7   # è®¤ä¸ºæ˜¯â€œé«˜è´¨é‡å¾®é€šé“â€çš„è´¨é‡é˜ˆå€¼(0~1)\n",
    "MC_LOOKAHEAD = 10      # ç”¨äº market-state æ‰“æ ‡ç­¾æ—¶ï¼Œå‘å‰çœ‹å¤šå°‘æ ¹å†…æ˜¯å¦å‡ºç°é«˜è´¨é‡å¾®é€šé“\n",
    "MC_TRADE_HORIZON = 10  # æœ€æœ´ç´ å›æµ‹ä¸­ï¼Œè¿›åœºåæœ€å¤šè§‚å¯Ÿå¤šå°‘æ ¹barå†³å®šç›ˆäº\n",
    "MC_R_MULT = 1.0        # å¾®é€šé“æœ€æœ´ç´ æ­¢ç›ˆRå€æ•°ï¼ˆ1R=å…¥åœºåˆ°æ­¢æŸçš„è·ç¦»ï¼‰\n",
    "\n",
    "# â€”â€” è¿‡å»çª—å£çš„å¸‚åœºçŠ¶æ€ç»Ÿè®¡ï¼ˆé˜´é˜³æ¯”ä¾‹ã€run-lengthã€dom-score ç­‰ï¼‰ â€”â€”\n",
    "STATE_WIN_SHORT = 5    # çŸ­çª—å£ï¼ˆè¿‘æœŸ 5 æ ¹ï¼‰\n",
    "STATE_WIN_LONG = 10    # é•¿çª—å£ï¼ˆè¿‘æœŸ 10 æ ¹ï¼‰\n",
    "\n",
    "# â€”â€” æ³¢åŠ¨å‹ç¼© / Hurst æŒ‡æ ‡çª—å£ â€”â€”\n",
    "VOL_SHORT_WIN = 5      # çŸ­æœŸ realized vol / ATR è®¡ç®—çª—å£\n",
    "VOL_LONG_WIN = 20      # é•¿æœŸ realized vol / ATR è®¡ç®—çª—å£\n",
    "HURST_WINDOW = 50      # Hurst æŒ‡æ•°æ»šåŠ¨çª—å£ï¼ˆè¿‘ 50 æ ¹ 5minï¼‰\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“‹ æ‰“å°å½“å‰é…ç½®\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“Š å½“å‰å‚æ•°é…ç½®\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nã€åŸºç¡€å‚æ•°ã€‘\")\n",
    "print(f\"  ATRå‘¨æœŸ: {N_ATR}, ERçª—å£: {N_ER}, å›çœ‹çª—å£: {L_BACK}\")\n",
    "print(f\"\\nã€KAMAå‚æ•°ã€‘\")\n",
    "print(f\"  n_er={KAMA_N_ER}, fast={KAMA_FAST}, slow={KAMA_SLOW}, slope_lag={KAMA_SLOPE_LAG}\")\n",
    "print(f\"\\nã€Triple Barrierã€‘\")\n",
    "print(f\"  T_vert={T_VERT} (~{T_VERT*5}åˆ†é’Ÿ), pt_mult={PT_MULT}, sl_mult={SL_MULT}\")\n",
    "print(f\"\\nã€éœ‡è¡åŒºé—´åˆ¤å®šã€‘ (éœ€åŒæ—¶æ»¡è¶³)\")\n",
    "print(f\"  ER < {THR_ER_LOW}, chop > {THR_CHOP_HIGH}, range_atr < {THR_RANGE_ATR}\")\n",
    "print(f\"  RÂ² < {THR_R2_LOW}, future_move < {THR_FUTURE_MOVE}*ATR\")\n",
    "print(f\"\\nã€è¶‹åŠ¿åˆ¤å®šã€‘ (æ»¡è¶³éƒ¨åˆ†æ¡ä»¶)\")\n",
    "print(f\"  ER > {THR_ER_HIGH} æˆ– chop < {THR_CHOP_LOW}\")\n",
    "print(f\"  slope_norm > {THR_SLOPE_NORM}, dist_norm > {THR_DIST_NORM}\")\n",
    "print(f\"\\nã€æ ‡ç­¾å¹³æ»‘ã€‘\")\n",
    "print(f\"  æœ€å°è¶‹åŠ¿é•¿åº¦: {MIN_TREND_LEN}, æœ€å°éœ‡è¡é•¿åº¦: {MIN_RANGE_LEN}, å¹³æ»‘çª—å£: {SMOOTH_WINDOW}\")\n",
    "\n",
    "print(f\"\\nã€å¾®é€šé“ç»“æ„ã€‘\")\n",
    "print(f\"  MICRO_WIN={MICRO_WIN}, tick_size={MICRO_TICK_SIZE}\")\n",
    "print(f\"\\nã€å¾®é€šé“è´¨é‡ & æœªæ¥å¯äº¤æ˜“æ€§ã€‘\")\n",
    "print(f\"  è´¨é‡é˜ˆå€¼: MC_QUALITY_MIN={MC_QUALITY_MIN}\")\n",
    "print(f\"  æœªæ¥æŸ¥æ‰¾çª—å£: MC_LOOKAHEAD={MC_LOOKAHEAD} æ ¹Kçº¿\")\n",
    "print(f\"  äº¤æ˜“å›æµ‹çª—å£: MC_TRADE_HORIZON={MC_TRADE_HORIZON} æ ¹Kçº¿, Rå€æ•°: MC_R_MULT={MC_R_MULT}\")\n",
    "print(f\"\\nã€è¿‡å»çª—å£å¸‚åœºçŠ¶æ€ç»Ÿè®¡ã€‘\")\n",
    "print(f\"  STATE_WIN_SHORT={STATE_WIN_SHORT}, STATE_WIN_LONG={STATE_WIN_LONG}\")\n",
    "print(f\"\\nã€æ³¢åŠ¨å‹ç¼© / Hurstã€‘\")\n",
    "print(f\"  VOL_SHORT_WIN={VOL_SHORT_WIN}, VOL_LONG_WIN={VOL_LONG_WIN}, HURST_WINDOW={HURST_WINDOW}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nâœ… Cell 2 å®Œæˆ: å‚æ•°é…ç½® (ä¿®æ”¹åé‡æ–°è¿è¡Œåç»­Cellå³å¯)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 3: åŠ è½½å¹¶åˆå¹¶ 5min Kçº¿æ•°æ® ==========\n",
    "\n",
    "# è·å–æ‰€æœ‰ ES 5min Kçº¿æ–‡ä»¶\n",
    "five_min_files = sorted(DATA_DIR_FUTURE.glob(\"ES_5min_rth_*.csv\"))\n",
    "print(f\"æ‰¾åˆ°çš„ 5min Kçº¿æ–‡ä»¶æ•°é‡: {len(five_min_files)}\")\n",
    "\n",
    "if not five_min_files:\n",
    "    raise FileNotFoundError(f\"åœ¨ç›®å½• {DATA_DIR_FUTURE} ä¸‹æœªæ‰¾åˆ° ES_5min_rth_*.csv æ–‡ä»¶\")\n",
    "\n",
    "dfs = []\n",
    "for f in five_min_files:\n",
    "    print(f\"è¯»å–: {f.name}\")\n",
    "    df_year = pd.read_csv(f)\n",
    "    \n",
    "    # æ£€æŸ¥å¿…è¦åˆ—\n",
    "    required_cols = {\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"}\n",
    "    missing_cols = required_cols - set(df_year.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"æ–‡ä»¶ {f.name} ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}\")\n",
    "    \n",
    "    # è½¬æ¢æ—¶é—´æˆ³\n",
    "    df_year[\"timestamp\"] = pd.to_datetime(df_year[\"timestamp\"], errors=\"coerce\")\n",
    "    df_year = df_year.sort_values(\"timestamp\")\n",
    "    dfs.append(df_year)\n",
    "\n",
    "# åˆå¹¶æ‰€æœ‰å¹´ä»½\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "df = df.set_index(\"timestamp\")\n",
    "\n",
    "# ç¡®ä¿æ•°å€¼åˆ—ç±»å‹æ­£ç¡®\n",
    "for col in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
    "    df[col] = df[col].astype(float)\n",
    "\n",
    "print(f\"\\n=== df åŸºæœ¬ä¿¡æ¯ ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"æ—¶é—´èŒƒå›´: {df.index.min()} ~ {df.index.max()}\")\n",
    "print(f\"é‡å¤æ—¶é—´æˆ³æ•°é‡: {df.index.duplicated().sum()}\")\n",
    "\n",
    "# æ£€æŸ¥ç¼ºå¤±å€¼\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(f\"\\nå­˜åœ¨ç¼ºå¤±å€¼:\\n{missing[missing > 0]}\")\n",
    "else:\n",
    "    print(\"\\næ— ç¼ºå¤±å€¼\")\n",
    "\n",
    "display(df.head())\n",
    "print(\"\\nCell 3 å®Œæˆ: æ•°æ®åŠ è½½å®Œæ¯• âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 4: æ•°æ®é¢„å¤„ç† - åŸºç¡€åºåˆ—è®¡ç®— ==========\n",
    "\n",
    "df = df.copy()\n",
    "\n",
    "# 1. å¯¹æ•°æ”¶ç›Š (log return)\n",
    "# r_t = ln(C_t) - ln(C_{t-1})\n",
    "df[\"log_close\"] = np.log(df[\"close\"])\n",
    "df[\"log_return\"] = df[\"log_close\"].diff()\n",
    "\n",
    "# 2. çœŸå®æ³¢åŠ¨èŒƒå›´ (True Range) å’Œ ATR\n",
    "# TR_t = max(H_t - L_t, |H_t - C_{t-1}|, |L_t - C_{t-1}|)\n",
    "prev_close = df[\"close\"].shift(1)\n",
    "tr1 = df[\"high\"] - df[\"low\"]\n",
    "tr2 = np.abs(df[\"high\"] - prev_close)\n",
    "tr3 = np.abs(df[\"low\"] - prev_close)\n",
    "df[\"tr\"] = np.maximum(np.maximum(tr1, tr2), tr3)\n",
    "\n",
    "# ATR ä½¿ç”¨ EMA\n",
    "df[\"atr\"] = df[\"tr\"].ewm(span=N_ATR, adjust=False).mean()\n",
    "\n",
    "# 3. æ»šåŠ¨é«˜ä½ä»·åŒºé—´\n",
    "df[\"roll_max\"] = df[\"close\"].rolling(window=L_BACK).max()\n",
    "df[\"roll_min\"] = df[\"close\"].rolling(window=L_BACK).min()\n",
    "df[\"roll_range\"] = df[\"roll_max\"] - df[\"roll_min\"] # é«˜ä»·å‡ä½ä»·\n",
    "\n",
    "# 4. ç›¸å¯¹åŒºé—´ (range / ATR)\n",
    "df[\"range_atr\"] = df[\"roll_range\"] / df[\"atr\"].replace(0, np.nan)\n",
    "\n",
    "print(\"=== åŸºç¡€åºåˆ—è®¡ç®—å®Œæˆ ===\")\n",
    "print(f\"log_return: mean={df['log_return'].mean():.6f}, std={df['log_return'].std():.6f}\")\n",
    "print(f\"ATR: mean={df['atr'].mean():.4f}, std={df['atr'].std():.4f}\")\n",
    "print(f\"range_atr: mean={df['range_atr'].mean():.4f}, std={df['range_atr'].std():.4f}\")\n",
    "\n",
    "display(df[[\"open\", \"high\", \"low\", \"close\", \"log_return\", \"tr\", \"atr\", \"range_atr\"]].tail(10))\n",
    "print(\"\\nCell 4 å®Œæˆ âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 1.5: è¦†ç›–æ‰æ‰€æœ‰å¯èƒ½æ®‹ç•™çš„ Windows å­—ä½“è®¾ç½® ==========\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ä½¿ç”¨åœ¨ Cell 1 ä¸­å·²ç»å‡†å¤‡å¥½çš„ CHINESE_FONT_PROP\n",
    "# å®ƒå¯¹åº”çš„å­—ä½“ååº”è¯¥æ˜¯ \"Heiti TC\"ï¼ˆä»ä½ åˆšæ‰çš„è¾“å‡ºçœ‹ï¼‰\n",
    "font_name = CHINESE_FONT_PROP.get_name()\n",
    "print(f\"CHINESE_FONT_PROP.get_name() = {font_name}\")\n",
    "\n",
    "# å½»åº•è¦†ç›–å…¨å±€å­—ä½“é…ç½®ï¼ˆæŠŠä¹‹å‰å¯èƒ½çš„ Microsoft YaHei é¡¶æ‰ï¼‰\n",
    "mpl.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "mpl.rcParams[\"font.sans-serif\"] = [font_name, \"Arial\", \"DejaVu Sans\", \"sans-serif\"]\n",
    "mpl.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "print(\"\\nâœ… å·²é‡ç½® Matplotlib å…¨å±€å­—ä½“é…ç½®:\")\n",
    "print(\"  font.family     :\", mpl.rcParams[\"font.family\"])\n",
    "print(\"  font.sans-serif :\", mpl.rcParams[\"font.sans-serif\"])\n",
    "print(\"  axes.unicode_minus:\", mpl.rcParams[\"axes.unicode_minus\"])\n",
    "\n",
    "# åšä¸€ä¸ªç®€å•æµ‹è¯•å›¾\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.title(\"æµ‹è¯•æ ‡é¢˜ï¼šä¸­æ–‡åº”è¯¥æ­£å¸¸æ˜¾ç¤º\", fontproperties=CHINESE_FONT_PROP)\n",
    "plt.xlabel(\"æ¨ªè½´ï¼šæ•°é‡\", fontproperties=CHINESE_FONT_PROP)\n",
    "plt.ylabel(\"çºµè½´ï¼šæŒ‡æ ‡\", fontproperties=CHINESE_FONT_PROP)\n",
    "plt.plot([1, 2, 3])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCell 1.5 å®Œæˆ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 5: KAMA ä¸ ER (æ•ˆç‡å› å­) è®¡ç®— ==========\n",
    "\n",
    "def calc_efficiency_ratio(close: pd.Series, n: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ•ˆç‡å› å­ ER (Efficiency Ratio)\n",
    "    \n",
    "    ER_t = |C_t - C_{t-n}| / sum(|C_i - C_{i-1}|, i=t-n+1 to t)\n",
    "    \n",
    "    ER â†’ 1: ä»·æ ¼å‡ ä¹\"èµ°ç›´çº¿\"ï¼Œè¶‹åŠ¿å¹²å‡€\n",
    "    ER â†’ 0: å¤§é‡æ¥å›éœ‡è¡ï¼Œå…¸å‹äº¤æ˜“åŒºé—´\n",
    "    \"\"\"\n",
    "    change = (close - close.shift(n)).abs()\n",
    "    volatility = close.diff().abs().rolling(window=n).sum()\n",
    "    er = change / volatility.replace(0, np.nan)\n",
    "    return er.clip(0, 1)\n",
    "# ç»™åé¢ KAMA ä¸€ä¸ªâ€œå½“å‰æ˜¯è¶‹åŠ¿è¿˜æ˜¯éœ‡è¡â€çš„é‡\n",
    "\n",
    "def calc_kama(close: pd.Series, n_er: int, fast: int, slow: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    è®¡ç®— KAMA (Kaufman Adaptive Moving Average)\n",
    "    \n",
    "    SC (Smoothing Constant) = [ER * (fast_sc - slow_sc) + slow_sc]^2\n",
    "    KAMA_t = KAMA_{t-1} + SC * (Close_t - KAMA_{t-1})\n",
    "    \"\"\"\n",
    "    # è®¡ç®— ER\n",
    "    er = calc_efficiency_ratio(close, n_er)\n",
    "    \n",
    "    # å¹³æ»‘å¸¸æ•°\n",
    "    fast_sc = 2 / (fast + 1)\n",
    "    slow_sc = 2 / (slow + 1)\n",
    "    sc = (er * (fast_sc - slow_sc) + slow_sc) ** 2\n",
    "    '''\n",
    "    è¶‹åŠ¿å¹²å‡€ï¼ˆERâ‰ˆ1ï¼‰â†’ SC æ¥è¿‘ fast_scÂ²ï¼ˆè·Ÿå¾—å¾ˆå¿«ï¼‰\n",
    "    éœ‡è¡ä¸¥é‡ï¼ˆERâ‰ˆ0ï¼‰â†’ SC æ¥è¿‘ slow_scÂ²ï¼ˆèµ°å¾—å¾ˆæ…¢ï¼Œå¾ˆé’ï¼‰\n",
    "    '''\n",
    "    \n",
    "    # åˆå§‹åŒ– KAMA\n",
    "    kama = np.zeros(len(close))\n",
    "    kama[:] = np.nan # å…ˆå…¨è®¾ä¸º NaN\n",
    "    \n",
    "    # æ‰¾ç¬¬ä¸€ä¸ªé NaN çš„ SC ä½ç½®\n",
    "    first_valid = sc.first_valid_index()\n",
    "    if first_valid is None:\n",
    "        return pd.Series(kama, index=close.index)\n",
    "    \n",
    "    first_idx = close.index.get_loc(first_valid)\n",
    "    \n",
    "    # ç”¨ SMA åˆå§‹åŒ–\n",
    "    kama[first_idx] = close.iloc[:first_idx + 1].mean()\n",
    "    \n",
    "    # é€’å½’è®¡ç®—\n",
    "    close_arr = close.values\n",
    "    sc_arr = sc.values\n",
    "    for i in range(first_idx + 1, len(close)):\n",
    "        if np.isnan(sc_arr[i]):\n",
    "            kama[i] = kama[i - 1]\n",
    "        else:\n",
    "            kama[i] = kama[i - 1] + sc_arr[i] * (close_arr[i] - kama[i - 1]) # åªçœ‹å½“å‰æ—¶åˆ»å’Œå‰ä¸€æ—¶åˆ»\n",
    "    \n",
    "    return pd.Series(kama, index=close.index)\n",
    "\n",
    "\n",
    "# è®¡ç®— ER (ä½¿ç”¨å…¨å±€ N_ER å‚æ•°)\n",
    "df[\"er\"] = calc_efficiency_ratio(df[\"close\"], N_ER)\n",
    "\n",
    "# è®¡ç®— KAMA\n",
    "df[\"kama\"] = calc_kama(df[\"close\"], KAMA_N_ER, KAMA_FAST, KAMA_SLOW) # kamaæ˜¯ä¸€æ¡å‡çº¿\n",
    "\n",
    "# è®¡ç®— KAMA æ–œç‡ (ä½¿ç”¨ KAMA_SLOPE_LAG)\n",
    "df[\"kama_slope\"] = (df[\"kama\"] - df[\"kama\"].shift(KAMA_SLOPE_LAG)) / KAMA_SLOPE_LAG\n",
    "\n",
    "# æ–œç‡æ ‡å‡†åŒ– (ç›¸å¯¹ ATR)\n",
    "# slope_norm_t = slope_t / ATR_t\n",
    "df[\"slope_norm\"] = df[\"kama_slope\"] / df[\"atr\"].replace(0, np.nan)\n",
    "\n",
    "# ä»·æ ¼åç¦»åº¦ (ç›¸å¯¹ ATR)\n",
    "# dist_norm_t = (C_t - KAMA_t) / ATR_t\n",
    "df[\"dist_norm\"] = (df[\"close\"] - df[\"kama\"]) / df[\"atr\"].replace(0, np.nan) # ä»·æ ¼åç¦»å‡çº¿çš„è·ç¦»\n",
    "\n",
    "print(\"=== KAMA & ER è®¡ç®—å®Œæˆ ===\")\n",
    "print(f\"ER: mean={df['er'].mean():.4f}, median={df['er'].median():.4f}\")\n",
    "print(f\"KAMAæ–œç‡æ ‡å‡†åŒ–: mean={df['slope_norm'].mean():.4f}, std={df['slope_norm'].std():.4f}\")\n",
    "print(f\"ä»·æ ¼åç¦»åº¦: mean={df['dist_norm'].mean():.4f}, std={df['dist_norm'].std():.4f}\")\n",
    "\n",
    "\n",
    "# ER åˆ†å¸ƒç›´æ–¹å›¾\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(df[\"er\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].axvline(x=THR_ER_LOW, color=\"red\", linestyle=\"--\", label=f\"éœ‡è¡é˜ˆå€¼: {THR_ER_LOW}\")\n",
    "axes[0].axvline(x=THR_ER_HIGH, color=\"green\", linestyle=\"--\", label=f\"è¶‹åŠ¿é˜ˆå€¼: {THR_ER_HIGH}\")\n",
    "axes[0].set_xlabel(\"ER (æ•ˆç‡å› å­)\", fontproperties=CHINESE_FONT_PROP)\n",
    "axes[0].set_ylabel(\"æ•°é‡\", fontproperties=CHINESE_FONT_PROP)\n",
    "axes[0].set_title(\"ER æ•ˆç‡å› å­åˆ†å¸ƒ\", fontproperties=CHINESE_FONT_PROP)\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(df[\"slope_norm\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[1].axvline(x=THR_SLOPE_NORM, color=\"green\", linestyle=\"--\", label=f\"+é˜ˆå€¼: {THR_SLOPE_NORM}\")\n",
    "axes[1].axvline(x=-THR_SLOPE_NORM, color=\"red\", linestyle=\"--\", label=f\"-é˜ˆå€¼: -{THR_SLOPE_NORM}\")\n",
    "axes[1].set_xlabel(\"KAMA æ–œç‡ (æ ‡å‡†åŒ–)\", fontproperties=CHINESE_FONT_PROP)\n",
    "axes[1].set_ylabel(\"æ•°é‡\", fontproperties=CHINESE_FONT_PROP)\n",
    "axes[1].set_title(\"KAMA æ–œç‡æ ‡å‡†åŒ–åˆ†å¸ƒ\", fontproperties=CHINESE_FONT_PROP)\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].hist(df[\"dist_norm\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[2].axvline(x=THR_DIST_NORM, color=\"green\", linestyle=\"--\", label=f\"+é˜ˆå€¼: {THR_DIST_NORM}\")\n",
    "axes[2].axvline(x=-THR_DIST_NORM, color=\"red\", linestyle=\"--\", label=f\"-é˜ˆå€¼: -{THR_DIST_NORM}\")\n",
    "axes[2].set_xlabel(\"ä»·æ ¼åç¦»åº¦ (æ ‡å‡†åŒ–)\", fontproperties=CHINESE_FONT_PROP)\n",
    "axes[2].set_ylabel(\"æ•°é‡\", fontproperties=CHINESE_FONT_PROP)\n",
    "axes[2].set_title(\"ä»·æ ¼åç¦»åº¦åˆ†å¸ƒ\", fontproperties=CHINESE_FONT_PROP)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nCell 5 å®Œæˆ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 6: æœ¬åœ°ç»“æ„/éœ‡è¡åº¦ç‰¹å¾ ==========\n",
    "\n",
    "def rolling_linreg(y: np.ndarray, window: int) -> tuple:\n",
    "    \"\"\"\n",
    "    å¯¹ y åºåˆ—åšæ»šåŠ¨çº¿æ€§å›å½’ï¼Œè¿”å› (æ–œç‡, RÂ²)\n",
    "    ä½¿ç”¨ç®€å•çš„æœ€å°äºŒä¹˜æ³•: y = beta * x + alpha\n",
    "    x = [0, 1, 2, ..., window-1]\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    beta = np.full(n, np.nan)\n",
    "    r2 = np.full(n, np.nan)\n",
    "    \n",
    "    x = np.arange(window)\n",
    "    x_mean = x.mean()\n",
    "    ss_x = ((x - x_mean) ** 2).sum()\n",
    "    \n",
    "    for i in range(window - 1, n):\n",
    "        y_win = y[i - window + 1:i + 1]\n",
    "        if np.any(np.isnan(y_win)):\n",
    "            continue\n",
    "        \n",
    "        y_mean = y_win.mean()\n",
    "        \n",
    "        # æ–œç‡\n",
    "        ss_xy = ((x - x_mean) * (y_win - y_mean)).sum()\n",
    "        b = ss_xy / ss_x if ss_x > 0 else 0\n",
    "        \n",
    "        # RÂ²\n",
    "        y_pred = b * (x - x_mean) + y_mean\n",
    "        ss_res = ((y_win - y_pred) ** 2).sum()\n",
    "        ss_tot = ((y_win - y_mean) ** 2).sum()\n",
    "        r2_val = 1 - ss_res / ss_tot if ss_tot > 0 else 0\n",
    "        \n",
    "        beta[i] = b\n",
    "        r2[i] = max(0, min(1, r2_val))  # clip to [0, 1]\n",
    "    \n",
    "    return beta, r2\n",
    "# è¿”å›çº¿æ€§å›å½’çš„æ–œç‡å’Œ RÂ²ï¼ˆæ‹Ÿåˆåº¦ï¼‰\n",
    "'''\n",
    "- RÂ² â†’ 1ï¼šä»·æ ¼å˜åŒ–å‡ ä¹å®Œç¾çº¿æ€§ï¼Œå¼ºè¶‹åŠ¿\n",
    "- RÂ² â†’ 0ï¼šä»·æ ¼å˜åŒ–ä¸çº¿æ€§æ¨¡å‹åå·®å¤§ï¼Œæ— æ˜ç¡®æ–¹å‘\n",
    "'''\n",
    "\n",
    "# 1. æ»šåŠ¨çº¿æ€§å›å½’ (å¯¹ log_close)\n",
    "print(\"è®¡ç®—æ»šåŠ¨çº¿æ€§å›å½’...\")\n",
    "log_close_arr = df[\"log_close\"].values # å¯¹æ”¶ç›˜ä»·å–å¯¹æ•°\n",
    "beta_arr, r2_arr = rolling_linreg(log_close_arr, L_BACK) # è®¡ç®—æ”¶ç›˜ä»·çš„æ»šåŠ¨çº¿æ€§å›å½’\n",
    "\n",
    "df[\"beta\"] = beta_arr           # æ–œç‡\n",
    "df[\"r2\"] = r2_arr               # RÂ² (æ‹Ÿåˆç¨‹åº¦)\n",
    "'''\n",
    "æ¯ä¸€æ ¹ bar éƒ½æœ‰ä¸€ä¸ªï¼š\n",
    "\n",
    "betaï¼šè¿™æ®µæ—¶é—´æ•´ä½“æ˜¯å¾€ä¸Šè¿˜æ˜¯å¾€ä¸‹ã€æ–œå¾—å¤š\n",
    "r2ï¼šè¿™æ®µæ—¶é—´æœ‰å¤šâ€œé¡ºç›´â€\n",
    "'''\n",
    "\n",
    "# 2. éœ‡è¡åº¦ (Choppiness Index - åŸºäºATRçš„æ ‡å‡†å®šä¹‰)\n",
    "# Choppiness Index = 100 * log10(sum(ATR_n) / (highest - lowest)) / log10(n)\n",
    "# è¿™ä¸ªå®šä¹‰ä¸ ER åœ¨æ•°å­¦ä¸Šä¸å®Œå…¨ç­‰ä»·ï¼Œå› ä¸ºå®ƒä½¿ç”¨ ATR è€Œéæ”¶ç›˜ä»·å˜åŠ¨\n",
    "print(\"è®¡ç®—éœ‡è¡åº¦ (Choppiness Index)...\")\n",
    "\n",
    "# ä½¿ç”¨ä¸åŒçš„çª—å£æ¥é¿å…ä¸ ER å®Œç¾ç›¸å…³\n",
    "CHOP_WINDOW = 14  # æ ‡å‡† Choppiness Index ä½¿ç”¨ 14 å‘¨æœŸ\n",
    "\n",
    "# è®¡ç®— ATR çš„æ»šåŠ¨å’Œ\n",
    "atr_sum = df[\"tr\"].rolling(window=CHOP_WINDOW).sum() # çª—å£å†…çœŸå®æ³¢å¹… TR çš„æ€»å’Œï¼ˆæ‰€æœ‰æ¥å›çš„æ³¢åŠ¨éƒ½ç®—è¿›å»ï¼‰\n",
    "\n",
    "# è®¡ç®—çª—å£å†…çš„é«˜ä½ç‚¹å·®\n",
    "high_max = df[\"high\"].rolling(window=CHOP_WINDOW).max()\n",
    "low_min = df[\"low\"].rolling(window=CHOP_WINDOW).min()\n",
    "hl_range = high_max - low_min\n",
    "\n",
    "'''\n",
    "å¦‚æœ atr_sum >> hl_rangeï¼šè¯´æ˜ä»·æ ¼æ¥å›èµ°æ¥å›èµ°ï¼ŒåŠ¨å¾—å¾ˆå¤šä½†æ²¡èµ°è¿œ â†’ æåº¦éœ‡è¡\n",
    "å¦‚æœ atr_sum â‰ˆ hl_rangeï¼šè¯´æ˜åŸºæœ¬å°±æ˜¯èµ°ä¸€æ¡è·¯ï¼Œå‡ ä¹æ²¡å›å¤´ â†’ è¶‹åŠ¿å¾ˆå¹²å‡€\n",
    "'''\n",
    "\n",
    "# Choppiness Index (æ ‡å‡†åŒ–åˆ° 0-1)\n",
    "# åŸå§‹å…¬å¼: 100 * log10(atr_sum / hl_range) / log10(n)\n",
    "# ç®€åŒ–: chop = log10(atr_sum / hl_range) / log10(n)\n",
    "# å€¼è¶Šé«˜ = è¶Šéœ‡è¡, å€¼è¶Šä½ = è¶Šè¶‹åŠ¿\n",
    "import math\n",
    "df[\"chop\"] = np.log10(atr_sum / hl_range.replace(0, np.nan)) / math.log10(CHOP_WINDOW)\n",
    "df[\"chop\"] = df[\"chop\"].clip(0, 1)\n",
    "'''\n",
    "- chop â†’ 1ï¼šTRæ€»å’Œè¿œå¤§äºå®é™…ç§»åŠ¨è·ç¦»ï¼Œéœ‡è¡ä¸¥é‡\n",
    "- chop â†’ 0ï¼šTRæ€»å’Œæ¥è¿‘å®é™…ç§»åŠ¨ï¼Œè¶‹åŠ¿å¹²å‡€\n",
    "'''\n",
    "\n",
    "\n",
    "# è¡¥å……: æ·»åŠ ä¸€ä¸ªåŸºäº bar é‡å åº¦çš„éœ‡è¡æŒ‡æ ‡ (ä¸ ER ä¸åŒ)\n",
    "# è®¡ç®—ç›¸é‚» bar çš„é‡å ç¨‹åº¦\n",
    "prev_high = df[\"high\"].shift(1)\n",
    "prev_low = df[\"low\"].shift(1)\n",
    "overlap = np.minimum(df[\"high\"], prev_high) - np.maximum(df[\"low\"], prev_low)\n",
    "overlap = overlap.clip(lower=0)\n",
    "bar_range = (df[\"high\"] - df[\"low\"] + prev_high - prev_low) / 2\n",
    "df[\"overlap_ratio\"] = (overlap / bar_range.replace(0, np.nan)).rolling(window=L_BACK).mean() # overlap / bar_rangeï¼šä¸¤æ ¹ K å åœ¨ä¸€èµ·çš„æ¯”ä¾‹\n",
    "df[\"overlap_ratio\"] = df[\"overlap_ratio\"].clip(0, 1)\n",
    "'''\n",
    "è¶Šé«˜ = ä¸¤ä¸¤ K çº¿ç»å¸¸å•ƒåœ¨ä¸€èµ· = å…¸å‹çª„å¹…éœ‡è¡/ç®±ä½“\n",
    "è¶Šä½ = æ¯æ ¹ bar éƒ½å¾€ä¸€ä¸ªæ–¹å‘æ¨è¿›ï¼Œä¸æ€ä¹ˆå›è¸© â†’ è¶‹åŠ¿æ›´å¹²å‡€\n",
    "'''\n",
    "\n",
    "\n",
    "# 3. éªŒè¯: ER ä¸ chop çš„å…³ç³»\n",
    "# æ³¨æ„: ä½¿ç”¨æ ‡å‡† Choppiness Index å®šä¹‰åï¼ŒER ä¸ chop ä¸å†å®Œç¾è´Ÿç›¸å…³\n",
    "# é«˜ ER -> ä½ chop (è¶‹åŠ¿)\n",
    "# ä½ ER -> é«˜ chop (éœ‡è¡)\n",
    "\n",
    "print(\"\\n=== æœ¬åœ°ç»“æ„ç‰¹å¾è®¡ç®—å®Œæˆ ===\")\n",
    "print(f\"Beta (æ–œç‡): mean={df['beta'].mean():.6f}, std={df['beta'].std():.6f}\")\n",
    "print(f\"RÂ² (æ‹Ÿåˆåº¦): mean={df['r2'].mean():.4f}, median={df['r2'].median():.4f}\")\n",
    "print(f\"Chop (Choppiness Index): mean={df['chop'].mean():.4f}, median={df['chop'].median():.4f}\")\n",
    "print(f\"Overlap Ratio (é‡å åº¦): mean={df['overlap_ratio'].mean():.4f}, median={df['overlap_ratio'].median():.4f}\")\n",
    "\n",
    "\n",
    "# ER vs Chop æ•£ç‚¹å›¾\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# RÂ² åˆ†å¸ƒ\n",
    "axes[0].hist(df[\"r2\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].axvline(x=THR_R2_LOW, color=\"red\", linestyle=\"--\", label=f\"ä½è¶‹åŠ¿é˜ˆå€¼: {THR_R2_LOW}\")\n",
    "axes[0].set_xlabel(\"RÂ² (æ‹Ÿåˆåº¦)\")\n",
    "axes[0].set_ylabel(\"æ•°é‡\")\n",
    "axes[0].set_title(\"RÂ² æ‹Ÿåˆåº¦åˆ†å¸ƒ\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Chop åˆ†å¸ƒ\n",
    "axes[1].hist(df[\"chop\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[1].axvline(x=THR_CHOP_HIGH, color=\"red\", linestyle=\"--\", label=f\"éœ‡è¡é˜ˆå€¼: {THR_CHOP_HIGH}\")\n",
    "axes[1].axvline(x=THR_CHOP_LOW, color=\"green\", linestyle=\"--\", label=f\"è¶‹åŠ¿é˜ˆå€¼: {THR_CHOP_LOW}\")\n",
    "axes[1].set_xlabel(\"Chop (éœ‡è¡åº¦)\")\n",
    "axes[1].set_ylabel(\"æ•°é‡\")\n",
    "axes[1].set_title(\"éœ‡è¡åº¦ (Chop) åˆ†å¸ƒ\")\n",
    "axes[1].legend()\n",
    "\n",
    "# ER vs Chop æ•£ç‚¹å›¾ (é‡‡æ ·ä»¥åŠ é€Ÿ)\n",
    "sample_idx = np.random.choice(len(df.dropna()), size=min(5000, len(df.dropna())), replace=False)\n",
    "df_sample = df.dropna().iloc[sample_idx]\n",
    "axes[2].scatter(df_sample[\"er\"], df_sample[\"chop\"], alpha=0.3, s=5)\n",
    "axes[2].set_xlabel(\"ER (æ•ˆç‡å› å­)\")\n",
    "axes[2].set_ylabel(\"Chop (Choppiness Index)\")\n",
    "axes[2].set_title(\"ER vs Chop ç›¸å…³æ€§\")\n",
    "axes[2].axvline(x=THR_ER_LOW, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "axes[2].axhline(y=THR_CHOP_HIGH, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# è®¡ç®—ç›¸å…³ç³»æ•°\n",
    "corr_er_chop = df[[\"er\", \"chop\"]].dropna().corr().iloc[0, 1]\n",
    "corr_er_overlap = df[[\"er\", \"overlap_ratio\"]].dropna().corr().iloc[0, 1]\n",
    "print(f\"\\nER vs Chop ç›¸å…³ç³»æ•°: {corr_er_chop:.4f} (åº”è¯¥ä¸å†æ˜¯ -1.0)\")\n",
    "print(f\"ER vs Overlap Ratio ç›¸å…³ç³»æ•°: {corr_er_overlap:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nCell 6 å®Œæˆ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 7: Triple Barrier æ–¹å‘æ ‡ç­¾ ==========\n",
    "\n",
    "def triple_barrier_labels(close: np.ndarray, atr: np.ndarray, \n",
    "                          t_vert: int, pt_mult: float, sl_mult: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ä¸ºæ¯ä¸ª bar è®¡ç®— triple barrier æ ‡ç­¾\n",
    "    \n",
    "    å‚æ•°:\n",
    "        close: æ”¶ç›˜ä»·æ•°ç»„\n",
    "        atr: ATR æ•°ç»„\n",
    "        t_vert: å‚ç›´æ—¶é—´éšœç¢ (æœ€å¤§æŒæœ‰æ—¶é—´)\n",
    "        pt_mult: æ­¢ç›ˆå€æ•° (ç›¸å¯¹ ATR)\n",
    "        sl_mult: æ­¢æŸå€æ•° (ç›¸å¯¹ ATR)\n",
    "    \n",
    "    è¿”å›:\n",
    "        d: æ–¹å‘æ ‡ç­¾æ•°ç»„ (-1, 0, 1)\n",
    "           +1: å…ˆè§¦åŠä¸Šè½¨ (æ­¢ç›ˆ)\n",
    "           -1: å…ˆè§¦åŠä¸‹è½¨ (æ­¢æŸ)\n",
    "            0: æœªè§¦åŠä»»ä½•è½¨é“ (æ—¶é—´æˆªæ­¢)\n",
    "    \"\"\"\n",
    "    n = len(close)\n",
    "    d = np.zeros(n, dtype=np.int32)\n",
    "    \n",
    "    for t in range(n):\n",
    "        if np.isnan(atr[t]) or atr[t] <= 0:\n",
    "            d[t] = 0\n",
    "            continue\n",
    "        \n",
    "        c_t = close[t]\n",
    "        upper = c_t + pt_mult * atr[t]  # æ­¢ç›ˆä¸Šè½¨\n",
    "        lower = c_t - sl_mult * atr[t]  # æ­¢æŸä¸‹è½¨\n",
    "        \n",
    "        # éå†æœªæ¥è·¯å¾„\n",
    "        hit_up = False\n",
    "        hit_down = False\n",
    "        first_hit_up = n + 1\n",
    "        first_hit_down = n + 1\n",
    "        \n",
    "        for j in range(1, min(t_vert + 1, n - t)): # å‘æœªæ¥çœ‹ t_vert æ ¹\n",
    "            future_close = close[t + j]\n",
    "            \n",
    "            if future_close >= upper and not hit_up:\n",
    "                hit_up = True\n",
    "                first_hit_up = j\n",
    "            \n",
    "            if future_close <= lower and not hit_down:\n",
    "                hit_down = True\n",
    "                first_hit_down = j\n",
    "            \n",
    "            if hit_up and hit_down:\n",
    "                break\n",
    "        \n",
    "        # åˆ¤æ–­å“ªä¸ªå…ˆè§¦åŠ\n",
    "        if hit_up and hit_down:\n",
    "            if first_hit_up < first_hit_down:\n",
    "                d[t] = 1\n",
    "            elif first_hit_down < first_hit_up:\n",
    "                d[t] = -1\n",
    "            else:\n",
    "                d[t] = 0  # åŒæ—¶è§¦åŠï¼Œæ ‡ä¸ºä¸­æ€§\n",
    "        elif hit_up:\n",
    "            d[t] = 1\n",
    "        elif hit_down:\n",
    "            d[t] = -1\n",
    "        else:\n",
    "            # æœªè§¦åŠä»»ä½•è½¨é“ï¼Œç”¨æœ€ç»ˆæ–¹å‘\n",
    "            end_idx = min(t + t_vert, n - 1)\n",
    "            if end_idx > t:\n",
    "                final_move = close[end_idx] - c_t\n",
    "                if final_move > 0:\n",
    "                    d[t] = 1\n",
    "                elif final_move < 0:\n",
    "                    d[t] = -1\n",
    "                else:\n",
    "                    d[t] = 0\n",
    "            else:\n",
    "                d[t] = 0\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "# è®¡ç®— Triple Barrier æ ‡ç­¾\n",
    "print(\"è®¡ç®— Triple Barrier æ–¹å‘æ ‡ç­¾...\")\n",
    "print(f\"å‚æ•°: T_vert={T_VERT}, pt_mult={PT_MULT}, sl_mult={SL_MULT}\")\n",
    "\n",
    "close_arr = df[\"close\"].values.astype(np.float64)\n",
    "atr_arr = df[\"atr\"].values.astype(np.float64)\n",
    "\n",
    "df[\"d_barrier\"] = triple_barrier_labels(close_arr, atr_arr, T_VERT, PT_MULT, SL_MULT)\n",
    "\n",
    "# ç»Ÿè®¡åˆ†å¸ƒ\n",
    "barrier_counts = df[\"d_barrier\"].value_counts().sort_index()\n",
    "print(\"\\n=== Triple Barrier æ ‡ç­¾åˆ†å¸ƒ ===\")\n",
    "for k, v in barrier_counts.items():\n",
    "    label_name = {-1: \"DOWN\", 0: \"NEUTRAL\", 1: \"UP\"}.get(k, str(k))\n",
    "    print(f\"  {label_name} ({k}): {v} ({v / len(df):.2%})\")\n",
    "\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "barrier_counts.plot(kind=\"bar\", ax=ax, color=[\"#ef5350\", \"#ffeb3b\", \"#26a69a\"], edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Triple Barrier æ ‡ç­¾\")\n",
    "ax.set_ylabel(\"æ•°é‡\")\n",
    "ax.set_title(\"Triple Barrier æ–¹å‘æ ‡ç­¾åˆ†å¸ƒ\")\n",
    "ax.set_xticklabels([\"ä¸‹è·Œ (-1)\", \"ä¸­æ€§ (0)\", \"ä¸Šæ¶¨ (+1)\"], rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nCell 7 å®Œæˆ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 8: å¸‚åœºå‘¨æœŸæ ‡ç­¾ - Step 1: è¯†åˆ«éœ‡è¡åŒºé—´ ==========\n",
    "\n",
    "def identify_trading_range(df: pd.DataFrame, \n",
    "                           thr_er_low: float,\n",
    "                           thr_chop_high: float,\n",
    "                           thr_range_atr: float,\n",
    "                           thr_r2_low: float,\n",
    "                           thr_future_move: float,\n",
    "                           l_range_fwd: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    è¯†åˆ«éœ‡è¡/äº¤æ˜“åŒºé—´\n",
    "    \n",
    "    åˆ¤å®šæ¡ä»¶ (æ»¡è¶³æ‰€æœ‰æ¡ä»¶):\n",
    "    1. ER < thr_er_low (æ•ˆç‡ä½, æ¥å›éœ‡è¡)\n",
    "    2. chop > thr_chop_high (éœ‡è¡åº¦é«˜)\n",
    "    3. range_atr < thr_range_atr (åŒºé—´ç›¸å¯¹çª„)\n",
    "    4. RÂ² < thr_r2_low (æ— æ˜æ˜¾çº¿æ€§è¶‹åŠ¿)\n",
    "    5. æœªæ¥ç§»åŠ¨ < thr_future_move * ATR (ä»åœ¨åŒºé—´å†…)\n",
    "    \n",
    "    è¿”å›:\n",
    "        is_range: bool æ•°ç»„, True è¡¨ç¤ºè¯¥ bar å¤„äºäº¤æ˜“åŒºé—´\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    is_range = np.zeros(n, dtype=bool)\n",
    "    \n",
    "    er = df[\"er\"].values\n",
    "    chop = df[\"chop\"].values\n",
    "    range_atr = df[\"range_atr\"].values\n",
    "    r2 = df[\"r2\"].values\n",
    "    close = df[\"close\"].values\n",
    "    atr = df[\"atr\"].values\n",
    "    \n",
    "    # å¯¹æ¯ä¸ªæ—¶é—´ç‚¹ t åšä¸€å¥—è§„åˆ™åˆ¤æ–­\n",
    "    for t in range(n):\n",
    "        # æ£€æŸ¥åŸºæœ¬æ¡ä»¶\n",
    "        if np.isnan(er[t]) or np.isnan(chop[t]) or np.isnan(range_atr[t]) or np.isnan(r2[t]):\n",
    "            continue\n",
    "        if np.isnan(atr[t]) or atr[t] <= 0:\n",
    "            continue\n",
    "        \n",
    "        cond_er = er[t] < thr_er_low\n",
    "        cond_chop = chop[t] > thr_chop_high\n",
    "        cond_range = range_atr[t] < thr_range_atr\n",
    "        cond_r2 = r2[t] < thr_r2_low\n",
    "        \n",
    "        # æ£€æŸ¥æœªæ¥ç§»åŠ¨ (åˆ©ç”¨å°‘é‡æœªæ¥ä¿¡æ¯)\n",
    "        future_idx = min(t + l_range_fwd, n - 1)\n",
    "        if future_idx > t:\n",
    "            future_move = abs(close[future_idx] - close[t])\n",
    "            cond_future = future_move < thr_future_move * atr[t]\n",
    "        else:\n",
    "            cond_future = True\n",
    "        \n",
    "        # éœ‡è¡åŒºé—´åˆ¤å®š: æ»¡è¶³ ER + chop æ¡ä»¶ AND (range æˆ– RÂ² æˆ– future move)\n",
    "        if cond_er and cond_chop and (cond_range or cond_r2 or cond_future):\n",
    "            is_range[t] = True\n",
    "    \n",
    "    return is_range\n",
    "\n",
    "\n",
    "# è®¡ç®—åˆæ­¥çš„éœ‡è¡åŒºé—´æ ‡è®°\n",
    "print(\"è¯†åˆ«éœ‡è¡/äº¤æ˜“åŒºé—´...\")\n",
    "is_range = identify_trading_range(\n",
    "    df, \n",
    "    thr_er_low=THR_ER_LOW,\n",
    "    thr_chop_high=THR_CHOP_HIGH,\n",
    "    thr_range_atr=THR_RANGE_ATR,\n",
    "    thr_r2_low=THR_R2_LOW,\n",
    "    thr_future_move=THR_FUTURE_MOVE,\n",
    "    l_range_fwd=L_RANGE_FWD\n",
    ")\n",
    "\n",
    "df[\"is_range_raw\"] = is_range # åˆæ­¥éœ‡è¡åŒºé—´æ ‡è®°\n",
    "\n",
    "# ç»Ÿè®¡\n",
    "range_count = is_range.sum()\n",
    "print(f\"\\nåˆæ­¥éœ‡è¡åŒºé—´ bar æ•°é‡: {range_count} ({range_count / len(df):.2%})\")\n",
    "\n",
    "# åº”ç”¨æœ€å°é•¿åº¦çº¦æŸ (Brooks 20-bar rule)\n",
    "# åªæœ‰è¿ç»­ >= MIN_RANGE_LEN ä¸ª bar éƒ½æ˜¯éœ‡è¡åŒºé—´ï¼Œæ‰ç¡®è®¤ä¸ºéœ‡è¡\n",
    "def apply_min_length_constraint(arr: np.ndarray, min_len: int, value: bool) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    å¯¹è¿ç»­æ®µåº”ç”¨æœ€å°é•¿åº¦çº¦æŸ\n",
    "    \"\"\"\n",
    "    result = arr.copy()\n",
    "    n = len(arr)\n",
    "    i = 0\n",
    "    \n",
    "    while i < n:\n",
    "        if arr[i] == value:\n",
    "            # æ‰¾è¿ç»­æ®µçš„ç»“æŸ\n",
    "            j = i + 1\n",
    "            while j < n and arr[j] == value:\n",
    "                j += 1\n",
    "            \n",
    "            seg_len = j - i\n",
    "            if seg_len < min_len:\n",
    "                # å¤ªçŸ­ï¼Œå–æ¶ˆæ ‡è®°\n",
    "                result[i:j] = not value\n",
    "            \n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return result\n",
    "# åªæœ‰è¿ç»­ True æ®µé•¿åº¦ â‰¥ MIN_RANGE_LENï¼ˆé»˜è®¤15ï¼‰æ‰ä¿ç•™ä¸º range\n",
    "\n",
    "# åº”ç”¨æœ€å°é•¿åº¦çº¦æŸ\n",
    "is_range_filtered = apply_min_length_constraint(is_range, MIN_RANGE_LEN, True)\n",
    "df[\"is_range\"] = is_range_filtered\n",
    "\n",
    "range_count_filtered = is_range_filtered.sum()\n",
    "print(f\"è¿‡æ»¤åéœ‡è¡åŒºé—´ bar æ•°é‡ (min_len={MIN_RANGE_LEN}): {range_count_filtered} ({range_count_filtered / len(df):.2%})\")\n",
    "\n",
    "'''\n",
    "# æ‰©å±•è¾¹ç¼˜ (å‰åå„ 2-3 bar)\n",
    "EDGE_EXTEND = 2\n",
    "\n",
    "def extend_range_edges(arr: np.ndarray, extend: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    å¯¹éœ‡è¡åŒºé—´çš„è¾¹ç¼˜è¿›è¡Œæ‰©å±•\n",
    "    \"\"\"\n",
    "    result = arr.copy()\n",
    "    n = len(arr)\n",
    "    \n",
    "    for i in range(n):\n",
    "        if arr[i]:\n",
    "            # å‘å‰æ‰©å±•\n",
    "            for k in range(1, extend + 1):\n",
    "                if i - k >= 0:\n",
    "                    result[i - k] = True\n",
    "            # å‘åæ‰©å±•\n",
    "            for k in range(1, extend + 1):\n",
    "                if i + k < n:\n",
    "                    result[i + k] = True\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "is_range_extended = extend_range_edges(is_range_filtered, EDGE_EXTEND)\n",
    "df[\"is_range_extended\"] = is_range_extended\n",
    "\n",
    "range_count_extended = is_range_extended.sum()\n",
    "print(f\"æ‰©å±•è¾¹ç¼˜åéœ‡è¡åŒºé—´ bar æ•°é‡ (extend={EDGE_EXTEND}): {range_count_extended} ({range_count_extended / len(df):.2%})\")\n",
    "'''\n",
    "\n",
    "# =========================\n",
    "# âœ… ç”¨â€œè¾¹ç¼˜å†…ç¼©â€æ›¿ä»£â€œè¾¹ç¼˜æ‰©å±•â€\n",
    "# =========================\n",
    "\n",
    "EDGE_SHRINK = 2  # è¾¹ç¼˜å†…ç¼©æ•°é‡\n",
    "\n",
    "def shrink_range_edges(arr: np.ndarray, shrink: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    å¯¹æ¯æ®µè¿ç»­ True çš„éœ‡è¡åŒºé—´ï¼Œæ®µé¦–/æ®µå°¾å„ç¼©è¿›å» shrink æ ¹\n",
    "    - å¦‚æœæŸæ®µé•¿åº¦ <= 2*shrinkï¼šç¼©å®Œå°±æ²¡äº†ï¼Œæ•´æ®µç½® False\n",
    "    \"\"\"\n",
    "    result = arr.copy()\n",
    "    n = len(arr)\n",
    "    i = 0\n",
    "\n",
    "    while i < n:\n",
    "        if result[i]:\n",
    "            # æ‰¾åˆ°è¿ç»­ True æ®µ [i, j)\n",
    "            j = i + 1\n",
    "            while j < n and result[j]:\n",
    "                j += 1\n",
    "\n",
    "            seg_len = j - i\n",
    "            if seg_len <= 2 * shrink:\n",
    "                # ç¼©å®Œä¸å­˜åœ¨äº†\n",
    "                result[i:j] = False\n",
    "            else:\n",
    "                # æ®µé¦– shrink æ ¹ç½® False\n",
    "                result[i:i+shrink] = False\n",
    "                # æ®µå°¾ shrink æ ¹ç½® False\n",
    "                result[j-shrink:j] = False\n",
    "\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "# å…ˆæœ€å°é•¿åº¦è¿‡æ»¤ï¼ˆä½ åŸæœ¬å°±æ˜¯è¿™ä¹ˆåšçš„ï¼‰\n",
    "is_range_filtered = apply_min_length_constraint(is_range, MIN_RANGE_LEN, True)\n",
    "df[\"is_range\"] = is_range_filtered\n",
    "\n",
    "# å†åšè¾¹ç¼˜å†…ç¼©\n",
    "is_range_shrunk = shrink_range_edges(is_range_filtered, EDGE_SHRINK)\n",
    "\n",
    "# å¯é€‰ï¼ˆæ¨èï¼‰ï¼šç¼©å®Œåå†åšä¸€æ¬¡æœ€å°é•¿åº¦è¿‡æ»¤ï¼Œé˜²æ­¢æ®µè¢«ç¼©å¾—å¤ªç¢\n",
    "is_range_shrunk = apply_min_length_constraint(is_range_shrunk, MIN_RANGE_LEN, True)\n",
    "\n",
    "df[\"is_range_shrunk\"] = is_range_shrunk\n",
    "\n",
    "# ç»Ÿè®¡\n",
    "range_count_shrunk = is_range_shrunk.sum()\n",
    "print(f\"è¾¹ç¼˜å†…ç¼©åéœ‡è¡åŒºé—´ bar æ•°é‡ (shrink={EDGE_SHRINK}): \"\n",
    "      f\"{range_count_shrunk} ({range_count_shrunk / len(df):.2%})\")\n",
    "\n",
    "\n",
    "print(\"\\nCell 8 å®Œæˆ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 9: å¸‚åœºå‘¨æœŸæ ‡ç­¾ - Step 2: è¯†åˆ«è¶‹åŠ¿ ==========\n",
    "\n",
    "def identify_trend(df: pd.DataFrame,\n",
    "                   is_range: np.ndarray,\n",
    "                   thr_er_high: float,\n",
    "                   thr_chop_low: float,\n",
    "                   thr_slope_norm: float,\n",
    "                   thr_dist_norm: float,\n",
    "                   l_fwd: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    è¯†åˆ«è¶‹åŠ¿ (å¯¹ééœ‡è¡åŒºé—´çš„ bar)\n",
    "    \n",
    "    è¶‹åŠ¿åˆ¤å®šæ¡ä»¶:\n",
    "    1. æ–¹å‘ä¸€è‡´æ€§: sign(slope_norm) == sign(æœªæ¥æ”¶ç›Š)\n",
    "    2. æ–œç‡å¼ºåº¦: |slope_norm| > thr_slope_norm\n",
    "    3. æ•ˆç‡/è¶‹åŠ¿æ€§: ER > thr_er_high æˆ– chop < thr_chop_low\n",
    "    4. ä»·æ ¼åç¦»: |dist_norm| > thr_dist_norm (å¯é€‰)\n",
    "    5. Triple barrier æ–¹å‘ç¡®è®¤\n",
    "    \n",
    "    è¿”å›:\n",
    "        regime: æ ‡ç­¾æ•°ç»„ (-1, 0, 1)\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    regime = np.zeros(n, dtype=np.int32)\n",
    "    \n",
    "    close = df[\"close\"].values\n",
    "    er = df[\"er\"].values\n",
    "    chop = df[\"chop\"].values\n",
    "    slope_norm = df[\"slope_norm\"].values\n",
    "    dist_norm = df[\"dist_norm\"].values\n",
    "    d_barrier = df[\"d_barrier\"].values\n",
    "    \n",
    "    for t in range(n):\n",
    "        # å¦‚æœå·²è¢«æ ‡è®°ä¸ºéœ‡è¡åŒºé—´ï¼Œç›´æ¥è®¾ä¸º 0\n",
    "        if is_range[t]:\n",
    "            regime[t] = 0\n",
    "            continue\n",
    "        \n",
    "        # æ£€æŸ¥ç‰¹å¾æ˜¯å¦æœ‰æ•ˆ\n",
    "        if (np.isnan(er[t]) or np.isnan(chop[t]) or \n",
    "            np.isnan(slope_norm[t]) or np.isnan(dist_norm[t])):\n",
    "            regime[t] = 0\n",
    "            continue\n",
    "        \n",
    "        # æ£€æŸ¥æœªæ¥æ”¶ç›Šæ–¹å‘\n",
    "        future_idx = min(t + l_fwd, n - 1)\n",
    "        if future_idx > t:\n",
    "            future_return = close[future_idx] - close[t]\n",
    "            future_direction = np.sign(future_return)\n",
    "        else:\n",
    "            future_direction = 0\n",
    "        \n",
    "        # è¶‹åŠ¿å€™é€‰æ¡ä»¶\n",
    "        slope_direction = np.sign(slope_norm[t])\n",
    "        \n",
    "        # æ¡ä»¶1: æ–¹å‘ä¸€è‡´æ€§\n",
    "        direction_consistent = (slope_direction == future_direction) and (slope_direction != 0)\n",
    "        \n",
    "        # æ¡ä»¶2: æ–œç‡å¼ºåº¦\n",
    "        slope_strong = abs(slope_norm[t]) > thr_slope_norm\n",
    "        \n",
    "        # æ¡ä»¶3: æ•ˆç‡/è¶‹åŠ¿æ€§ (æ»¡è¶³å…¶ä¸€å³å¯)\n",
    "        trend_quality = (er[t] > thr_er_high) or (chop[t] < thr_chop_low)\n",
    "        \n",
    "        # æ¡ä»¶4: ä»·æ ¼åç¦» (å¯é€‰åŠ å¼ºæ¡ä»¶)\n",
    "        price_deviated = abs(dist_norm[t]) > thr_dist_norm\n",
    "        \n",
    "        # ç»¼åˆåˆ¤å®š\n",
    "        # å¿…é¡»æ»¡è¶³: æ–¹å‘ä¸€è‡´ + æ–œç‡å¼ºåº¦ + (è¶‹åŠ¿è´¨é‡ æˆ– ä»·æ ¼åç¦»)\n",
    "        is_trend = direction_consistent and slope_strong and (trend_quality or price_deviated)\n",
    "        \n",
    "        if is_trend:\n",
    "            # å†ç”¨ triple barrier ç¡®è®¤æ–¹å‘\n",
    "            if d_barrier[t] == slope_direction:\n",
    "                regime[t] = int(slope_direction)\n",
    "            elif d_barrier[t] == 0:\n",
    "                # barrier æ— æ–¹å‘ï¼Œä½†ç»“æ„æ˜¾ç¤ºè¶‹åŠ¿ï¼Œä»ç»™è¶‹åŠ¿æ ‡ç­¾\n",
    "                regime[t] = int(slope_direction)\n",
    "            else:\n",
    "                # barrier æ–¹å‘ä¸ç»“æ„ä¸ä¸€è‡´ï¼Œæ ‡ä¸ºæ¨¡ç³Š (0)\n",
    "                regime[t] = 0\n",
    "        else:\n",
    "            regime[t] = 0\n",
    "    \n",
    "    return regime\n",
    "\n",
    "\n",
    "# è¯†åˆ«è¶‹åŠ¿\n",
    "print(\"è¯†åˆ«è¶‹åŠ¿...\")\n",
    "regime_raw = identify_trend(\n",
    "    df,\n",
    "    is_range=df[\"is_range_shrunk\"].values,\n",
    "    thr_er_high=THR_ER_HIGH,\n",
    "    thr_chop_low=THR_CHOP_LOW,\n",
    "    thr_slope_norm=THR_SLOPE_NORM,\n",
    "    thr_dist_norm=THR_DIST_NORM,\n",
    "    l_fwd=L_FWD\n",
    ")\n",
    "\n",
    "df[\"regime_raw\"] = regime_raw\n",
    "\n",
    "# ç»Ÿè®¡\n",
    "regime_counts_raw = pd.Series(regime_raw).value_counts().sort_index()\n",
    "print(\"\\n=== åŸå§‹ Regime æ ‡ç­¾åˆ†å¸ƒ ===\")\n",
    "for k, v in regime_counts_raw.items():\n",
    "    label_name = {-1: \"DOWN (-1)\", 0: \"RANGE (0)\", 1: \"UP (+1)\"}.get(k, str(k))\n",
    "    print(f\"  {label_name}: {v} ({v / len(df):.2%})\")\n",
    "\n",
    "print(\"\\nCell 9 å®Œæˆ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 10: å¸‚åœºå‘¨æœŸæ ‡ç­¾ - Step 3: æ ‡ç­¾å¹³æ»‘ ==========\n",
    "\n",
    "# å¼ºåˆ¶ç”¨æˆ‘ä»¬åœ¨ Cell 1 é‡Œå‘ç°çš„ Mac ä¸­æ–‡å­—ä½“\n",
    "mpl.rcParams['font.family'] = 'sans-serif' \n",
    "mpl.rcParams['font.sans-serif'] = ['Heiti TC'] # å’Œ Cell 1 æ‰“å°å‡ºæ¥çš„åå­—ä¸€è‡´ \n",
    "mpl.rcParams['axes.unicode_minus'] = False \n",
    "print(\"å½“å‰å­—ä½“é…ç½®:\", mpl.rcParams['font.sans-serif'])\n",
    "\n",
    "def smooth_regime_labels(regime: np.ndarray, \n",
    "                         min_trend_len: int,\n",
    "                         smooth_window: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    å¯¹æ ‡ç­¾åºåˆ—è¿›è¡Œå¹³æ»‘å¤„ç†\n",
    "    \n",
    "    æ­¥éª¤:\n",
    "    1. åˆ é™¤å¤ªçŸ­çš„è¶‹åŠ¿æ®µ (é•¿åº¦ < min_trend_len çš„ Â±1 æ®µæ”¹ä¸º 0)\n",
    "    2. å¤šæ•°æŠ•ç¥¨å¹³æ»‘\n",
    "    \n",
    "    å‚æ•°:\n",
    "        regime: åŸå§‹æ ‡ç­¾æ•°ç»„ (-1, 0, 1)\n",
    "        min_trend_len: æœ€å°è¶‹åŠ¿æ®µé•¿åº¦\n",
    "        smooth_window: å¹³æ»‘çª—å£å¤§å°\n",
    "    \n",
    "    è¿”å›:\n",
    "        smoothed: å¹³æ»‘åçš„æ ‡ç­¾æ•°ç»„\n",
    "    \"\"\"\n",
    "    n = len(regime)\n",
    "    result = regime.copy()\n",
    "    \n",
    "    # Step 1: åˆ é™¤å¤ªçŸ­çš„è¶‹åŠ¿æ®µ\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        if result[i] != 0:  # è¶‹åŠ¿æ®µ\n",
    "            # æ‰¾è¿ç»­æ®µçš„ç»“æŸ\n",
    "            j = i + 1\n",
    "            while j < n and result[j] == result[i]:\n",
    "                j += 1\n",
    "            \n",
    "            seg_len = j - i\n",
    "            if seg_len < min_trend_len:\n",
    "                # å¤ªçŸ­ï¼Œæ”¹ä¸º 0 (éœ‡è¡)\n",
    "                result[i:j] = 0\n",
    "            \n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    # Step 2: å¤šæ•°æŠ•ç¥¨å¹³æ»‘ (ä¸­å€¼æ»¤æ³¢çš„å˜ä½“)\n",
    "    half_win = smooth_window // 2\n",
    "    smoothed = result.copy()\n",
    "    \n",
    "    for i in range(n):\n",
    "        start = max(0, i - half_win)\n",
    "        end = min(n, i + half_win + 1)\n",
    "        window = result[start:end]\n",
    "        \n",
    "        # ç»Ÿè®¡æ¯ä¸ªæ ‡ç­¾çš„å‡ºç°æ¬¡æ•°\n",
    "        counts = {-1: 0, 0: 0, 1: 0}\n",
    "        for v in window:\n",
    "            counts[v] = counts.get(v, 0) + 1\n",
    "        \n",
    "        # æ‰¾å‡ºæœ€å¤šçš„æ ‡ç­¾\n",
    "        max_count = max(counts.values())\n",
    "        candidates = [k for k, v in counts.items() if v == max_count]\n",
    "        \n",
    "        # å¦‚æœæœ‰å¤šä¸ªå€™é€‰ï¼Œä¼˜å…ˆä¿æŒåŸå€¼æˆ–å– 0\n",
    "        if len(candidates) == 1:\n",
    "            smoothed[i] = candidates[0]\n",
    "        elif result[i] in candidates:\n",
    "            smoothed[i] = result[i]\n",
    "        else:\n",
    "            smoothed[i] = 0\n",
    "    \n",
    "    return smoothed\n",
    "\n",
    "\n",
    "# åº”ç”¨å¹³æ»‘\n",
    "print(f\"æ ‡ç­¾å¹³æ»‘ (min_trend_len={MIN_TREND_LEN}, smooth_window={SMOOTH_WINDOW})...\")\n",
    "regime_smoothed = smooth_regime_labels(\n",
    "    df[\"regime_raw\"].values,\n",
    "    min_trend_len=MIN_TREND_LEN,\n",
    "    smooth_window=SMOOTH_WINDOW\n",
    ")\n",
    "\n",
    "df[\"regime\"] = regime_smoothed\n",
    "\n",
    "# æœ€ç»ˆç»Ÿè®¡\n",
    "regime_counts_final = pd.Series(regime_smoothed).value_counts().sort_index()\n",
    "print(\"\\n=== æœ€ç»ˆ Market Cycle æ ‡ç­¾åˆ†å¸ƒ ===\")\n",
    "total = len(df)\n",
    "for k, v in regime_counts_final.items():\n",
    "    label_name = {-1: \"DOWN (-1)\", 0: \"RANGE (0)\", 1: \"UP (+1)\"}.get(k, str(k))\n",
    "    print(f\"  {label_name}: {v:,} ({v / total:.2%})\")\n",
    "\n",
    "# ä¸ Al Brooks 80/20 è§„åˆ™å¯¹æ¯”\n",
    "range_pct = regime_counts_final.get(0, 0) / total\n",
    "trend_pct = (regime_counts_final.get(-1, 0) + regime_counts_final.get(1, 0)) / total\n",
    "print(f\"\\néœ‡è¡åŒºé—´æ¯”ä¾‹: {range_pct:.2%} (Al Brooks: ~80%)\")\n",
    "print(f\"è¶‹åŠ¿åŒºé—´æ¯”ä¾‹: {trend_pct:.2%} (Al Brooks: ~20%)\")\n",
    "\n",
    "# å¯è§†åŒ–å¯¹æ¯”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# åŸå§‹ vs å¹³æ»‘åçš„åˆ†å¸ƒ\n",
    "regime_counts_raw_series = pd.Series(df[\"regime_raw\"].values).value_counts().sort_index()\n",
    "\n",
    "width = 0.35\n",
    "x = np.arange(3)\n",
    "labels = [\"ä¸‹è·Œ (-1)\", \"éœ‡è¡ (0)\", \"ä¸Šæ¶¨ (+1)\"]\n",
    "\n",
    "\n",
    "axes[0].bar(x - width/2, [regime_counts_raw_series.get(k, 0) for k in [-1, 0, 1]], \n",
    "            width, label=\"åŸå§‹\", alpha=0.8)\n",
    "axes[0].bar(x + width/2, [regime_counts_final.get(k, 0) for k in [-1, 0, 1]], \n",
    "            width, label=\"å¹³æ»‘å\", alpha=0.8)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(labels)\n",
    "axes[0].set_ylabel(\"æ•°é‡\")\n",
    "axes[0].set_title(\"å¸‚åœºå‘¨æœŸæ ‡ç­¾åˆ†å¸ƒå¯¹æ¯” (å¹³æ»‘å‰å)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# é¥¼å›¾\n",
    "colors = [\"#ef5350\", \"#ffeb3b\", \"#26a69a\"]\n",
    "sizes = [regime_counts_final.get(k, 0) for k in [-1, 0, 1]]\n",
    "axes[1].pie(sizes, labels=labels, colors=colors, autopct=\"%1.1f%%\", startangle=90)\n",
    "axes[1].set_title(\"æœ€ç»ˆå¸‚åœºå‘¨æœŸåˆ†å¸ƒ\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCell 10 å®Œæˆ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 11: Bokeh Kçº¿å›¾å¯è§†åŒ–å‡½æ•°å®šä¹‰ ==========\n",
    "\n",
    "def plot_kline_with_regime(df_day: pd.DataFrame, title: str = \"K-line with Market Regime\") -> figure:\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶å•æ—¥ K çº¿å›¾ï¼Œå¹¶åœ¨èƒŒæ™¯ä¸Šå¡«å…… market regime é¢œè‰²\n",
    "    \n",
    "    é¢œè‰²æ˜ å°„:\n",
    "    - DOWN (-1): æµ…çº¢è‰²\n",
    "    - RANGE (0): æµ…é»„è‰²  \n",
    "    - UP (+1): æµ…ç»¿è‰²\n",
    "    \n",
    "    Args:\n",
    "        df_day: å•æ—¥çš„ K çº¿æ•°æ® DataFrameï¼Œéœ€åŒ…å«:\n",
    "                timestamp_dt, open, high, low, close, regime\n",
    "        title: å›¾è¡¨æ ‡é¢˜\n",
    "    \n",
    "    Returns:\n",
    "        Bokeh figure å¯¹è±¡\n",
    "    \"\"\"\n",
    "    df_day = df_day.copy().reset_index(drop=True)\n",
    "    \n",
    "    # è®¡ç®— K çº¿çš„æ¶¨è·Œé¢œè‰²\n",
    "    df_day[\"color\"] = df_day.apply(\n",
    "        lambda row: \"#26a69a\" if row[\"close\"] >= row[\"open\"] else \"#ef5350\",\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # K çº¿å®½åº¦ï¼ˆæ¯«ç§’ï¼Œ5 åˆ†é’Ÿ K çº¿ç”¨ 4 åˆ†é’Ÿå®½åº¦ï¼‰\n",
    "    w = 4 * 60 * 1000  # 4 minutes in milliseconds\n",
    "    \n",
    "    # åˆ›å»º ColumnDataSource\n",
    "    source = ColumnDataSource(data={\n",
    "        \"timestamp\": df_day[\"timestamp_dt\"],\n",
    "        \"open\": df_day[\"open\"],\n",
    "        \"high\": df_day[\"high\"],\n",
    "        \"low\": df_day[\"low\"],\n",
    "        \"close\": df_day[\"close\"],\n",
    "        \"color\": df_day[\"color\"],\n",
    "        \"regime\": df_day[\"regime\"].fillna(0).astype(int),\n",
    "    })\n",
    "    \n",
    "    # è®¡ç®— Y è½´èŒƒå›´\n",
    "    y_min = df_day[\"low\"].min() * 0.9995\n",
    "    y_max = df_day[\"high\"].max() * 1.0005\n",
    "    \n",
    "    # åˆ›å»º figure\n",
    "    p = figure(\n",
    "        title=title,\n",
    "        x_axis_type=\"datetime\",\n",
    "        width=1200,\n",
    "        height=500,\n",
    "        y_range=Range1d(y_min, y_max),\n",
    "        tools=\"pan,wheel_zoom,box_zoom,reset,save\",\n",
    "        background_fill_color=\"#fafafa\",\n",
    "    )\n",
    "    \n",
    "    # ---- æ·»åŠ  regime èƒŒæ™¯è‰²å— ----\n",
    "    regime_colors = {\n",
    "        -1: \"#ffcccc\",  # DOWN - æµ…çº¢è‰²\n",
    "        0: \"#ffffcc\",   # RANGE - æµ…é»„è‰²\n",
    "        1: \"#ccffcc\",   # UP - æµ…ç»¿è‰²\n",
    "    }\n",
    "    \n",
    "    # æ‰¾å‡ºè¿ç»­ç›¸åŒ regime çš„åŒºé—´\n",
    "    df_day[\"regime_filled\"] = df_day[\"regime\"].fillna(0).astype(int)\n",
    "    df_day[\"regime_change\"] = (df_day[\"regime_filled\"] != df_day[\"regime_filled\"].shift()).cumsum()\n",
    "    \n",
    "    regime_groups = df_day.groupby(\"regime_change\").agg({\n",
    "        \"timestamp_dt\": [\"first\", \"last\"],\n",
    "        \"regime_filled\": \"first\"\n",
    "    })\n",
    "    regime_groups.columns = [\"start_time\", \"end_time\", \"regime\"]\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ª regime åŒºé—´æ·»åŠ èƒŒæ™¯\n",
    "    for _, row in regime_groups.iterrows():\n",
    "        regime = int(row[\"regime\"])\n",
    "        if regime in regime_colors:\n",
    "            start = row[\"start_time\"]\n",
    "            end = row[\"end_time\"] + pd.Timedelta(minutes=5)\n",
    "            \n",
    "            box = BoxAnnotation(\n",
    "                left=start,\n",
    "                right=end,\n",
    "                fill_color=regime_colors[regime],\n",
    "                fill_alpha=0.4,\n",
    "                level=\"underlay\",\n",
    "            )\n",
    "            p.add_layout(box)\n",
    "    \n",
    "    # ---- ç»˜åˆ¶ K çº¿ ----\n",
    "    # å½±çº¿\n",
    "    p.segment(\n",
    "        x0=\"timestamp\", y0=\"high\",\n",
    "        x1=\"timestamp\", y1=\"low\",\n",
    "        source=source,\n",
    "        color=\"black\",\n",
    "        line_width=1,\n",
    "    )\n",
    "    \n",
    "    # K çº¿å®ä½“\n",
    "    p.vbar(\n",
    "        x=\"timestamp\",\n",
    "        width=w,\n",
    "        top=\"open\",\n",
    "        bottom=\"close\",\n",
    "        source=source,\n",
    "        fill_color=\"color\",\n",
    "        line_color=\"black\",\n",
    "        line_width=0.5,\n",
    "    )\n",
    "    \n",
    "    # ---- æ·»åŠ  HoverTool ----\n",
    "    hover = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"æ—¶é—´\", \"@timestamp{%Y-%m-%d %H:%M}\"),\n",
    "            (\"å¼€\", \"@open{0.2f}\"),\n",
    "            (\"é«˜\", \"@high{0.2f}\"),\n",
    "            (\"ä½\", \"@low{0.2f}\"),\n",
    "            (\"æ”¶\", \"@close{0.2f}\"),\n",
    "            (\"Regime\", \"@regime (-1=DOWN, 0=RANGE, 1=UP)\"),\n",
    "        ],\n",
    "        formatters={\"@timestamp\": \"datetime\"},\n",
    "        mode=\"vline\",\n",
    "    )\n",
    "    p.add_tools(hover)\n",
    "    \n",
    "    # è®¾ç½®è½´æ ‡ç­¾\n",
    "    p.xaxis.axis_label = \"Time\"\n",
    "    p.yaxis.axis_label = \"Price\"\n",
    "    p.xaxis.major_label_orientation = 0.8\n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "def plot_multiday_kline(df_multi: pd.DataFrame, title: str = \"Multi-Day K-line\", \n",
    "                        width: int = 2400, height: int = 600) -> figure:\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶è¿ç»­å¤šæ—¥ K çº¿å›¾ï¼Œç§»é™¤ä¼‘ç›˜æ—¶é—´çš„ç©ºéš™\n",
    "    ä½¿ç”¨ index-based x è½´\n",
    "    \"\"\"\n",
    "    df = df_multi.copy().reset_index(drop=True)\n",
    "    df[\"idx\"] = df.index\n",
    "    \n",
    "    # K çº¿æ¶¨è·Œé¢œè‰²\n",
    "    df[\"color\"] = df.apply(\n",
    "        lambda row: \"#26a69a\" if row[\"close\"] >= row[\"open\"] else \"#ef5350\",\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # æ ¼å¼åŒ–æ—¶é—´æ ‡ç­¾\n",
    "    df[\"time_label\"] = df[\"timestamp_dt\"].dt.strftime(\"%m-%d %H:%M\")\n",
    "    df[\"date_str\"] = df[\"date\"].astype(str)\n",
    "    \n",
    "    bar_width = 0.6\n",
    "    \n",
    "    # ColumnDataSource\n",
    "    source = ColumnDataSource(data={\n",
    "        \"idx\": df[\"idx\"],\n",
    "        \"timestamp\": df[\"timestamp_dt\"],\n",
    "        \"open\": df[\"open\"],\n",
    "        \"high\": df[\"high\"],\n",
    "        \"low\": df[\"low\"],\n",
    "        \"close\": df[\"close\"],\n",
    "        \"color\": df[\"color\"],\n",
    "        \"regime\": df[\"regime\"].fillna(0).astype(int),\n",
    "        \"date_str\": df[\"date_str\"],\n",
    "        \"time_label\": df[\"time_label\"],\n",
    "    })\n",
    "    \n",
    "    # Y è½´èŒƒå›´\n",
    "    y_min = df[\"low\"].min() * 0.998\n",
    "    y_max = df[\"high\"].max() * 1.002\n",
    "    \n",
    "    # åˆ›å»º figure\n",
    "    p = figure(\n",
    "        title=title,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        y_range=Range1d(y_min, y_max),\n",
    "        tools=\"pan,wheel_zoom,box_zoom,reset,save,crosshair\",\n",
    "        background_fill_color=\"#fafafa\",\n",
    "    )\n",
    "    \n",
    "    p.x_range.range_padding = 0.01\n",
    "    \n",
    "    # ---- æ·»åŠ  regime èƒŒæ™¯è‰²å— ----\n",
    "    regime_colors = {\n",
    "        -1: \"#ffcccc\",  # DOWN\n",
    "        0: \"#ffffcc\",   # RANGE\n",
    "        1: \"#ccffcc\",   # UP\n",
    "    }\n",
    "    \n",
    "    df[\"regime_filled\"] = df[\"regime\"].fillna(0).astype(int)\n",
    "    df[\"regime_change\"] = (df[\"regime_filled\"] != df[\"regime_filled\"].shift()).cumsum()\n",
    "    \n",
    "    regime_groups = df.groupby(\"regime_change\").agg({\n",
    "        \"idx\": [\"first\", \"last\"],\n",
    "        \"regime_filled\": \"first\"\n",
    "    })\n",
    "    regime_groups.columns = [\"start_idx\", \"end_idx\", \"regime\"]\n",
    "    \n",
    "    for _, row in regime_groups.iterrows():\n",
    "        regime = int(row[\"regime\"])\n",
    "        if regime in regime_colors:\n",
    "            start = row[\"start_idx\"] - 0.5\n",
    "            end = row[\"end_idx\"] + 0.5\n",
    "            \n",
    "            box = BoxAnnotation(\n",
    "                left=start,\n",
    "                right=end,\n",
    "                fill_color=regime_colors[regime],\n",
    "                fill_alpha=0.4,\n",
    "                level=\"underlay\",\n",
    "            )\n",
    "            p.add_layout(box)\n",
    "    \n",
    "    # ---- æ·»åŠ æ—¥æœŸåˆ†å‰²çº¿ ----\n",
    "    unique_dates = df[\"date\"].unique()\n",
    "    for i, date in enumerate(unique_dates):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        \n",
    "        first_bar_idx = df[df[\"date\"] == date][\"idx\"].min()\n",
    "        \n",
    "        vline = Span(\n",
    "            location=first_bar_idx - 0.5,\n",
    "            dimension=\"height\",\n",
    "            line_color=\"gray\",\n",
    "            line_dash=\"dashed\",\n",
    "            line_width=1.5,\n",
    "            line_alpha=0.7,\n",
    "        )\n",
    "        p.add_layout(vline)\n",
    "        \n",
    "        label = Label(\n",
    "            x=first_bar_idx + 5,\n",
    "            y=y_max * 0.999,\n",
    "            text=str(date),\n",
    "            text_font_size=\"10pt\",\n",
    "            text_color=\"gray\",\n",
    "            text_align=\"left\",\n",
    "        )\n",
    "        p.add_layout(label)\n",
    "    \n",
    "    # ---- ç»˜åˆ¶ K çº¿ ----\n",
    "    p.segment(\n",
    "        x0=\"idx\", y0=\"high\",\n",
    "        x1=\"idx\", y1=\"low\",\n",
    "        source=source,\n",
    "        color=\"black\",\n",
    "        line_width=1,\n",
    "    )\n",
    "    \n",
    "    p.vbar(\n",
    "        x=\"idx\",\n",
    "        width=bar_width,\n",
    "        top=\"open\",\n",
    "        bottom=\"close\",\n",
    "        source=source,\n",
    "        fill_color=\"color\",\n",
    "        line_color=\"black\",\n",
    "        line_width=0.5,\n",
    "    )\n",
    "    \n",
    "    # ---- X è½´æ ‡ç­¾ ----\n",
    "    tick_interval = 12  # çº¦ 1 å°æ—¶\n",
    "    tick_indices = list(range(0, len(df), tick_interval))\n",
    "    \n",
    "    p.xaxis.ticker = tick_indices\n",
    "    p.xaxis.major_label_overrides = {\n",
    "        i: df.loc[i, \"time_label\"] for i in tick_indices if i < len(df)\n",
    "    }\n",
    "    p.xaxis.major_label_orientation = 0.8\n",
    "    \n",
    "    # ---- HoverTool ----\n",
    "    hover = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"æ—¥æœŸ\", \"@date_str\"),\n",
    "            (\"æ—¶é—´\", \"@time_label\"),\n",
    "            (\"å¼€\", \"@open{0.2f}\"),\n",
    "            (\"é«˜\", \"@high{0.2f}\"),\n",
    "            (\"ä½\", \"@low{0.2f}\"),\n",
    "            (\"æ”¶\", \"@close{0.2f}\"),\n",
    "            (\"Regime\", \"@regime (-1=DOWN, 0=RANGE, 1=UP)\"),\n",
    "        ],\n",
    "        mode=\"vline\",\n",
    "    )\n",
    "    p.add_tools(hover)\n",
    "    \n",
    "    p.xaxis.axis_label = \"Time\"\n",
    "    p.yaxis.axis_label = \"Price\"\n",
    "    p.title.text_font_size = \"14pt\"\n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "print(\"Bokeh ç»‘å®šå‡½æ•°å·²å®šä¹‰:\")\n",
    "print(\"  - plot_kline_with_regime(df_day, title)\")\n",
    "print(\"  - plot_multiday_kline(df_multi, title, width, height)\")\n",
    "print(\"\\nCell 11 å®Œæˆ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 12: éšæœºé€‰å–å•æ—¥ Kçº¿å›¾å¯è§†åŒ– ==========\n",
    "import random\n",
    "\n",
    "# å‡†å¤‡ç»˜å›¾æ•°æ®\n",
    "df_plot = df.copy().reset_index()\n",
    "df_plot[\"timestamp_dt\"] = pd.to_datetime(df_plot[\"timestamp\"].astype(str).str.slice(0, 19))\n",
    "df_plot[\"date\"] = df_plot[\"timestamp_dt\"].dt.date\n",
    "\n",
    "# è·å–æ‰€æœ‰æœ‰æ•ˆçš„äº¤æ˜“æ—¥ (æœ‰ regime æ ‡ç­¾)\n",
    "valid_dates = df_plot[df_plot[\"regime\"].notna()][\"date\"].unique()\n",
    "print(f\"æœ‰æ•ˆäº¤æ˜“æ—¥æ€»æ•°: {len(valid_dates)}\")\n",
    "\n",
    "# éšæœºé€‰å– 10 ä¸ªäº¤æ˜“æ—¥\n",
    "NUM_DAYS = 10\n",
    "random.seed(42)\n",
    "selected_dates = sorted(random.sample(list(valid_dates), min(NUM_DAYS, len(valid_dates))))\n",
    "\n",
    "print(f\"\\né€‰å–çš„ {len(selected_dates)} ä¸ªäº¤æ˜“æ—¥:\")\n",
    "for d in selected_dates:\n",
    "    print(f\"  - {d}\")\n",
    "\n",
    "\n",
    "# ç»˜åˆ¶æ¯å¤©çš„å›¾è¡¨\n",
    "plots = []\n",
    "for date in selected_dates:\n",
    "    df_day = df_plot[df_plot[\"date\"] == date].copy()\n",
    "    \n",
    "    if len(df_day) == 0:\n",
    "        print(f\"  [è·³è¿‡] {date}: æ— æ•°æ®\")\n",
    "        continue\n",
    "    \n",
    "    # ç»Ÿè®¡å½“å¤©çš„ regime åˆ†å¸ƒ\n",
    "    regime_counts = df_day[\"regime\"].value_counts()\n",
    "    regime_str = \", \".join([f\"{k}:{v}\" for k, v in sorted(regime_counts.items())])\n",
    "    \n",
    "    title = f\"ES 5min - {date} | Regimeåˆ†å¸ƒ: {regime_str} | DOWN=æµ…çº¢, RANGE=æµ…é»„, UP=æµ…ç»¿\"\n",
    "    p = plot_kline_with_regime(df_day, title=title)\n",
    "    plots.append(p)\n",
    "    print(f\"  [å®Œæˆ] {date}: {len(df_day)} æ ¹ K çº¿\")\n",
    "\n",
    "# æ˜¾ç¤ºå›¾è¡¨\n",
    "if plots:\n",
    "    layout = column(*plots)\n",
    "    show(layout)\n",
    "    \n",
    "    # ä¿å­˜ HTML åˆ° charts ç›®å½•\n",
    "    chart_path = OUTPUT_DIR_CHARTS / \"market_cycle_random_days.html\"\n",
    "    output_file(str(chart_path))\n",
    "    save(layout, filename=str(chart_path), title=\"Market Cycle - Random Days\")\n",
    "    print(f\"\\nå›¾è¡¨å·²ä¿å­˜åˆ°: {chart_path}\")\n",
    "\n",
    "print(\"\\nCell 12 å®Œæˆ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 13: è¿ç»­å¤šæ—¥ Kçº¿å›¾å¯è§†åŒ– ==========\n",
    "\n",
    "# é…ç½®\n",
    "NUM_CONSECUTIVE_DAYS = 10\n",
    "CHART_WIDTH = 2400\n",
    "CHART_HEIGHT = 600\n",
    "\n",
    "# å¯ä»¥æŒ‡å®šèµ·å§‹æ—¥æœŸï¼Œæˆ–éšæœºé€‰æ‹©\n",
    "START_MODE = \"random\"  # \"random\" æˆ– \"specific\"\n",
    "SPECIFIC_START_DATE = \"2023-10-01\"\n",
    "\n",
    "# è·å–æœ‰æ•ˆäº¤æ˜“æ—¥åˆ—è¡¨\n",
    "valid_dates_sorted = sorted(df_plot[df_plot[\"regime\"].notna()][\"date\"].unique())\n",
    "print(f\"æœ‰æ•ˆäº¤æ˜“æ—¥æ€»æ•°: {len(valid_dates_sorted)}\")\n",
    "\n",
    "# é€‰æ‹©è¿ç»­çš„äº¤æ˜“æ—¥\n",
    "if START_MODE == \"specific\":\n",
    "    from datetime import datetime\n",
    "    target_start = datetime.strptime(SPECIFIC_START_DATE, \"%Y-%m-%d\").date()\n",
    "    if target_start in valid_dates_sorted:\n",
    "        start_idx = valid_dates_sorted.index(target_start)\n",
    "    else:\n",
    "        later_dates = [d for d in valid_dates_sorted if d >= target_start]\n",
    "        start_idx = valid_dates_sorted.index(later_dates[0]) if later_dates else 0\n",
    "    print(f\"æŒ‡å®šèµ·å§‹æ—¥æœŸ: {SPECIFIC_START_DATE}\")\n",
    "else:\n",
    "    random.seed(123)\n",
    "    max_start_idx = len(valid_dates_sorted) - NUM_CONSECUTIVE_DAYS\n",
    "    start_idx = random.randint(0, max(0, max_start_idx))\n",
    "    print(f\"éšæœºé€‰æ‹©èµ·å§‹ç´¢å¼•: {start_idx}\")\n",
    "\n",
    "end_idx = min(start_idx + NUM_CONSECUTIVE_DAYS, len(valid_dates_sorted))\n",
    "consecutive_dates = valid_dates_sorted[start_idx:end_idx]\n",
    "\n",
    "print(f\"\\né€‰å–çš„ {len(consecutive_dates)} ä¸ªè¿ç»­äº¤æ˜“æ—¥:\")\n",
    "print(f\"  èµ·å§‹: {consecutive_dates[0]}\")\n",
    "print(f\"  ç»“æŸ: {consecutive_dates[-1]}\")\n",
    "\n",
    "# ç­›é€‰æ•°æ®\n",
    "df_multi = df_plot[df_plot[\"date\"].isin(consecutive_dates)].copy()\n",
    "df_multi = df_multi.sort_values(\"timestamp_dt\").reset_index(drop=True)\n",
    "print(f\"\\næ€» K çº¿æ•°: {len(df_multi)}\")\n",
    "\n",
    "# ç»Ÿè®¡ regime åˆ†å¸ƒ\n",
    "regime_multi_counts = df_multi[\"regime\"].value_counts().sort_index()\n",
    "print(\"\\nè¿ç»­å¤šæ—¥ Regime åˆ†å¸ƒ:\")\n",
    "for k, v in regime_multi_counts.items():\n",
    "    label_name = {-1: \"DOWN (-1)\", 0: \"RANGE (0)\", 1: \"UP (+1)\"}.get(k, str(k))\n",
    "    print(f\"  {label_name}: {v} ({v / len(df_multi):.2%})\")\n",
    "\n",
    "# ç»˜åˆ¶å¤šæ—¥è¿ç»­å›¾\n",
    "title = f\"ES 5min ({consecutive_dates[0]} ~ {consecutive_dates[-1]}) | DOWN=æµ…çº¢, RANGE=æµ…é»„, UP=æµ…ç»¿\"\n",
    "p_multi = plot_multiday_kline(df_multi, title=title, width=CHART_WIDTH, height=CHART_HEIGHT)\n",
    "\n",
    "show(p_multi)\n",
    "\n",
    "# ä¿å­˜ HTML åˆ° charts ç›®å½•\n",
    "chart_path = OUTPUT_DIR_CHARTS / \"market_cycle_multiday.html\"\n",
    "output_file(str(chart_path))\n",
    "save(p_multi, filename=str(chart_path), title=\"Market Cycle - Multi-Day\")\n",
    "print(f\"\\nè¿ç»­å¤šæ—¥å›¾è¡¨å·²ä¿å­˜åˆ°: {chart_path}\")\n",
    "\n",
    "print(\"\\nCell 13 å®Œæˆ âœ…\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 14: è‡ªå®šä¹‰æ—¥æœŸ Kçº¿å›¾å¯è§†åŒ– ==========\n",
    "# ä¿®æ”¹ custom_dates åˆ—è¡¨æ¥æŸ¥çœ‹ç‰¹å®šæ—¥æœŸ\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "custom_dates = [\n",
    "    \"2023-03-29\",\n",
    "    \"2023-07-20\",\n",
    "    \"2024-02-19\",\n",
    "    \"2024-03-13\",\n",
    "    \"2025-09-29\",\n",
    "]\n",
    "\n",
    "# è½¬æ¢ä¸º date ç±»å‹\n",
    "custom_dates_parsed = [datetime.strptime(d, \"%Y-%m-%d\").date() for d in custom_dates]\n",
    "\n",
    "# è¿‡æ»¤æœ‰æ•ˆæ—¥æœŸ\n",
    "valid_custom_dates = [d for d in custom_dates_parsed if d in set(valid_dates_sorted)]\n",
    "invalid_dates = [d for d in custom_dates_parsed if d not in set(valid_dates_sorted)]\n",
    "\n",
    "if invalid_dates:\n",
    "    print(f\"[è­¦å‘Š] ä»¥ä¸‹æ—¥æœŸæ— æœ‰æ•ˆæ•°æ®ï¼Œå°†è¢«è·³è¿‡: {invalid_dates}\")\n",
    "\n",
    "if not valid_custom_dates:\n",
    "    print(\"[é”™è¯¯] æ²¡æœ‰æœ‰æ•ˆçš„è‡ªå®šä¹‰æ—¥æœŸå¯ç»˜åˆ¶ã€‚\")\n",
    "else:\n",
    "    print(f\"å°†ç»˜åˆ¶ä»¥ä¸‹ {len(valid_custom_dates)} ä¸ªè‡ªå®šä¹‰æ—¥æœŸ:\")\n",
    "    for d in valid_custom_dates:\n",
    "        print(f\"  - {d}\")\n",
    "    \n",
    "    # ç»˜åˆ¶å›¾è¡¨\n",
    "    custom_plots = []\n",
    "    for date in valid_custom_dates:\n",
    "        df_day = df_plot[df_plot[\"date\"] == date].copy()\n",
    "        \n",
    "        if len(df_day) == 0:\n",
    "            print(f\"  [è·³è¿‡] {date}: æ— æ•°æ®\")\n",
    "            continue\n",
    "        \n",
    "        # ç»Ÿè®¡å½“å¤©çš„ regime åˆ†å¸ƒ\n",
    "        regime_counts = df_day[\"regime\"].value_counts()\n",
    "        regime_str = \", \".join([f\"{k}:{v}\" for k, v in sorted(regime_counts.items())])\n",
    "        \n",
    "        title = f\"ES 5min - {date} | Regimeåˆ†å¸ƒ: {regime_str} | DOWN=æµ…çº¢, RANGE=æµ…é»„, UP=æµ…ç»¿\"\n",
    "        p = plot_kline_with_regime(df_day, title=title)\n",
    "        custom_plots.append(p)\n",
    "        print(f\"  [å®Œæˆ] {date}: {len(df_day)} æ ¹ K çº¿\")\n",
    "    \n",
    "    if custom_plots:\n",
    "        custom_layout = column(*custom_plots)\n",
    "        show(custom_layout)\n",
    "        \n",
    "        # ä¿å­˜ HTML åˆ° charts ç›®å½•\n",
    "        chart_path = OUTPUT_DIR_CHARTS / \"market_cycle_custom_dates.html\"\n",
    "        output_file(str(chart_path))\n",
    "        save(custom_layout, filename=str(chart_path), title=\"Market Cycle - Custom Dates\")\n",
    "        print(f\"\\nè‡ªå®šä¹‰æ—¥æœŸå›¾è¡¨å·²ä¿å­˜åˆ°: {chart_path}\")\n",
    "\n",
    "print(\"\\nCell 14 å®Œæˆ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 15: å¯¼å‡ºæ ‡æ³¨æ•°æ®ä¸ç‰¹å¾çŸ©é˜µ ==========\n",
    "\n",
    "# é€‰æ‹©è¦å¯¼å‡ºçš„ç‰¹å¾åˆ—\n",
    "feature_cols = [\n",
    "    \"log_return\",       # å¯¹æ•°æ”¶ç›Š\n",
    "    \"atr\",              # ATR\n",
    "    \"range_atr\",        # ç›¸å¯¹åŒºé—´\n",
    "    \"er\",               # æ•ˆç‡å› å­\n",
    "    \"kama\",             # KAMA\n",
    "    \"slope_norm\",       # KAMA æ–œç‡æ ‡å‡†åŒ–\n",
    "    \"dist_norm\",        # ä»·æ ¼åç¦»åº¦æ ‡å‡†åŒ–\n",
    "    \"beta\",             # å›å½’æ–œç‡\n",
    "    \"r2\",               # RÂ²\n",
    "    \"chop\",             # Choppiness Index (åŸºäºATR)\n",
    "    \"overlap_ratio\",    # Bar é‡å åº¦\n",
    "    \"d_barrier\",        # Triple Barrier æ–¹å‘æ ‡ç­¾\n",
    "]\n",
    "\n",
    "# é¢å¤–çš„ä»·æ ¼å’Œæ—¶é—´ç‰¹å¾\n",
    "extra_cols = [\n",
    "    \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "]\n",
    "\n",
    "# æ ‡ç­¾åˆ—\n",
    "label_cols = [\n",
    "    \"regime\",           # æœ€ç»ˆå¸‚åœºå‘¨æœŸæ ‡ç­¾\n",
    "    \"regime_raw\",       # åŸå§‹æ ‡ç­¾ (æœªå¹³æ»‘)\n",
    "    \"is_range\",         # æ˜¯å¦ä¸ºéœ‡è¡åŒºé—´ (å¸ƒå°”)\n",
    "    \"is_range_shrunk\",  # æ”¶ç¼©åçš„éœ‡è¡åŒºé—´\n",
    "]\n",
    "\n",
    "# æ„å»ºå¯¼å‡º DataFrame\n",
    "df_export = df[extra_cols + feature_cols + label_cols].copy()\n",
    "df_export = df_export.reset_index()\n",
    "\n",
    "# æ·»åŠ å¹´ä»½å’Œæ—¥æœŸåˆ—\n",
    "df_export[\"timestamp_dt\"] = pd.to_datetime(df_export[\"timestamp\"].astype(str).str.slice(0, 19))\n",
    "df_export[\"year\"] = df_export[\"timestamp_dt\"].dt.year\n",
    "df_export[\"date\"] = df_export[\"timestamp_dt\"].dt.date\n",
    "\n",
    "# è¿‡æ»¤æ‰ NaN æ ·æœ¬ (warm-up æœŸ)\n",
    "df_valid = df_export.dropna(subset=[\"regime\"])\n",
    "print(f\"æœ‰æ•ˆæ ·æœ¬æ•°: {len(df_valid)} / {len(df_export)} ({len(df_valid) / len(df_export):.2%})\")\n",
    "\n",
    "# ç»Ÿè®¡æœ€ç»ˆæ ‡ç­¾åˆ†å¸ƒ\n",
    "final_counts = df_valid[\"regime\"].value_counts().sort_index()\n",
    "print(\"\\n=== æœ€ç»ˆå¯¼å‡ºæ•°æ®çš„ Regime åˆ†å¸ƒ ===\")\n",
    "for k, v in final_counts.items():\n",
    "    label_name = {-1: \"DOWN (-1)\", 0: \"RANGE (0)\", 1: \"UP (+1)\"}.get(k, str(k))\n",
    "    print(f\"  {label_name}: {v:,} ({v / len(df_valid):.2%})\")\n",
    "\n",
    "# æŒ‰å¹´ä»½ç»Ÿè®¡\n",
    "print(\"\\n=== æŒ‰å¹´ä»½ç»Ÿè®¡ ===\")\n",
    "year_stats = df_valid.groupby(\"year\")[\"regime\"].value_counts().unstack(fill_value=0)\n",
    "year_stats[\"total\"] = year_stats.sum(axis=1)\n",
    "display(year_stats)\n",
    "\n",
    "# ä¿å­˜ä¸º CSV å’Œ Parquet åˆ° data ç›®å½•\n",
    "OUTPUT_CSV = OUTPUT_DIR_DATA / \"market_cycle_labeled_data.csv\"\n",
    "OUTPUT_PARQUET = OUTPUT_DIR_DATA / \"market_cycle_labeled_data.parquet\"\n",
    "\n",
    "df_valid.to_csv(OUTPUT_CSV, index=False)\n",
    "df_valid.to_parquet(OUTPUT_PARQUET, index=False)\n",
    "\n",
    "print(f\"\\næ•°æ®å·²å¯¼å‡º:\")\n",
    "print(f\"  - CSV: {OUTPUT_CSV}\")\n",
    "print(f\"  - Parquet: {OUTPUT_PARQUET}\")\n",
    "\n",
    "# æ„å»ºç”¨äºæ¨¡å‹è®­ç»ƒçš„ç‰¹å¾çŸ©é˜µ X å’Œæ ‡ç­¾ y\n",
    "X = df_valid[feature_cols].copy()\n",
    "y = df_valid[\"regime\"].astype(int).copy()\n",
    "\n",
    "print(f\"\\n=== æ¨¡å‹è®­ç»ƒæ•°æ® ===\")\n",
    "print(f\"ç‰¹å¾çŸ©é˜µ X shape: {X.shape}\")\n",
    "print(f\"æ ‡ç­¾ y shape: {y.shape}\")\n",
    "print(f\"ç‰¹å¾åˆ—: {feature_cols}\")\n",
    "\n",
    "# æ£€æŸ¥ç‰¹å¾çš„åŸºæœ¬ç»Ÿè®¡\n",
    "print(\"\\nç‰¹å¾ç»Ÿè®¡:\")\n",
    "display(X.describe())\n",
    "\n",
    "print(\"\\nCell 15 å®Œæˆ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 16: ç‰¹å¾å½’ä¸€åŒ– (Rolling Z-Score Normalization) ==========\n",
    "'''\n",
    "ğŸ“Œ å½’ä¸€åŒ–ç­–ç•¥:\n",
    "   1. å¯¹ç»å¯¹é‡çº²ç‰¹å¾ (atr, tr, bar_range, volume) è¿›è¡Œæ»šåŠ¨ Z-score å½’ä¸€åŒ–\n",
    "   2. å·²ç»æ˜¯æ¯”ä¾‹/æœ‰ç•Œçš„ç‰¹å¾ (er, chop, body_ratio ç­‰) ä¸éœ€è¦é¢å¤–å¤„ç†\n",
    "   3. åˆ é™¤åŸå§‹å°ºåº¦ç‰¹å¾ï¼Œåªä¿ç•™å½’ä¸€åŒ–ç‰ˆæœ¬\n",
    "   \n",
    "ğŸ“Œ è®¾è®¡åŸåˆ™:\n",
    "   - æ¯ä¸ª t åªä½¿ç”¨è¿‡å» W æ ¹ bar çš„ç»Ÿè®¡é‡ï¼Œä¸çœ‹æœªæ¥\n",
    "   - Ïƒ å¾ˆå°æ—¶ (< eps) ç½®ä¸º 0ï¼Œé¿å…æ•°å€¼çˆ†ç‚¸\n",
    "   - å‰ W-1 æ ¹ bar çš„å½’ä¸€åŒ–å€¼è®¾ä¸º NaNï¼Œåç»­è¿‡æ»¤æ‰\n",
    "'''\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”§ ç‰¹å¾å½’ä¸€åŒ–å¤„ç†\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# æ»šåŠ¨å½’ä¸€åŒ–å‚æ•°\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "NORM_WINDOW = 100  # æ»šåŠ¨çª—å£å¤§å° (~8å°æ—¶ RTH)\n",
    "EPS = 1e-8         # é¿å…é™¤é›¶\n",
    "\n",
    "# ========== 1. å®šä¹‰éœ€è¦å½’ä¸€åŒ–çš„ç‰¹å¾ ==========\n",
    "RAW_SCALE_FEATURES = [\"atr\", \"tr\", \"bar_range\", \"volume\"]\n",
    "\n",
    "# ========== 2. æ»šåŠ¨ Z-Score å½’ä¸€åŒ–å‡½æ•° ==========\n",
    "def rolling_zscore(series: pd.Series, window: int, eps: float = 1e-8) -> pd.Series:\n",
    "    '''\n",
    "    æ»šåŠ¨ Z-Score å½’ä¸€åŒ–\n",
    "    z_t = (x_t - Î¼_t) / Ïƒ_t\n",
    "    å…¶ä¸­ Î¼_t, Ïƒ_t æ˜¯è¿‡å» window æ ¹ bar çš„å‡å€¼å’Œæ ‡å‡†å·®\n",
    "    '''\n",
    "    roll_mean = series.rolling(window=window, min_periods=window).mean()\n",
    "    roll_std = series.rolling(window=window, min_periods=window).std()\n",
    "    roll_std = roll_std.replace(0, np.nan).clip(lower=eps)\n",
    "    zscore = (series - roll_mean) / roll_std\n",
    "    return zscore\n",
    "\n",
    "# ========== 3. å¯¹ç»å¯¹é‡çº²ç‰¹å¾è¿›è¡Œæ»šåŠ¨å½’ä¸€åŒ– ==========\n",
    "print(\"\\nå¯¹ç»å¯¹é‡çº²ç‰¹å¾è¿›è¡Œæ»šåŠ¨ Z-Score å½’ä¸€åŒ–...\")\n",
    "\n",
    "# âœ… ç¡®ä¿ bar_range å·²å®šä¹‰ï¼ˆé¿å…ä¾èµ– Cell 18ï¼‰\n",
    "if \"bar_range\" not in df.columns:\n",
    "    df[\"bar_range\"] = df[\"high\"] - df[\"low\"]\n",
    "\n",
    "\n",
    "for feat in RAW_SCALE_FEATURES:\n",
    "    if feat in df.columns:\n",
    "        norm_col = f\"{feat}_z\"\n",
    "        df[norm_col] = rolling_zscore(df[feat], NORM_WINDOW, EPS)\n",
    "        valid_count = df[norm_col].notna().sum()\n",
    "        print(f\"  {feat} â†’ {norm_col}: mean={df[norm_col].mean():.4f}, std={df[norm_col].std():.4f}, valid={valid_count:,}\")\n",
    "\n",
    "# ========== 4. KAMA ç‰¹æ®Šå¤„ç† ==========\n",
    "if \"kama\" in df.columns:\n",
    "    df[\"kama_z\"] = rolling_zscore(df[\"kama\"], NORM_WINDOW, EPS)\n",
    "    print(f\"  kama â†’ kama_z: mean={df['kama_z'].mean():.4f}, std={df['kama_z'].std():.4f}\")\n",
    "\n",
    "# ========== 5. Volume ç‰¹æ®Šå¤„ç† ==========\n",
    "if \"volume\" in df.columns:\n",
    "    df[\"log_volume\"] = np.log1p(df[\"volume\"])\n",
    "    df[\"log_volume_z\"] = rolling_zscore(df[\"log_volume\"], NORM_WINDOW, EPS)\n",
    "    print(f\"  volume â†’ log_volume_z: mean={df['log_volume_z'].mean():.4f}, std={df['log_volume_z'].std():.4f}\")\n",
    "\n",
    "# ========== 6. ç»Ÿè®¡å½’ä¸€åŒ–åçš„ç‰¹å¾åˆ†å¸ƒ ==========\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“Š å½’ä¸€åŒ–ç‰¹å¾ç»Ÿè®¡\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "new_norm_features = [c for c in df.columns if c.endswith(\"_z\") and c not in [\"range_z\", \"range_z_ma5\"]]\n",
    "print(f\"\\næ–°å¢å½’ä¸€åŒ–ç‰¹å¾: {len(new_norm_features)}\")\n",
    "for col in new_norm_features[:10]:\n",
    "    stats = df[col].describe()\n",
    "    print(f\"  {col}: min={stats['min']:.2f}, max={stats['max']:.2f}, mean={stats['mean']:.4f}\")\n",
    "\n",
    "# ========== 7. å®šä¹‰è¦ä»è®­ç»ƒä¸­æ’é™¤çš„åŸå§‹å°ºåº¦ç‰¹å¾ ==========\n",
    "FEATURES_TO_EXCLUDE_FROM_TRAINING = [\n",
    "    \"kama\",       # åŸå§‹ä»·æ ¼å°ºåº¦ â†’ ä½¿ç”¨ dist_norm æˆ– kama_z\n",
    "    \"atr\",        # åŸå§‹æ³¢åŠ¨å¹…åº¦ â†’ ä½¿ç”¨ range_atr æˆ– atr_z  \n",
    "    \"tr\",         # åŸå§‹ True Range â†’ ä½¿ç”¨ atr_z\n",
    "    \"bar_range\",  # åŸå§‹ bar é«˜ä½ä»·å·® â†’ ä½¿ç”¨ range_z\n",
    "    \"d_barrier\",  # æ³„éœ²æœªæ¥ä¿¡æ¯\n",
    "    \"regime_raw\",\n",
    "    \"is_range\",\n",
    "    \"is_range_extended\", # ç°åœ¨æ”¹ç”¨å†…ç¼©å‡½æ•°äº†ï¼Œä¸ä¼šå†å‘å¤–å»¶ä¼¸\n",
    "    \"is_range_shrunk\",\n",
    "    \"is_range_in_trend\",\n",
    "]\n",
    "\n",
    "print(f\"\\nâš ï¸ ä»¥ä¸‹ç‰¹å¾å°†ä»è®­ç»ƒé›†ä¸­æ’é™¤:\")\n",
    "for f in FEATURES_TO_EXCLUDE_FROM_TRAINING:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(\"\\nCell 16 å®Œæˆ: ç‰¹å¾å½’ä¸€åŒ– âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 17: å¤šå°ºåº¦è¶‹åŠ¿ç‰¹å¾ (Multi-Scale Trend Features) ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ 1.1(a) å¤šå°ºåº¦è¶‹åŠ¿å¼ºåº¦ / \"è¶‹åŠ¿èƒ½é‡\"\n",
    "   - ä¸åŒçª—å£çš„ log_close æ–œç‡: slope_short (10), slope_mid (30), slope_long (60)\n",
    "   - å¤šå°ºåº¦ä¸€è‡´æ€§æŒ‡æ ‡: trend_alignment = sign(slope_10) + sign(slope_30) + sign(slope_60)\n",
    "     - æ¥è¿‘ +3/-3: å¤šå°ºåº¦è¶‹åŠ¿åŒå‘\n",
    "     - æ¥è¿‘ 0: åˆ†æ­§å¤šï¼Œç»“æ„ä¸æ¸…æ™°ï¼ˆæ›´åƒ trading range è¾¹ç¼˜ï¼‰\n",
    "\"\"\"\n",
    "print(\"è®¡ç®—å¤šå°ºåº¦è¶‹åŠ¿ç‰¹å¾...\")\n",
    "\n",
    "# å¤šå°ºåº¦æ–œç‡çª—å£å®šä¹‰\n",
    "SLOPE_WINDOWS = [10, 30, 60]\n",
    "\n",
    "def calc_rolling_slope(log_close: pd.Series, window: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ»šåŠ¨çº¿æ€§å›å½’æ–œç‡ (å¯¹ log_close)\n",
    "    ä½¿ç”¨ç®€åŒ–çš„æœ€å°äºŒä¹˜æ³•\n",
    "    \"\"\"\n",
    "    def linreg_slope(y):\n",
    "        if len(y) < window or np.any(np.isnan(y)):\n",
    "            return np.nan\n",
    "        x = np.arange(len(y))\n",
    "        x_mean = x.mean()\n",
    "        y_mean = y.mean()\n",
    "        ss_xy = ((x - x_mean) * (y - y_mean)).sum()\n",
    "        ss_x = ((x - x_mean) ** 2).sum()\n",
    "        return ss_xy / ss_x if ss_x > 0 else 0\n",
    "    \n",
    "    return log_close.rolling(window=window).apply(linreg_slope, raw=True)\n",
    "\n",
    "\n",
    "# è®¡ç®—ä¸åŒçª—å£çš„æ–œç‡\n",
    "for w in SLOPE_WINDOWS:\n",
    "    col_name = f\"slope_{w}\"\n",
    "    df[col_name] = calc_rolling_slope(df[\"log_close\"], w)\n",
    "    # æ ‡å‡†åŒ– (ç›¸å¯¹ ATR)\n",
    "    df[f\"{col_name}_norm\"] = df[col_name] / df[\"atr\"].replace(0, np.nan) * 100  # æ”¾å¤§æ˜¾ç¤º\n",
    "# slope10>0,æœ€è¿‘10æ ¹barä¸Šæ¶¨è¶‹åŠ¿\n",
    "# slope60<0,æœ€è¿‘60æ ¹barä¸‹è·Œè¶‹åŠ¿\n",
    "\n",
    "# å¤šå°ºåº¦ä¸€è‡´æ€§æŒ‡æ ‡\n",
    "df[\"trend_alignment\"] = (\n",
    "    np.sign(df[\"slope_10\"]) + \n",
    "    np.sign(df[\"slope_30\"]) + \n",
    "    np.sign(df[\"slope_60\"])\n",
    ")\n",
    "\n",
    "# ç»Ÿè®¡\n",
    "print(\"\\n=== å¤šå°ºåº¦è¶‹åŠ¿ç‰¹å¾ç»Ÿè®¡ ===\")\n",
    "for w in SLOPE_WINDOWS:\n",
    "    col = f\"slope_{w}_norm\"\n",
    "    print(f\"slope_{w}_norm: mean={df[col].mean():.4f}, std={df[col].std():.4f}\")\n",
    "\n",
    "alignment_counts = df[\"trend_alignment\"].value_counts().sort_index()\n",
    "print(f\"\\ntrend_alignment åˆ†å¸ƒ:\")\n",
    "for k, v in alignment_counts.items():\n",
    "    pct = v / len(df) * 100\n",
    "    label = {-3: \"å¼ºä¸‹è·Œ\", -2: \"å¼±ä¸‹è·Œ\", -1: \"åˆ†æ­§\", 0: \"æ— æ–¹å‘\", \n",
    "             1: \"åˆ†æ­§\", 2: \"å¼±ä¸Šæ¶¨\", 3: \"å¼ºä¸Šæ¶¨\"}.get(int(k), str(k))\n",
    "    print(f\"  {int(k):+d} ({label}): {v:,} ({pct:.1f}%)\")\n",
    "\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# å¤šå°ºåº¦æ–œç‡åˆ†å¸ƒ\n",
    "for i, w in enumerate(SLOPE_WINDOWS):\n",
    "    axes[0].hist(df[f\"slope_{w}_norm\"].dropna(), bins=50, alpha=0.5, label=f\"slope_{w}\")\n",
    "axes[0].set_xlabel(\"æ–œç‡ (æ ‡å‡†åŒ–)\")\n",
    "axes[0].set_ylabel(\"æ•°é‡\")\n",
    "axes[0].set_title(\"å¤šå°ºåº¦æ–œç‡åˆ†å¸ƒ\")\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(-0.5, 0.5)\n",
    "\n",
    "# trend_alignment åˆ†å¸ƒ\n",
    "alignment_counts.plot(kind=\"bar\", ax=axes[1], color=\"steelblue\", edgecolor=\"black\")\n",
    "axes[1].set_xlabel(\"Trend Alignment\")\n",
    "axes[1].set_ylabel(\"æ•°é‡\")\n",
    "axes[1].set_title(\"å¤šå°ºåº¦ä¸€è‡´æ€§åˆ†å¸ƒ (Â±3=å¼ºè¶‹åŠ¿, 0=åˆ†æ­§)\")\n",
    "axes[1].set_xticklabels([f\"{int(x)}\" for x in alignment_counts.index], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCell 17 å®Œæˆ: å¤šå°ºåº¦è¶‹åŠ¿ç‰¹å¾ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 18: æ–¹å‘æ€§æ³¢åŠ¨ & Range Expansion (Directional Volatility) ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ 1.1(b) \"æ–¹å‘æ€§æ³¢åŠ¨\" vs \"æ— æ–¹å‘æ³¢åŠ¨\"\n",
    "   åŒºåˆ†\"é¡ºåŠ¿æ³¢åŠ¨\"å’Œ\"ä¹±æŠ–åŠ¨\"ï¼š\n",
    "   - vol_up = sum(r_i^2 for r_i > 0)\n",
    "   - vol_down = sum(r_i^2 for r_i < 0)\n",
    "   - dir_vol_ratio = |vol_up - vol_down| / vol_tot  âˆˆ [0,1]\n",
    "     - è¶Šæ¥è¿‘ 1: å•è¾¹ä¸»å¯¼ï¼ˆè¶‹åŠ¿æ›´æ˜æ˜¾ï¼‰\n",
    "     - è¶Šæ¥è¿‘ 0: ä¸Šä¸‹ä¹±æŠ–ï¼ˆå…¸å‹ rangeï¼‰\n",
    "\n",
    "ğŸ“Œ 1.1(c) Range Expansion / Compression\n",
    "   - range_z = (current_range - avg_range_N) / std_range_N\n",
    "   - å…¸å‹çš„ regime å˜åŒ–å¾€å¾€ä¼´éš range_z çš„ spike\n",
    "\"\"\"\n",
    "print(\"è®¡ç®—æ–¹å‘æ€§æ³¢åŠ¨ & Range Expansion ç‰¹å¾...\")\n",
    "\n",
    "DIR_VOL_WINDOW = 20\n",
    "RANGE_Z_WINDOW = 20\n",
    "\n",
    "# ========== 1.1(b) æ–¹å‘æ€§æ³¢åŠ¨ ==========\n",
    "def calc_directional_volatility(returns: pd.Series, window: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ–¹å‘æ€§æ³¢åŠ¨æŒ‡æ ‡\n",
    "    \"\"\"\n",
    "    # ä¸Šæ¶¨æ³¢åŠ¨ (r > 0 æ—¶çš„ r^2)\n",
    "    returns_sq = returns ** 2\n",
    "    vol_up = returns_sq.where(returns > 0, 0).rolling(window).sum() # çª—å£å†…æ­£æ”¶ç›Šçš„å¹³æ–¹å’Œ\n",
    "    vol_down = returns_sq.where(returns < 0, 0).rolling(window).sum() # çª—å£å†…è´Ÿæ”¶ç›Šçš„å¹³æ–¹å’Œ\n",
    "    vol_tot = vol_up + vol_down\n",
    "    \n",
    "    # æ–¹å‘æ€§æ³¢åŠ¨æ¯”ç‡\n",
    "    dir_vol_ratio = (vol_up - vol_down).abs() / vol_tot.replace(0, np.nan)\n",
    "    dir_vol_ratio = dir_vol_ratio.clip(0, 1)\n",
    "    \n",
    "    # ä¸Šæ¶¨æ³¢åŠ¨å æ¯” (åˆ¤æ–­æ–¹å‘)\n",
    "    vol_up_ratio = vol_up / vol_tot.replace(0, np.nan) # å•è¾¹ä¸»å¯¼ç¨‹åº¦\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"vol_up\": vol_up,\n",
    "        \"vol_down\": vol_down,\n",
    "        \"vol_tot\": vol_tot,\n",
    "        \"dir_vol_ratio\": dir_vol_ratio,\n",
    "        \"vol_up_ratio\": vol_up_ratio,\n",
    "    })\n",
    "\n",
    "\n",
    "dir_vol_df = calc_directional_volatility(df[\"log_return\"], DIR_VOL_WINDOW)\n",
    "df[\"dir_vol_ratio\"] = dir_vol_df[\"dir_vol_ratio\"]\n",
    "df[\"vol_up_ratio\"] = dir_vol_df[\"vol_up_ratio\"]\n",
    "\n",
    "# ========== 1.1(c) Range Expansion / Compression ==========\n",
    "df[\"bar_range\"] = df[\"high\"] - df[\"low\"]\n",
    "df[\"avg_range\"] = df[\"bar_range\"].rolling(RANGE_Z_WINDOW).mean()\n",
    "df[\"std_range\"] = df[\"bar_range\"].rolling(RANGE_Z_WINDOW).std()\n",
    "df[\"range_z\"] = (df[\"bar_range\"] - df[\"avg_range\"]) / df[\"std_range\"].replace(0, np.nan)\n",
    "\n",
    "# Range z-score çš„æ»šåŠ¨å‡å€¼ (ç”¨äºæ£€æµ‹æŒç»­å‹ç¼©/æ‰©å¼ )\n",
    "df[\"range_z_ma5\"] = df[\"range_z\"].rolling(5).mean()\n",
    "\n",
    "# ç»Ÿè®¡\n",
    "print(\"\\n=== æ–¹å‘æ€§æ³¢åŠ¨ç‰¹å¾ç»Ÿè®¡ ===\")\n",
    "print(f\"dir_vol_ratio: mean={df['dir_vol_ratio'].mean():.4f}, median={df['dir_vol_ratio'].median():.4f}\")\n",
    "print(f\"vol_up_ratio: mean={df['vol_up_ratio'].mean():.4f} (0.5=å¹³è¡¡, >0.5=ä¸Šæ¶¨ä¸»å¯¼)\")\n",
    "\n",
    "print(f\"\\n=== Range Expansion ç‰¹å¾ç»Ÿè®¡ ===\")\n",
    "print(f\"range_z: mean={df['range_z'].mean():.4f}, std={df['range_z'].std():.4f}\")\n",
    "print(f\"range_z_ma5: mean={df['range_z_ma5'].mean():.4f}\")\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# dir_vol_ratio åˆ†å¸ƒ\n",
    "axes[0].hist(df[\"dir_vol_ratio\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].axvline(x=0.5, color=\"red\", linestyle=\"--\", label=\"ä¸­æ€§çº¿ 0.5\")\n",
    "axes[0].set_xlabel(\"dir_vol_ratio\")\n",
    "axes[0].set_ylabel(\"æ•°é‡\")\n",
    "axes[0].set_title(\"æ–¹å‘æ€§æ³¢åŠ¨æ¯”ç‡åˆ†å¸ƒ\\n(é«˜=å•è¾¹, ä½=éœ‡è¡)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# vol_up_ratio åˆ†å¸ƒ\n",
    "axes[1].hist(df[\"vol_up_ratio\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7, color=\"green\")\n",
    "axes[1].axvline(x=0.5, color=\"red\", linestyle=\"--\", label=\"å¹³è¡¡çº¿ 0.5\")\n",
    "axes[1].set_xlabel(\"vol_up_ratio\")\n",
    "axes[1].set_ylabel(\"æ•°é‡\")\n",
    "axes[1].set_title(\"ä¸Šæ¶¨æ³¢åŠ¨å æ¯”\\n(>0.5=ä¸Šæ¶¨ä¸»å¯¼)\")\n",
    "axes[1].legend()\n",
    "\n",
    "# range_z åˆ†å¸ƒ\n",
    "axes[2].hist(df[\"range_z\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7, color=\"orange\")\n",
    "axes[2].axvline(x=0, color=\"red\", linestyle=\"--\", label=\"å¹³å‡æ°´å¹³\")\n",
    "axes[2].axvline(x=2, color=\"green\", linestyle=\"--\", label=\"+2Ïƒ (æ‰©å¼ )\")\n",
    "axes[2].axvline(x=-1, color=\"blue\", linestyle=\"--\", label=\"-1Ïƒ (å‹ç¼©)\")\n",
    "axes[2].set_xlabel(\"range_z\")\n",
    "axes[2].set_ylabel(\"æ•°é‡\")\n",
    "axes[2].set_title(\"Range Z-Score åˆ†å¸ƒ\\n(æ­£=æ‰©å¼ , è´Ÿ=å‹ç¼©)\")\n",
    "axes[2].legend()\n",
    "axes[2].set_xlim(-3, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCell 18 å®Œæˆ: æ–¹å‘æ€§æ³¢åŠ¨ & Range Expansion âœ…\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 19: å¤šç©º Bar ç»“æ„ & è¿ç»­æ€§ (Bull/Bear Bar Structure) ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ 1.2(a) Brooks å¾ˆçœ‹\"è¿ç»­å‡ æ ¹ bull/bear bar\"\n",
    "   - bull_ratio_N: çª—å£å†…é˜³çº¿æ¯”ä¾‹\n",
    "   - bear_ratio_N: çª—å£å†…é˜´çº¿æ¯”ä¾‹\n",
    "   - consec_bull: å½“å‰å‘å‰è¿ç»­çš„é˜³çº¿æ•°é‡\n",
    "   - consec_bear: å½“å‰å‘å‰è¿ç»­çš„é˜´çº¿æ•°é‡\n",
    "   \n",
    "   è¶‹åŠ¿æ®µä¸­å¸¸è§: bull_ratio æé«˜ä¸” consec_bull > 3\n",
    "   éœ‡è¡æ®µä¸­å¤šä¸º: 1-1ã€2-1 äº’æ¢ã€bull/bear æ··æ‚\n",
    "\"\"\"\n",
    "print(\"è®¡ç®—å¤šç©º Bar ç»“æ„ç‰¹å¾...\")\n",
    "\n",
    "BAR_STRUCTURE_WINDOW = 10\n",
    "\n",
    "# ========== Bull/Bear åˆ¤å®š ==========\n",
    "# é˜³çº¿: close > open, é˜´çº¿: close < open\n",
    "df[\"is_bull\"] = (df[\"close\"] > df[\"open\"]).astype(int)\n",
    "df[\"is_bear\"] = (df[\"close\"] < df[\"open\"]).astype(int)\n",
    "\n",
    "tick = MICRO_TICK_SIZE\n",
    "df[\"is_doji\"] = (np.abs(df[\"close\"] - df[\"open\"]) < tick/2).astype(int) # åå­—æ˜Ÿ\n",
    "\n",
    "\n",
    "# ========== çª—å£å†…æ¯”ä¾‹ ==========\n",
    "df[\"bull_ratio\"] = df[\"is_bull\"].rolling(BAR_STRUCTURE_WINDOW).mean() # è¶‹åŠ¿ä¸Šè¡Œbull_ratioé«˜\n",
    "df[\"bear_ratio\"] = df[\"is_bear\"].rolling(BAR_STRUCTURE_WINDOW).mean()# è¶‹åŠ¿ä¸‹è¡Œbear_ratioé«˜\n",
    "\n",
    "# Bull - Bear å·®å¼‚ (æ­£=å¤šå¤´ä¸»å¯¼, è´Ÿ=ç©ºå¤´ä¸»å¯¼)\n",
    "df[\"bull_bear_diff\"] = df[\"bull_ratio\"] - df[\"bear_ratio\"] # æ–¹å‘å‹åŠ›\n",
    "\n",
    "# ========== è¿ç»­æ€§è®¡ç®— ==========\n",
    "def calc_consecutive_bars(is_bull: np.ndarray, is_bear: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    è®¡ç®—è¿ç»­é˜³çº¿/é˜´çº¿æ•°é‡\n",
    "    \"\"\"\n",
    "    n = len(is_bull)\n",
    "    consec_bull = np.zeros(n, dtype=np.int32)\n",
    "    consec_bear = np.zeros(n, dtype=np.int32)\n",
    "    \n",
    "    for i in range(n):\n",
    "        # è¿ç»­é˜³çº¿\n",
    "        if is_bull[i]:\n",
    "            if i == 0:\n",
    "                consec_bull[i] = 1\n",
    "            else:\n",
    "                consec_bull[i] = consec_bull[i-1] + 1 if is_bull[i-1] else 1\n",
    "        else:\n",
    "            consec_bull[i] = 0\n",
    "        \n",
    "        # è¿ç»­é˜´çº¿\n",
    "        if is_bear[i]:\n",
    "            if i == 0:\n",
    "                consec_bear[i] = 1\n",
    "            else:\n",
    "                consec_bear[i] = consec_bear[i-1] + 1 if is_bear[i-1] else 1\n",
    "        else:\n",
    "            consec_bear[i] = 0\n",
    "    \n",
    "    return consec_bull, consec_bear\n",
    "\n",
    "\n",
    "consec_bull, consec_bear = calc_consecutive_bars(\n",
    "    df[\"is_bull\"].values.astype(np.int32), \n",
    "    df[\"is_bear\"].values.astype(np.int32) \n",
    ")\n",
    "df[\"consec_bull\"] = consec_bull\n",
    "df[\"consec_bear\"] = consec_bear\n",
    "\n",
    "# æœ€è¿‘ N æ ¹å†…æœ€å¤§è¿ç»­æ®µ\n",
    "df[\"max_consec_bull_10\"] = df[\"consec_bull\"].rolling(10).max()\n",
    "df[\"max_consec_bear_10\"] = df[\"consec_bear\"].rolling(10).max()\n",
    "\n",
    "# ========== ç»Ÿè®¡ ==========\n",
    "print(\"\\n=== Bull/Bear æ¯”ä¾‹ç»Ÿè®¡ ===\")\n",
    "print(f\"bull_ratio: mean={df['bull_ratio'].mean():.4f}\")\n",
    "print(f\"bear_ratio: mean={df['bear_ratio'].mean():.4f}\")\n",
    "print(f\"bull_bear_diff: mean={df['bull_bear_diff'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\n=== è¿ç»­æ€§ç»Ÿè®¡ ===\")\n",
    "print(f\"consec_bull: max={df['consec_bull'].max()}, mean={df['consec_bull'].mean():.2f}\")\n",
    "print(f\"consec_bear: max={df['consec_bear'].max()}, mean={df['consec_bear'].mean():.2f}\")\n",
    "\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Heiti TC', 'PingFang SC']\n",
    "\n",
    "\n",
    "# bull/bear ratio åˆ†å¸ƒ\n",
    "axes[0].hist(df[\"bull_ratio\"].dropna(), bins=30, alpha=0.6, label=\"bull_ratio\", color=\"green\")\n",
    "axes[0].hist(df[\"bear_ratio\"].dropna(), bins=30, alpha=0.6, label=\"bear_ratio\", color=\"red\")\n",
    "axes[0].axvline(x=0.5, color=\"black\", linestyle=\"--\", label=\"å¹³è¡¡çº¿\")\n",
    "axes[0].set_xlabel(\"æ¯”ä¾‹\")\n",
    "axes[0].set_ylabel(\"æ•°é‡\")\n",
    "axes[0].set_title(f\"Bull/Bear æ¯”ä¾‹åˆ†å¸ƒ (çª—å£={BAR_STRUCTURE_WINDOW})\")\n",
    "axes[0].legend()\n",
    "\n",
    "# bull_bear_diff åˆ†å¸ƒ\n",
    "axes[1].hist(df[\"bull_bear_diff\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[1].axvline(x=0, color=\"red\", linestyle=\"--\", label=\"å¹³è¡¡\")\n",
    "axes[1].set_xlabel(\"bull_ratio - bear_ratio\")\n",
    "axes[1].set_ylabel(\"æ•°é‡\")\n",
    "axes[1].set_title(\"å¤šç©ºå·®å¼‚åˆ†å¸ƒ\\n(æ­£=å¤šå¤´ä¸»å¯¼)\")\n",
    "axes[1].legend()\n",
    "\n",
    "# è¿ç»­æ€§åˆ†å¸ƒ\n",
    "consec_bull_counts = pd.Series(consec_bull).value_counts().sort_index()\n",
    "consec_bear_counts = pd.Series(consec_bear).value_counts().sort_index()\n",
    "x_range = range(0, min(15, max(consec_bull_counts.index.max(), consec_bear_counts.index.max()) + 1))\n",
    "axes[2].bar([x - 0.2 for x in x_range], \n",
    "            [consec_bull_counts.get(x, 0) for x in x_range], \n",
    "            width=0.4, label=\"è¿ç»­é˜³çº¿\", color=\"green\", alpha=0.7)\n",
    "axes[2].bar([x + 0.2 for x in x_range], \n",
    "            [consec_bear_counts.get(x, 0) for x in x_range], \n",
    "            width=0.4, label=\"è¿ç»­é˜´çº¿\", color=\"red\", alpha=0.7)\n",
    "axes[2].set_xlabel(\"è¿ç»­ Bar æ•°é‡\")\n",
    "axes[2].set_ylabel(\"å‡ºç°æ¬¡æ•°\")\n",
    "axes[2].set_title(\"è¿ç»­ Bull/Bear Bar åˆ†å¸ƒ\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCell 19 å®Œæˆ: å¤šç©º Bar ç»“æ„ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 20: å®ä½“/å½±çº¿ç»“æ„ (Body & Wick Ratios) ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ 1.2(b) Brooks é£æ ¼çš„å®ä½“/å½±çº¿åˆ†æï¼ˆä¸åšæ—¥å¼å½¢æ€åï¼‰\n",
    "   å¯¹æ¯æ ¹ bar:\n",
    "   - body_ratio = |close - open| / range  (å®ä½“å æ¯”)\n",
    "   - upper_wick_ratio = (high - max(open, close)) / range (ä¸Šå½±çº¿å æ¯”)\n",
    "   - lower_wick_ratio = (min(open, close) - low) / range (ä¸‹å½±çº¿å æ¯”)\n",
    "   \n",
    "   çª—å£å‡å€¼è§£è¯»:\n",
    "   - avg_body_ratio ä½ + å¤šå½±çº¿ â‡’ å…¸å‹ trading range\n",
    "   - avg_body_ratio é«˜ + bull/bear æç«¯ â‡’ è¶‹åŠ¿æ®µ\n",
    "   - Brooks: \"lots of tails, especially both sides â‡’ trading range\"\n",
    "\"\"\"\n",
    "print(\"è®¡ç®—å®ä½“/å½±çº¿ç»“æ„ç‰¹å¾...\")\n",
    "\n",
    "if \"bar_range\" not in df.columns:\n",
    "    df[\"bar_range\"] = df[\"high\"] - df[\"low\"]\n",
    "\n",
    "\n",
    "WICK_WINDOW = 10\n",
    "\n",
    "# ========== å• Bar çš„å®ä½“/å½±çº¿æ¯”ä¾‹ ==========\n",
    "df[\"body\"] = (df[\"close\"] - df[\"open\"]).abs()\n",
    "df[\"upper_wick\"] = df[\"high\"] - df[[\"open\", \"close\"]].max(axis=1)\n",
    "df[\"lower_wick\"] = df[[\"open\", \"close\"]].min(axis=1) - df[\"low\"]\n",
    "\n",
    "# æ¯”ä¾‹ (é¿å…é™¤ä»¥0)\n",
    "bar_range = df[\"bar_range\"].replace(0, np.nan)\n",
    "df[\"body_ratio\"] = df[\"body\"] / bar_range\n",
    "df[\"upper_wick_ratio\"] = df[\"upper_wick\"] / bar_range\n",
    "df[\"lower_wick_ratio\"] = df[\"lower_wick\"] / bar_range\n",
    "\n",
    "# åŒå½±çº¿æ¯”ä¾‹ (ä¸Šä¸‹å½±çº¿ä¹‹å’Œ)\n",
    "df[\"total_wick_ratio\"] = df[\"upper_wick_ratio\"] + df[\"lower_wick_ratio\"]\n",
    "\n",
    "# ========== çª—å£å‡å€¼ ==========\n",
    "df[\"avg_body_ratio\"] = df[\"body_ratio\"].rolling(WICK_WINDOW).mean()\n",
    "df[\"avg_upper_wick_ratio\"] = df[\"upper_wick_ratio\"].rolling(WICK_WINDOW).mean()\n",
    "df[\"avg_lower_wick_ratio\"] = df[\"lower_wick_ratio\"].rolling(WICK_WINDOW).mean()\n",
    "df[\"avg_total_wick_ratio\"] = df[\"total_wick_ratio\"].rolling(WICK_WINDOW).mean()\n",
    "\n",
    "# ========== å°å®ä½“/å¤§å½±çº¿çš„ bar æ•°é‡ (Brooks æ‰€è¯´çš„ trading range ç‰¹å¾) ==========\n",
    "# å®šä¹‰: body_ratio < 0.3 ä¸” total_wick_ratio > 0.5\n",
    "df[\"is_doji_like\"] = ((df[\"body_ratio\"] < 0.3) & (df[\"total_wick_ratio\"] > 0.5)).astype(int)\n",
    "df[\"doji_like_ratio\"] = df[\"is_doji_like\"].rolling(WICK_WINDOW).mean()\n",
    "\n",
    "# ========== ç»Ÿè®¡ ==========\n",
    "print(\"\\n=== å• Bar å®ä½“/å½±çº¿ç»Ÿè®¡ ===\")\n",
    "print(f\"body_ratio: mean={df['body_ratio'].mean():.4f}, median={df['body_ratio'].median():.4f}\")\n",
    "print(f\"upper_wick_ratio: mean={df['upper_wick_ratio'].mean():.4f}\")\n",
    "print(f\"lower_wick_ratio: mean={df['lower_wick_ratio'].mean():.4f}\")\n",
    "print(f\"total_wick_ratio: mean={df['total_wick_ratio'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\n=== çª—å£å‡å€¼ (N={WICK_WINDOW}) ===\")\n",
    "print(f\"avg_body_ratio: mean={df['avg_body_ratio'].mean():.4f}\")\n",
    "print(f\"avg_total_wick_ratio: mean={df['avg_total_wick_ratio'].mean():.4f}\")\n",
    "print(f\"doji_like_ratio: mean={df['doji_like_ratio'].mean():.4f} (å°å®ä½“å¤§å½±çº¿çš„æ¯”ä¾‹)\")\n",
    "\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Heiti TC', 'PingFang SC']\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# å• bar body_ratio åˆ†å¸ƒ\n",
    "axes[0, 0].hist(df[\"body_ratio\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0, 0].axvline(x=0.3, color=\"red\", linestyle=\"--\", label=\"å°å®ä½“é˜ˆå€¼ 0.3\")\n",
    "axes[0, 0].axvline(x=0.7, color=\"green\", linestyle=\"--\", label=\"å¤§å®ä½“é˜ˆå€¼ 0.7\")\n",
    "axes[0, 0].set_xlabel(\"body_ratio\")\n",
    "axes[0, 0].set_ylabel(\"æ•°é‡\")\n",
    "axes[0, 0].set_title(\"å• Bar å®ä½“æ¯”ä¾‹åˆ†å¸ƒ\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# å• bar å½±çº¿åˆ†å¸ƒ\n",
    "axes[0, 1].hist(df[\"upper_wick_ratio\"].dropna(), bins=50, alpha=0.6, label=\"ä¸Šå½±çº¿\", color=\"orange\")\n",
    "axes[0, 1].hist(df[\"lower_wick_ratio\"].dropna(), bins=50, alpha=0.6, label=\"ä¸‹å½±çº¿\", color=\"purple\")\n",
    "axes[0, 1].set_xlabel(\"wick_ratio\")\n",
    "axes[0, 1].set_ylabel(\"æ•°é‡\")\n",
    "axes[0, 1].set_title(\"ä¸Š/ä¸‹å½±çº¿æ¯”ä¾‹åˆ†å¸ƒ\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# çª—å£å‡å€¼ body vs wick\n",
    "axes[1, 0].scatter(df[\"avg_body_ratio\"].dropna().values[::100], \n",
    "                   df[\"avg_total_wick_ratio\"].dropna().values[::100], \n",
    "                   alpha=0.3, s=5)\n",
    "axes[1, 0].set_xlabel(\"avg_body_ratio\")\n",
    "axes[1, 0].set_ylabel(\"avg_total_wick_ratio\")\n",
    "axes[1, 0].set_title(f\"çª—å£å‡å€¼: å®ä½“ vs å½±çº¿ (N={WICK_WINDOW})\\n(é‡‡æ ·æ˜¾ç¤º)\")\n",
    "axes[1, 0].axhline(y=0.5, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "axes[1, 0].axvline(x=0.5, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# doji_like_ratio åˆ†å¸ƒ\n",
    "axes[1, 1].hist(df[\"doji_like_ratio\"].dropna(), bins=30, edgecolor=\"black\", alpha=0.7, color=\"gray\")\n",
    "axes[1, 1].set_xlabel(\"doji_like_ratio\")\n",
    "axes[1, 1].set_ylabel(\"æ•°é‡\")\n",
    "axes[1, 1].set_title(\"å°å®ä½“å¤§å½±çº¿ Bar æ¯”ä¾‹åˆ†å¸ƒ\\n(é«˜=å…¸å‹éœ‡è¡åŒºé—´)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCell 20 å®Œæˆ: å®ä½“/å½±çº¿ç»“æ„ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 21: ä¸‰é‡è…¿ç»“æ„ (Three-Type Leg Structure) ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ 1.2(c) Leg Structure Metrics â€” Brooks é£æ ¼çš„è…¿ç»“æ„é‡åŒ–\n",
    "\n",
    "ä¸‰ç§è…¿çš„å®šä¹‰:\n",
    "1. Leg Type A (Major Legs): Spike Break Leg â€” ç»“æ„æ€§çªç ´è…¿\n",
    "   - ä¸Šæ¶¨ä¸­: æŸæ ¹ bar çš„ low è·Œç ´å‰ä¸€æ ¹ bar çš„ low â†’ æ–° major leg å¼€å§‹\n",
    "   - ä¸‹è·Œä¸­: æŸæ ¹ bar çš„ high çªç ´å‰ä¸€æ ¹ bar çš„ high â†’ æ–° major leg å¼€å§‹\n",
    "   \n",
    "2. Leg Type B (Micro Legs): Opposite Close Leg â€” å¯¹å‘æ”¶ç›˜è…¿  \n",
    "   - ä¸Šæ¶¨ä¸­: å‡ºç°ä¸€æ ¹é˜´çº¿ (close < open) â†’ æ–° micro leg\n",
    "   - ä¸‹è·Œä¸­: å‡ºç°ä¸€æ ¹é˜³çº¿ (close > open) â†’ æ–° micro leg\n",
    "   \n",
    "3. Leg Type C (Fractal Legs): Implied Pullback Leg â€” éšå«å›è°ƒè…¿\n",
    "   - åŸºäºé‡å åº¦ã€å°å®ä½“ã€å½±çº¿ç­‰ç»“æ„ä¿¡å·åˆ¤å®š\n",
    "   - æ¨¡æ‹Ÿæ›´å°æ—¶é—´å‘¨æœŸä¸Šçš„ pullback ä¿¡æ¯\n",
    "\n",
    "è¾“å‡ºç‰¹å¾:\n",
    "- legA_id, legA_dir, legA_prev1_dir, legA_prev2_dir\n",
    "- legB_id, legB_dir, legB_prev1_dir, legB_prev2_dir  \n",
    "- legC_id, legC_dir, legC_prev1_dir, legC_prev2_dir\n",
    "\"\"\"\n",
    "print(\"è®¡ç®—ä¸‰é‡è…¿ç»“æ„ç‰¹å¾ (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...\")\n",
    "\n",
    "\n",
    "def calc_leg_type_A(high: np.ndarray, low: np.ndarray, close: np.ndarray, open_: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    è®¡ç®— Leg Type A: Spike Break Legs (Major Legs)\n",
    "    \n",
    "    è§„åˆ™:\n",
    "    - ä¸Šæ¶¨è¶‹åŠ¿ä¸­: low è·Œç ´å‰ä¸€æ ¹ low â†’ æ–°è…¿å¼€å§‹\n",
    "    - ä¸‹è·Œè¶‹åŠ¿ä¸­: high çªç ´å‰ä¸€æ ¹ high â†’ æ–°è…¿å¼€å§‹\n",
    "    - åˆå§‹æ–¹å‘ç”±ç¬¬ä¸€æ ¹ bar çš„ close-open å†³å®š\n",
    "    \"\"\"\n",
    "    n = len(high)\n",
    "    leg_id = np.zeros(n, dtype=np.int32)\n",
    "    leg_dir = np.zeros(n, dtype=np.int32)\n",
    "    \n",
    "    # åˆå§‹åŒ–\n",
    "    current_leg_id = 1\n",
    "    if close[0] >= open_[0]:\n",
    "        current_dir = 1  # ä¸Šæ¶¨\n",
    "    else:\n",
    "        current_dir = -1  # ä¸‹è·Œ\n",
    "    \n",
    "    leg_id[0] = current_leg_id\n",
    "    leg_dir[0] = current_dir\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        new_leg = False\n",
    "        \n",
    "        if current_dir == 1:  # å½“å‰ä¸Šæ¶¨è…¿\n",
    "            # å¦‚æœ low è·Œç ´å‰ä¸€æ ¹ low â†’ ç»“æ„æ€§ç ´å â†’ æ–°è…¿\n",
    "            if low[i] < low[i-1]:\n",
    "                new_leg = True\n",
    "                # æ–°è…¿æ–¹å‘ç”±å½“å‰ bar å†³å®š\n",
    "                if close[i] >= open_[i]:\n",
    "                    current_dir = 1\n",
    "                else:\n",
    "                    current_dir = -1\n",
    "        else:  # å½“å‰ä¸‹è·Œè…¿\n",
    "            # å¦‚æœ high çªç ´å‰ä¸€æ ¹ high â†’ ç»“æ„æ€§ç ´å â†’ æ–°è…¿\n",
    "            if high[i] > high[i-1]:\n",
    "                new_leg = True\n",
    "                if close[i] >= open_[i]:\n",
    "                    current_dir = 1\n",
    "                else:\n",
    "                    current_dir = -1\n",
    "        \n",
    "        if new_leg:\n",
    "            current_leg_id += 1\n",
    "        \n",
    "        leg_id[i] = current_leg_id\n",
    "        leg_dir[i] = current_dir\n",
    "    \n",
    "    return leg_id, leg_dir\n",
    "\n",
    "\n",
    "def calc_leg_type_B(close: np.ndarray, open_: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    è®¡ç®— Leg Type B: Opposite Close Legs (Micro Legs)\n",
    "    \n",
    "    è§„åˆ™:\n",
    "    - å½“å‰ä¸Šæ¶¨è…¿ä¸­ï¼Œå‡ºç°é˜´çº¿ â†’ æ–°è…¿ï¼ˆæ–¹å‘ç¿»è½¬ï¼‰\n",
    "    - å½“å‰ä¸‹è·Œè…¿ä¸­ï¼Œå‡ºç°é˜³çº¿ â†’ æ–°è…¿ï¼ˆæ–¹å‘ç¿»è½¬ï¼‰\n",
    "    \"\"\"\n",
    "    n = len(close)\n",
    "    leg_id = np.zeros(n, dtype=np.int32)\n",
    "    leg_dir = np.zeros(n, dtype=np.int32)\n",
    "    \n",
    "    current_leg_id = 1\n",
    "    if close[0] >= open_[0]:\n",
    "        current_dir = 1\n",
    "    else:\n",
    "        current_dir = -1\n",
    "    \n",
    "    leg_id[0] = current_leg_id\n",
    "    leg_dir[0] = current_dir\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        bar_dir = 1 if close[i] >= open_[i] else -1\n",
    "        \n",
    "        # å¦‚æœå½“å‰ bar æ–¹å‘ä¸è…¿æ–¹å‘ç›¸å â†’ æ–°è…¿\n",
    "        if bar_dir != current_dir:\n",
    "            current_leg_id += 1\n",
    "            current_dir = bar_dir\n",
    "        \n",
    "        leg_id[i] = current_leg_id\n",
    "        leg_dir[i] = current_dir\n",
    "    \n",
    "    return leg_id, leg_dir\n",
    "\n",
    "\n",
    "def calc_leg_type_C(high: np.ndarray, low: np.ndarray, close: np.ndarray, open_: np.ndarray,\n",
    "                    overlap_ratio: np.ndarray, body_ratio: np.ndarray, \n",
    "                    upper_wick_ratio: np.ndarray, lower_wick_ratio: np.ndarray,\n",
    "                    atr: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    è®¡ç®— Leg Type C: Implied Pullback Legs (Fractal Legs)\n",
    "    \n",
    "    è§„åˆ™ (ä¸Šæ¶¨ä¸­ï¼Œæ»¡è¶³2é¡¹æ¡ä»¶å³æ–°è…¿):\n",
    "    - overlap_ratio > 0.6\n",
    "    - body_ratio < 0.3\n",
    "    - upper_wick_ratio > 0.4 (ä¸Šæ¶¨ä¸­) æˆ– lower_wick_ratio > 0.4 (ä¸‹è·Œä¸­)\n",
    "    - æœ€è¿‘4æ ¹barçš„é«˜ç‚¹å¢å¹… < ATR * 0.3 (åŠ¨èƒ½åœæ»)\n",
    "    \"\"\"\n",
    "    n = len(high)\n",
    "    leg_id = np.zeros(n, dtype=np.int32)\n",
    "    leg_dir = np.zeros(n, dtype=np.int32)\n",
    "    \n",
    "    current_leg_id = 1\n",
    "    if close[0] >= open_[0]:\n",
    "        current_dir = 1\n",
    "    else:\n",
    "        current_dir = -1\n",
    "    \n",
    "    leg_id[0] = current_leg_id\n",
    "    leg_dir[0] = current_dir\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        conditions_met = 0\n",
    "        \n",
    "        # æ£€æŸ¥å„é¡¹æ¡ä»¶\n",
    "        if not np.isnan(overlap_ratio[i]) and overlap_ratio[i] > 0.6: \n",
    "            conditions_met += 1\n",
    "        \n",
    "        if not np.isnan(body_ratio[i]) and body_ratio[i] < 0.3:\n",
    "            conditions_met += 1\n",
    "        \n",
    "        if current_dir == 1:  # ä¸Šæ¶¨è…¿\n",
    "            if not np.isnan(upper_wick_ratio[i]) and upper_wick_ratio[i] > 0.4:\n",
    "                conditions_met += 1\n",
    "        else:  # ä¸‹è·Œè…¿\n",
    "            if not np.isnan(lower_wick_ratio[i]) and lower_wick_ratio[i] > 0.4:\n",
    "                conditions_met += 1\n",
    "        \n",
    "        # åŠ¨èƒ½åœæ»æ£€æŸ¥ (æœ€è¿‘4æ ¹bar)\n",
    "        if i >= 4 and not np.isnan(atr[i]) and atr[i] > 0:\n",
    "            if current_dir == 1:\n",
    "                high_gain = high[i] - high[i-4]\n",
    "            else:\n",
    "                high_gain = low[i-4] - low[i]\n",
    "            \n",
    "            if high_gain < atr[i] * 0.3:\n",
    "                conditions_met += 1\n",
    "        \n",
    "        # æ»¡è¶³ 2 é¡¹æ¡ä»¶ â†’ æ–°è…¿\n",
    "        if conditions_met >= 2:\n",
    "            current_leg_id += 1\n",
    "            # æ–¹å‘ç”±å½“å‰ bar å†³å®š\n",
    "            if close[i] >= open_[i]:\n",
    "                current_dir = 1\n",
    "            else:\n",
    "                current_dir = -1\n",
    "        \n",
    "        leg_id[i] = current_leg_id\n",
    "        leg_dir[i] = current_dir\n",
    "    \n",
    "    return leg_id, leg_dir\n",
    "\n",
    "\n",
    "# ========== è®¡ç®—ä¸‰ç§è…¿ ==========\n",
    "print(\"  è®¡ç®— Leg Type A (Major Legs)...\")\n",
    "legA_id, legA_dir = calc_leg_type_A(\n",
    "    df[\"high\"].values, df[\"low\"].values, \n",
    "    df[\"close\"].values, df[\"open\"].values\n",
    ")\n",
    "df[\"legA_id\"] = legA_id\n",
    "df[\"legA_dir\"] = legA_dir\n",
    "\n",
    "\n",
    "print(\"  è®¡ç®— Leg Type B (Micro Legs)...\")\n",
    "legB_id, legB_dir = calc_leg_type_B(df[\"close\"].values, df[\"open\"].values)\n",
    "df[\"legB_id\"] = legB_id\n",
    "df[\"legB_dir\"] = legB_dir\n",
    "\n",
    "print(\"  è®¡ç®— Leg Type C (Fractal Legs)...\")\n",
    "# å…ˆå¡«å…… NaN ä¸º 0ï¼Œé¿å… numba é”™è¯¯\n",
    "overlap_arr = df[\"overlap_ratio\"].values.astype(np.float64)\n",
    "body_arr = df[\"body_ratio\"].values.astype(np.float64)\n",
    "upper_wick_arr = df[\"upper_wick_ratio\"].values.astype(np.float64)\n",
    "lower_wick_arr = df[\"lower_wick_ratio\"].values.astype(np.float64)\n",
    "atr_arr = df[\"atr\"].values.astype(np.float64)\n",
    "\n",
    "\n",
    "legC_id, legC_dir = calc_leg_type_C(\n",
    "    df[\"high\"].values, df[\"low\"].values,\n",
    "    df[\"close\"].values, df[\"open\"].values,\n",
    "    overlap_arr, body_arr, upper_wick_arr, lower_wick_arr, atr_arr\n",
    ")\n",
    "df[\"legC_id\"] = legC_id\n",
    "df[\"legC_dir\"] = legC_dir\n",
    "\n",
    "# ========== æ·»åŠ å‰å‡ è…¿æ–¹å‘ç‰¹å¾ ==========\n",
    "for leg_type in [\"A\", \"B\", \"C\"]:\n",
    "    dir_col = f\"leg{leg_type}_dir\"\n",
    "    df[f\"leg{leg_type}_prev1_dir\"] = df[dir_col].shift(1)\n",
    "    df[f\"leg{leg_type}_prev2_dir\"] = df[dir_col].shift(2)\n",
    "\n",
    "# ========== è®¡ç®—æ¯ç§è…¿çš„å˜åŒ–ç‡ (ç”¨äºæ£€æµ‹é¢‘ç‡) ==========\n",
    "for leg_type in [\"A\", \"B\", \"C\"]:\n",
    "    id_col = f\"leg{leg_type}_id\"\n",
    "    # è…¿å˜åŒ–æ ‡è®°\n",
    "    df[f\"leg{leg_type}_change\"] = (df[id_col] != df[id_col].shift(1)).astype(int)\n",
    "    # æœ€è¿‘ N æ ¹å†…çš„è…¿å˜åŒ–æ¬¡æ•°\n",
    "    df[f\"leg{leg_type}_changes_10\"] = df[f\"leg{leg_type}_change\"].rolling(10).sum()\n",
    "\n",
    "# ========== ç»Ÿè®¡ ==========\n",
    "print(\"\\n=== ä¸‰é‡è…¿ç»“æ„ç»Ÿè®¡ ===\")\n",
    "for leg_type, desc in [(\"A\", \"Major\"), (\"B\", \"Micro\"), (\"C\", \"Fractal\")]:\n",
    "    total_legs = df[f\"leg{leg_type}_id\"].max()\n",
    "    avg_changes = df[f\"leg{leg_type}_changes_10\"].mean()\n",
    "    print(f\"Leg {leg_type} ({desc}): æ€»è…¿æ•°={total_legs:,}, å¹³å‡æ¯10barå˜åŒ–={avg_changes:.2f}æ¬¡\")\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Heiti TC', 'PingFang SC']\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, (leg_type, desc, color) in enumerate([(\"A\", \"Major\", \"blue\"), \n",
    "                                               (\"B\", \"Micro\", \"green\"), \n",
    "                                               (\"C\", \"Fractal\", \"orange\")]):\n",
    "    changes_col = f\"leg{leg_type}_changes_10\"\n",
    "    axes[i].hist(df[changes_col].dropna(), bins=20, edgecolor=\"black\", alpha=0.7, color=color)\n",
    "    axes[i].set_xlabel(f\"æ¯10barçš„Leg{leg_type}å˜åŒ–æ¬¡æ•°\")\n",
    "    axes[i].set_ylabel(\"æ•°é‡\")\n",
    "    axes[i].set_title(f\"Leg {leg_type} ({desc}) å˜åŒ–é¢‘ç‡\\n(é«˜=æ›´å¤šè…¿åˆ‡æ¢)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCell 21 å®Œæˆ: ä¸‰é‡è…¿ç»“æ„ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 22: Spike & Channel è¯„åˆ† (Phase Detection) ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ ä½¿ç”¨ä¸‰å¥—è…¿ç»“æ„æ¥è¯†åˆ« Spike (è¶‹åŠ¿çˆ†å‘æœŸ) å’Œ Channel (è¶‹åŠ¿æˆç†ŸæœŸ)\n",
    "\n",
    "Spike (è¶‹åŠ¿çˆ†å‘) ç‰¹ç‚¹:\n",
    "- åªæœ‰ 1 æ¡ major legï¼ˆæˆ–åªåˆ° legA=2ï¼‰\n",
    "- micro legsï¼ˆB å‹ï¼‰æå°‘\n",
    "- implied legsï¼ˆC å‹ï¼‰æå°‘\n",
    "- æ–œç‡å¼ºã€é‡å å°‘ã€range_z é«˜ã€er é«˜ã€chop ä½\n",
    "\n",
    "Channel (è¶‹åŠ¿æˆç†Ÿ/è¡°ç«­) ç‰¹ç‚¹:\n",
    "- major legA â‰¥ 2 ä¸”è¶‹åŠ¿ä»å»¶ç»­\n",
    "- micro legsï¼ˆB å‹ï¼‰æ•°é‡ä¸Šå‡\n",
    "- implied legsï¼ˆC å‹ï¼‰å¯†åº¦ä¸Šå‡\n",
    "- slope ä¸‹é™ä½†æ–¹å‘ä»ç¨³å®š\n",
    "- é«˜é‡å åº¦\n",
    "\n",
    "è¾“å‡º:\n",
    "- spike_score: 0-1 åˆ†æ•°ï¼Œè¶Šé«˜è¶Šåƒ Spike\n",
    "- channel_score: 0-1 åˆ†æ•°ï¼Œè¶Šé«˜è¶Šåƒ Channel\n",
    "- phase: 0=normal, 1=spike, 2=channel\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Phase è¯†åˆ«ï¼ˆæ— æ ‡ç­¾æ³„éœ²ç‰ˆæœ¬ï¼‰\n",
    "- ä»ç„¶è¾“å‡º spike_score / channel_score / phase\n",
    "- phase ä¸å†ä¾èµ– df['regime']ï¼ˆæ ‡ç­¾ï¼‰ï¼Œè€Œæ˜¯ä¾èµ–éæ ‡ç­¾çš„ trend_proxyï¼ˆè¶‹åŠ¿ä»£ç†ï¼‰\n",
    "  trend_proxy ä½¿ç”¨ï¼šslope_norm, er, chop, trend_alignmentï¼ˆéƒ½ä¸éœ€è¦æ ‡ç­¾ï¼‰\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"è®¡ç®— Spike & Channel è¯„åˆ†...\")\n",
    "\n",
    "PHASE_WINDOW = 10\n",
    "\n",
    "# ========== Spike Score ==========\n",
    "def calc_spike_score(df: pd.DataFrame, window: int = 10) -> pd.Series:\n",
    "    \"\"\"\n",
    "    è®¡ç®— Spike Score (è¶‹åŠ¿çˆ†å‘åˆ†æ•°)\n",
    "    \n",
    "    ç»„æˆ:\n",
    "    1. Major leg å°‘ (legA_changes ä½)\n",
    "    2. Micro leg å°‘ (legB_changes ä½) \n",
    "    3. Fractal leg å°‘ (legC_changes ä½)\n",
    "    4. ER é«˜\n",
    "    5. chop ä½\n",
    "    6. range_z é«˜\n",
    "    7. å¤šå°ºåº¦ä¸€è‡´æ€§é«˜ (|trend_alignment| == 3)\n",
    "    \"\"\"\n",
    "    scores = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # 1. Major leg å˜åŒ–å°‘ (spike ä¸­é€šå¸¸åªæœ‰ 1-2 æ¡å¤§è…¿)\n",
    "    legA_changes = df[\"legA_changes_10\"]\n",
    "    scores[\"legA_score\"] = 1 - (legA_changes / 10).clip(0, 1)  # å˜åŒ–å°‘ â†’ é«˜åˆ†\n",
    "    \n",
    "    # 2. Micro leg å˜åŒ–å°‘\n",
    "    legB_changes = df[\"legB_changes_10\"]\n",
    "    scores[\"legB_score\"] = 1 - (legB_changes / 10).clip(0, 1)\n",
    "    \n",
    "    # 3. Fractal leg å˜åŒ–å°‘\n",
    "    legC_changes = df[\"legC_changes_10\"]\n",
    "    scores[\"legC_score\"] = 1 - (legC_changes / 10).clip(0, 1)\n",
    "    \n",
    "    # 4. ER é«˜ (> 0.4 å¼€å§‹åŠ åˆ†)\n",
    "    scores[\"er_score\"] = ((df[\"er\"] - 0.2) / 0.6).clip(0, 1)\n",
    "    \n",
    "    # 5. chop ä½ (< 0.5 å¼€å§‹åŠ åˆ†)\n",
    "    scores[\"chop_score\"] = ((0.7 - df[\"chop\"]) / 0.5).clip(0, 1)\n",
    "    \n",
    "    # 6. range_z é«˜ (æ‰©å¼ )\n",
    "    scores[\"range_z_score\"] = (df[\"range_z\"] / 3).clip(0, 1)\n",
    "    \n",
    "    # 7. å¤šå°ºåº¦ä¸€è‡´æ€§\n",
    "    scores[\"alignment_score\"] = (df[\"trend_alignment\"].abs() / 3).clip(0, 1)\n",
    "    \n",
    "    # åŠ æƒå¹³å‡\n",
    "    weights = {\n",
    "        \"legA_score\": 0.15,\n",
    "        \"legB_score\": 0.15,\n",
    "        \"legC_score\": 0.10,\n",
    "        \"er_score\": 0.20,\n",
    "        \"chop_score\": 0.15,\n",
    "        \"range_z_score\": 0.10,\n",
    "        \"alignment_score\": 0.15,\n",
    "    }\n",
    "    \n",
    "    spike_score = sum(scores[col] * w for col, w in weights.items())\n",
    "    return spike_score.clip(0, 1)\n",
    "\n",
    "\n",
    "# ========== Channel Score ==========\n",
    "def calc_channel_score(df: pd.DataFrame, window: int = 10) -> pd.Series:\n",
    "    \"\"\"\n",
    "    è®¡ç®— Channel Score (è¶‹åŠ¿æˆç†Ÿ/è¡°ç«­åˆ†æ•°)\n",
    "    \n",
    "    ç»„æˆ:\n",
    "    1. Major leg >= 2\n",
    "    2. Micro leg å¢å¤š (legB_changes é«˜)\n",
    "    3. Fractal leg å¢å¤š (legC_changes é«˜)\n",
    "    4. é‡å åº¦é«˜\n",
    "    5. æ–œç‡è¡°å‡ (ä½†æ–¹å‘ä»ä¸€è‡´)\n",
    "    6. ER ä¸­ç­‰ (0.3-0.5)\n",
    "    \"\"\"\n",
    "    scores = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # 1. Micro leg å˜åŒ–å¤š (channel å†…éƒ¨æœ‰å¾ˆå¤šå°å›è°ƒ)\n",
    "    legB_changes = df[\"legB_changes_10\"]\n",
    "    scores[\"legB_score\"] = (legB_changes / 8).clip(0, 1)\n",
    "    \n",
    "    # 2. Fractal leg å˜åŒ–å¤š\n",
    "    legC_changes = df[\"legC_changes_10\"]\n",
    "    scores[\"legC_score\"] = (legC_changes / 6).clip(0, 1)\n",
    "    \n",
    "    # 3. é‡å åº¦é«˜\n",
    "    scores[\"overlap_score\"] = ((df[\"overlap_ratio\"] - 0.3) / 0.5).clip(0, 1)\n",
    "    \n",
    "    # 4. ER ä¸­ç­‰ (ä¸å¤ªé«˜ä¹Ÿä¸å¤ªä½)\n",
    "    er = df[\"er\"]\n",
    "    scores[\"er_score\"] = 1 - 2 * (er - 0.35).abs()  # 0.35 é™„è¿‘æœ€é«˜\n",
    "    scores[\"er_score\"] = scores[\"er_score\"].clip(0, 1)\n",
    "    \n",
    "    # 5. æ–œç‡ä»æœ‰ä½†åœ¨è¡°å‡ (slope_norm æ­£/è´Ÿä½†ç»å¯¹å€¼ < 0.05)\n",
    "    slope_abs = df[\"slope_norm\"].abs()\n",
    "    scores[\"slope_score\"] = ((0.1 - slope_abs) / 0.1 + 0.5).clip(0, 1)\n",
    "    \n",
    "    # 6. å°å®ä½“æ¯”ä¾‹é«˜ (channel å†…éƒ¨å¸¸è§)\n",
    "    scores[\"body_score\"] = ((0.6 - df[\"avg_body_ratio\"]) / 0.4).clip(0, 1)\n",
    "    \n",
    "    # åŠ æƒå¹³å‡\n",
    "    weights = {\n",
    "        \"legB_score\": 0.20,\n",
    "        \"legC_score\": 0.15,\n",
    "        \"overlap_score\": 0.20,\n",
    "        \"er_score\": 0.15,\n",
    "        \"slope_score\": 0.15,\n",
    "        \"body_score\": 0.15,\n",
    "    }\n",
    "    \n",
    "    channel_score = sum(scores[col] * w for col, w in weights.items())\n",
    "    return channel_score.clip(0, 1)\n",
    "\n",
    "\n",
    "# è®¡ç®—åˆ†æ•°\n",
    "df[\"spike_score\"] = calc_spike_score(df, PHASE_WINDOW)\n",
    "df[\"channel_score\"] = calc_channel_score(df, PHASE_WINDOW)\n",
    "\n",
    "# ========== Phase åˆ¤å®š ==========\n",
    "# phase: 0=normal, 1=spike, 2=channel\n",
    "SPIKE_THRESHOLD = 0.55\n",
    "CHANNEL_THRESHOLD = 0.50\n",
    "\n",
    "\n",
    "# --- 1) ç”¨â€œéæ ‡ç­¾ç‰¹å¾â€æ„é€ è¶‹åŠ¿ä»£ç† trend_proxy ---\n",
    "# å¯ä»¥æŠŠé˜ˆå€¼å½“æˆè¶…å‚è°ƒ\n",
    "TREND_SLOPE_THR = 0.02   # æ–œç‡å¤Ÿå¼º\n",
    "TREND_ER_THR    = 0.35   # ER å¤Ÿé«˜ï¼ˆè¶‹åŠ¿æ•ˆç‡ï¼‰\n",
    "TREND_CHOP_THR  = 0.55   # chop å¤Ÿä½ï¼ˆä¸å¤ªéœ‡è¡ï¼‰\n",
    "TREND_ALIGN_THR = 2      # å¤šå°ºåº¦ä¸€è‡´æ€§ï¼ˆ>=2 è¡¨ç¤ºå¤§æ¦‚ç‡åŒå‘ï¼‰\n",
    "\n",
    "trend_proxy = (\n",
    "    (df[\"slope_norm\"].abs() > TREND_SLOPE_THR) &\n",
    "    ((df[\"er\"] > TREND_ER_THR) | (df[\"chop\"] < TREND_CHOP_THR)) &\n",
    "    (df[\"trend_alignment\"].abs() >= TREND_ALIGN_THR)\n",
    ")\n",
    "\n",
    "df[\"trend_proxy_phase\"] = trend_proxy.astype(int)  # å¯é€‰ï¼šä¾¿äºè°ƒè¯•/ç”»å›¾\n",
    "\n",
    "# --- 2) phase ä»…åœ¨ trend_proxy=True æ—¶æ‰å¯èƒ½ä¸º 1/2 ---\n",
    "phase = np.zeros(len(df), dtype=np.int32)\n",
    "\n",
    "spike = df[\"spike_score\"].fillna(0).values\n",
    "channel = df[\"channel_score\"].fillna(0).values\n",
    "tp = trend_proxy.fillna(False).values\n",
    "\n",
    "phase[(tp) & (spike > SPIKE_THRESHOLD) & (spike > channel)] = 1\n",
    "phase[(tp) & (channel > CHANNEL_THRESHOLD) & (channel >= spike)] = 2\n",
    "# å…¶ä½™ä¿æŒ 0\n",
    "\n",
    "df[\"phase\"] = phase\n",
    "\n",
    "'''\n",
    "def determine_phase(spike: float, channel: float, regime: int) -> int:\n",
    "    \"\"\"\n",
    "    åˆ¤å®šå½“å‰é˜¶æ®µ\n",
    "    - å¦‚æœ regime == 0 (éœ‡è¡åŒºé—´)ï¼Œåˆ™ phase = 0\n",
    "    - å¦‚æœ spike_score é«˜ä¸”åœ¨è¶‹åŠ¿ä¸­ï¼Œåˆ™ phase = 1 (spike)\n",
    "    - å¦‚æœ channel_score é«˜ä¸”åœ¨è¶‹åŠ¿ä¸­ï¼Œåˆ™ phase = 2 (channel)\n",
    "    \"\"\"\n",
    "    if pd.isna(regime) or regime == 0:\n",
    "        return 0\n",
    "    \n",
    "    if spike > SPIKE_THRESHOLD and spike > channel:\n",
    "        return 1  # Spike\n",
    "    elif channel > CHANNEL_THRESHOLD:\n",
    "        return 2  # Channel\n",
    "    else:\n",
    "        return 0  # Normal trend\n",
    "\n",
    "\n",
    "df[\"phase\"] = df.apply(\n",
    "    lambda row: determine_phase(row[\"spike_score\"], row[\"channel_score\"], row[\"regime\"]),\n",
    "    axis=1\n",
    ")\n",
    "'''\n",
    "\n",
    "# ========== ç»Ÿè®¡ ==========\n",
    "print(\"\\n=== Spike & Channel è¯„åˆ†ç»Ÿè®¡ ===\")\n",
    "print(f\"spike_score: mean={df['spike_score'].mean():.4f}, max={df['spike_score'].max():.4f}\")\n",
    "print(f\"channel_score: mean={df['channel_score'].mean():.4f}, max={df['channel_score'].max():.4f}\")\n",
    "\n",
    "phase_counts = df[\"phase\"].value_counts().sort_index()\n",
    "print(f\"\\n=== Phase åˆ†å¸ƒ ===\")\n",
    "phase_names = {0: \"Normal/Range\", 1: \"Spike\", 2: \"Channel\"}\n",
    "for k, v in phase_counts.items():\n",
    "    print(f\"  {phase_names.get(k, k)}: {v:,} ({v/len(df)*100:.2f}%)\")\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Heiti TC', 'PingFang SC']\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Spike Score åˆ†å¸ƒ\n",
    "axes[0].hist(df[\"spike_score\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7, color=\"red\")\n",
    "axes[0].axvline(x=SPIKE_THRESHOLD, color=\"black\", linestyle=\"--\", label=f\"é˜ˆå€¼ {SPIKE_THRESHOLD}\")\n",
    "axes[0].set_xlabel(\"spike_score\")\n",
    "axes[0].set_ylabel(\"æ•°é‡\")\n",
    "axes[0].set_title(\"Spike Score åˆ†å¸ƒ\\n(è¶‹åŠ¿çˆ†å‘åˆ†æ•°)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Channel Score åˆ†å¸ƒ\n",
    "axes[1].hist(df[\"channel_score\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7, color=\"purple\")\n",
    "axes[1].axvline(x=CHANNEL_THRESHOLD, color=\"black\", linestyle=\"--\", label=f\"é˜ˆå€¼ {CHANNEL_THRESHOLD}\")\n",
    "axes[1].set_xlabel(\"channel_score\")\n",
    "axes[1].set_ylabel(\"æ•°é‡\")\n",
    "axes[1].set_title(\"Channel Score åˆ†å¸ƒ\\n(è¶‹åŠ¿è¡°ç«­åˆ†æ•°)\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Phase åˆ†å¸ƒ\n",
    "phase_counts.plot(kind=\"bar\", ax=axes[2], color=[\"gray\", \"red\", \"purple\"], edgecolor=\"black\")\n",
    "axes[2].set_xlabel(\"Phase\")\n",
    "axes[2].set_ylabel(\"æ•°é‡\")\n",
    "axes[2].set_title(\"Phase åˆ†å¸ƒ\")\n",
    "axes[2].set_xticklabels([\"Normal/Range\", \"Spike\", \"Channel\"], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCell 22 å®Œæˆ: Spike & Channel è¯„åˆ† âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 23: å…³é”®ä»·ä½ç‰¹å¾ (Key Price Levels) ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ 1.2(e) Brooks äº¤æ˜“å‘˜åœ¨ç›˜ä¸­éå¸¸çœ‹é‡çš„å…³é”®ä»·ä½:\n",
    "   - pos_in_day_range = (close - day_low) / (day_high - day_low)  âˆˆ [0,1]\n",
    "   - dist_to_open = (close - day_open) / ATR_day\n",
    "   - æ¥è¿‘å½“æ—¥é«˜/ä½ä¸”è¡Œæƒ…å·²ç»æ¶¨è·Œå¾ˆå¤š â‡’ æ›´å®¹æ˜“ trading range æˆ– climactic\n",
    "   - ä¸€ç›´è´´åœ¨ open è¿‘æ—ä¸Šä¸‹æ™ƒ â‡’ å…¸å‹\"å¼€ç›˜åŒºé—´\"çš„æ— æ–¹å‘éœ‡è¡\n",
    "\n",
    "æ³¨æ„: è¿™äº›ç‰¹å¾ä½¿ç”¨ cumulative çš„å½“æ—¥ä¿¡æ¯ï¼Œä¸æ³„éœ²æœªæ¥\n",
    "\"\"\"\n",
    "print(\"è®¡ç®—å…³é”®ä»·ä½ç‰¹å¾...\")\n",
    "\n",
    "# é¦–å…ˆéœ€è¦è¯†åˆ«æ¯ä¸ªäº¤æ˜“æ—¥\n",
    "df_temp = df.copy()\n",
    "df_temp = df_temp.reset_index()\n",
    "df_temp[\"timestamp_dt\"] = pd.to_datetime(df_temp[\"timestamp\"].astype(str).str.slice(0, 19))\n",
    "df_temp[\"date\"] = df_temp[\"timestamp_dt\"].dt.date\n",
    "df_temp = df_temp.set_index(\"timestamp\")\n",
    "\n",
    "# ========== è®¡ç®—æ¯æ—¥çš„å…³é”®ä»·ä½ (æ»šåŠ¨ç´¯ç§¯ï¼Œä¸æ³„éœ²æœªæ¥) ==========\n",
    "def calc_intraday_features(group):\n",
    "    \"\"\"\n",
    "    è®¡ç®—å•æ—¥å†…çš„å…³é”®ä»·ä½ç‰¹å¾ (ä½¿ç”¨ expanding ç¡®ä¿ä¸æ³„éœ²æœªæ¥)\n",
    "    \"\"\"\n",
    "    n = len(group)\n",
    "    \n",
    "    # å½“æ—¥å¼€ç›˜ä»· (ç¬¬ä¸€æ ¹ bar çš„ open)\n",
    "    day_open = group[\"open\"].iloc[0]\n",
    "    \n",
    "    # ç´¯ç§¯åˆ°å½“å‰çš„æœ€é«˜/æœ€ä½ä»· (expanding)\n",
    "    cum_high = group[\"high\"].expanding().max()\n",
    "    cum_low = group[\"low\"].expanding().min()\n",
    "    cum_range = cum_high - cum_low\n",
    "    \n",
    "    # å½“å‰ä»·æ ¼åœ¨æ—¥å†…åŒºé—´çš„ä½ç½® [0, 1]\n",
    "    pos_in_day_range = (group[\"close\"] - cum_low) / cum_range.replace(0, np.nan)\n",
    "    \n",
    "    # å½“å‰ä»·æ ¼ç›¸å¯¹å¼€ç›˜ä»·çš„è·ç¦» (æ ‡å‡†åŒ–)\n",
    "    # ä½¿ç”¨å½“æ—¥ç´¯ç§¯ ATR çš„å‡å€¼\n",
    "    cum_atr_mean = group[\"atr\"].expanding().mean()\n",
    "    dist_to_open = (group[\"close\"] - day_open) / cum_atr_mean.replace(0, np.nan)\n",
    "    \n",
    "    # å½“æ—¥ç´¯ç§¯æ”¶ç›Š (ç›¸å¯¹å¼€ç›˜)\n",
    "    day_return = (group[\"close\"] - day_open) / day_open\n",
    "    \n",
    "    # å½“æ—¥ç´¯ç§¯æ³¢åŠ¨ (ATR æ€»å’Œ / æ ¹æ•°)\n",
    "    day_volatility = group[\"atr\"].expanding().mean()\n",
    "    \n",
    "    # å½“æ—¥ bar åºå· (ä» 0 å¼€å§‹)\n",
    "    bar_of_day = np.arange(n)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"day_open\": day_open,\n",
    "        \"cum_high\": cum_high,\n",
    "        \"cum_low\": cum_low,\n",
    "        \"cum_range\": cum_range,\n",
    "        \"pos_in_day_range\": pos_in_day_range,\n",
    "        \"dist_to_open\": dist_to_open,\n",
    "        \"day_return\": day_return,\n",
    "        \"day_volatility\": day_volatility,\n",
    "        \"bar_of_day\": bar_of_day,\n",
    "    }, index=group.index)\n",
    "\n",
    "\n",
    "# æŒ‰æ—¥æœŸåˆ†ç»„è®¡ç®—\n",
    "print(\"  æŒ‰æ—¥æœŸåˆ†ç»„è®¡ç®—æ—¥å†…ç‰¹å¾...\")\n",
    "intraday_features = df_temp.groupby(\"date\").apply(calc_intraday_features)\n",
    "\n",
    "# å¤„ç† MultiIndex\n",
    "if isinstance(intraday_features.index, pd.MultiIndex):\n",
    "    intraday_features = intraday_features.droplevel(0)\n",
    "\n",
    "# åˆå¹¶åˆ°ä¸» df\n",
    "for col in [\"pos_in_day_range\", \"dist_to_open\", \"day_return\", \"bar_of_day\"]:\n",
    "    df[col] = intraday_features[col]\n",
    "\n",
    "# ========== é¢å¤–çš„æ—¥å†…ä½ç½®ç‰¹å¾ ==========\n",
    "# æ˜¯å¦æ¥è¿‘å½“æ—¥é«˜ç‚¹ (top 10%)\n",
    "df[\"near_day_high\"] = (df[\"pos_in_day_range\"] > 0.9).astype(int)\n",
    "# æ˜¯å¦æ¥è¿‘å½“æ—¥ä½ç‚¹ (bottom 10%)\n",
    "df[\"near_day_low\"] = (df[\"pos_in_day_range\"] < 0.1).astype(int)\n",
    "# æ˜¯å¦åœ¨å¼€ç›˜åŒºé—´ (dist_to_open çš„ç»å¯¹å€¼ < 0.5 ATR)\n",
    "df[\"near_open\"] = (df[\"dist_to_open\"].abs() < 0.5).astype(int)\n",
    "\n",
    "# ========== æ—¶é—´ç‰¹å¾ (æ— æ³„éœ²) ==========\n",
    "# å°† bar_of_day æ ‡å‡†åŒ– (ä¸€å¤©çº¦ 79 æ ¹ 5min bar for RTH)\n",
    "df[\"time_of_day_norm\"] = df[\"bar_of_day\"] / 79  # 0=å¼€ç›˜, 1=æ”¶ç›˜\n",
    "\n",
    "# å¼€ç›˜/æ”¶ç›˜åŒºé—´æ ‡è®°\n",
    "df[\"is_open_range\"] = (df[\"bar_of_day\"] < 6).astype(int)   # å‰30åˆ†é’Ÿ\n",
    "df[\"is_close_range\"] = (df[\"bar_of_day\"] > 70).astype(int)  # æœ€å45åˆ†é’Ÿ\n",
    "\n",
    "# ========== ç»Ÿè®¡ ==========\n",
    "print(\"\\n=== å…³é”®ä»·ä½ç‰¹å¾ç»Ÿè®¡ ===\")\n",
    "print(f\"pos_in_day_range: mean={df['pos_in_day_range'].mean():.4f}, std={df['pos_in_day_range'].std():.4f}\")\n",
    "print(f\"dist_to_open: mean={df['dist_to_open'].mean():.4f}, std={df['dist_to_open'].std():.4f}\")\n",
    "print(f\"day_return: mean={df['day_return'].mean():.6f}, std={df['day_return'].std():.6f}\")\n",
    "\n",
    "print(f\"\\n=== ä½ç½®åˆ†å¸ƒ ===\")\n",
    "print(f\"near_day_high æ¯”ä¾‹: {df['near_day_high'].mean()*100:.2f}%\")\n",
    "print(f\"near_day_low æ¯”ä¾‹: {df['near_day_low'].mean()*100:.2f}%\")\n",
    "print(f\"near_open æ¯”ä¾‹: {df['near_open'].mean()*100:.2f}%\")\n",
    "\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Heiti TC', 'PingFang SC']\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# pos_in_day_range åˆ†å¸ƒ\n",
    "axes[0].hist(df[\"pos_in_day_range\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].axvline(x=0.5, color=\"red\", linestyle=\"--\", label=\"ä¸­é—´ä½ç½®\")\n",
    "axes[0].set_xlabel(\"pos_in_day_range\")\n",
    "axes[0].set_ylabel(\"æ•°é‡\")\n",
    "axes[0].set_title(\"æ—¥å†…ä½ç½®åˆ†å¸ƒ\\n(0=æ—¥ä½, 1=æ—¥é«˜)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# dist_to_open åˆ†å¸ƒ\n",
    "axes[1].hist(df[\"dist_to_open\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7, color=\"orange\")\n",
    "axes[1].axvline(x=0, color=\"red\", linestyle=\"--\", label=\"å¼€ç›˜ä»·\")\n",
    "axes[1].set_xlabel(\"dist_to_open (ATR å•ä½)\")\n",
    "axes[1].set_ylabel(\"æ•°é‡\")\n",
    "axes[1].set_title(\"ç›¸å¯¹å¼€ç›˜ä»·è·ç¦»\\n(æ­£=é«˜äºå¼€ç›˜)\")\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(-5, 5)\n",
    "\n",
    "# bar_of_day åˆ†å¸ƒ (åº”è¯¥æ˜¯å‡åŒ€çš„)\n",
    "axes[2].hist(df[\"bar_of_day\"].dropna(), bins=79, edgecolor=\"black\", alpha=0.7, color=\"green\")\n",
    "axes[2].set_xlabel(\"bar_of_day\")\n",
    "axes[2].set_ylabel(\"æ•°é‡\")\n",
    "axes[2].set_title(\"æ—¥å†… Bar åºå·åˆ†å¸ƒ\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCell 23 å®Œæˆ: å…³é”®ä»·ä½ç‰¹å¾ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 24: å¤šæ—¶é—´å‘¨æœŸç‰¹å¾ (Multi-Timeframe Features) ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ Section 2: æŠŠ 15min / 1h çš„ä¿¡æ¯ä¹ŸåŠ å…¥è®­ç»ƒé›†\n",
    "\n",
    "æ€è·¯:\n",
    "1. å°† 5min æ•°æ® resample åˆ° 15min å’Œ 60min\n",
    "2. åœ¨é«˜å‘¨æœŸä¸Šè®¡ç®—ç›¸åŒçš„æŒ‡æ ‡ (KAMA, ER, ATR, slope, chop ç­‰)\n",
    "3. ä½¿ç”¨ floor å¯¹é½ï¼Œç¡®ä¿ä¸æ³„éœ²æœªæ¥ä¿¡æ¯\n",
    "4. åˆå¹¶å› 5min æ•°æ®\n",
    "\n",
    "è¿™æ ·æ¨¡å‹å¯ä»¥å­¦åˆ°:\n",
    "- \"å½“ 1h åœ¨ä¸Šå‡è¶‹åŠ¿ã€15min åœ¨éœ‡è¡æ—¶ï¼Œ5min çš„å‘ä¸‹æ³¢åŠ¨å¤šåŠæ˜¯ pullback\"\n",
    "\"\"\"\n",
    "print(\"è®¡ç®—å¤šæ—¶é—´å‘¨æœŸç‰¹å¾...\")\n",
    "\n",
    "# ========== 1. Resample to 15min and 60min ==========\n",
    "print(\"  Resampling åˆ° 15min å’Œ 60min...\")\n",
    "\n",
    "# ä¿å­˜åŸå§‹ 5min df\n",
    "df_5 = df.copy()\n",
    "\n",
    "# éœ€è¦å…ˆ reset index æ¥å¤„ç†\n",
    "df_5_reset = df_5.reset_index()\n",
    "df_5_reset[\"timestamp_dt\"] = pd.to_datetime(df_5_reset[\"timestamp\"].astype(str).str.slice(0, 19))\n",
    "df_5_reset = df_5_reset.set_index(\"timestamp_dt\")\n",
    "\n",
    "# Resample å‡½æ•°\n",
    "def resample_ohlcv(df_src: pd.DataFrame, freq: str) -> pd.DataFrame:\n",
    "    \"\"\"å°† OHLCV æ•°æ® resample åˆ°æŒ‡å®šé¢‘ç‡\"\"\"\n",
    "    df_resampled = df_src.resample(freq).agg({\n",
    "        \"open\": \"first\",\n",
    "        \"high\": \"max\",\n",
    "        \"low\": \"min\",\n",
    "        \"close\": \"last\",\n",
    "        \"volume\": \"sum\",\n",
    "    }).dropna()\n",
    "    return df_resampled\n",
    "\n",
    "df_15 = resample_ohlcv(df_5_reset, \"15T\")\n",
    "df_60 = resample_ohlcv(df_5_reset, \"60T\")\n",
    "\n",
    "print(f\"  15min bars: {len(df_15):,}\")\n",
    "print(f\"  60min bars: {len(df_60):,}\")\n",
    "\n",
    "# ========== 2. åœ¨é«˜å‘¨æœŸä¸Šè®¡ç®—æŒ‡æ ‡ ==========\n",
    "def calc_htf_features(df_htf: pd.DataFrame, suffix: str, \n",
    "                      n_atr: int = 14, n_er: int = 14, l_back: int = 14) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åœ¨é«˜å‘¨æœŸæ•°æ®ä¸Šè®¡ç®—æ ¸å¿ƒæŒ‡æ ‡\n",
    "    \"\"\"\n",
    "    df_htf = df_htf.copy()\n",
    "    \n",
    "    # Log close\n",
    "    df_htf[\"log_close\"] = np.log(df_htf[\"close\"])\n",
    "    \n",
    "    # ATR\n",
    "    prev_close = df_htf[\"close\"].shift(1)\n",
    "    tr1 = df_htf[\"high\"] - df_htf[\"low\"]\n",
    "    tr2 = (df_htf[\"high\"] - prev_close).abs()\n",
    "    tr3 = (df_htf[\"low\"] - prev_close).abs()\n",
    "    df_htf[\"tr\"] = np.maximum(np.maximum(tr1, tr2), tr3)\n",
    "    df_htf[f\"atr_{suffix}\"] = df_htf[\"tr\"].ewm(span=n_atr, adjust=False).mean()\n",
    "    \n",
    "    # ER\n",
    "    change = (df_htf[\"close\"] - df_htf[\"close\"].shift(n_er)).abs()\n",
    "    volatility = df_htf[\"close\"].diff().abs().rolling(window=n_er).sum()\n",
    "    df_htf[f\"er_{suffix}\"] = (change / volatility.replace(0, np.nan)).clip(0, 1)\n",
    "    \n",
    "    # Slope (ç®€åŒ–ç‰ˆ)\n",
    "    df_htf[f\"slope_{suffix}\"] = df_htf[\"log_close\"].diff(5) / 5\n",
    "    df_htf[f\"slope_{suffix}_norm\"] = df_htf[f\"slope_{suffix}\"] / df_htf[f\"atr_{suffix}\"].replace(0, np.nan) * 100\n",
    "    \n",
    "    # Chop (ç®€åŒ–ç‰ˆ)\n",
    "    chop_window = min(14, l_back)\n",
    "    atr_sum = df_htf[\"tr\"].rolling(window=chop_window).sum()\n",
    "    high_max = df_htf[\"high\"].rolling(window=chop_window).max()\n",
    "    low_min = df_htf[\"low\"].rolling(window=chop_window).min()\n",
    "    hl_range = high_max - low_min\n",
    "    df_htf[f\"chop_{suffix}\"] = (np.log10(atr_sum / hl_range.replace(0, np.nan)) / np.log10(chop_window)).clip(0, 1)\n",
    "    \n",
    "    # è¶‹åŠ¿æ–¹å‘ (åŸºäº slope)\n",
    "    df_htf[f\"trend_dir_{suffix}\"] = np.sign(df_htf[f\"slope_{suffix}\"])\n",
    "    \n",
    "    # åªè¿”å›éœ€è¦çš„åˆ—\n",
    "    result_cols = [f\"atr_{suffix}\", f\"er_{suffix}\", f\"slope_{suffix}_norm\", \n",
    "                   f\"chop_{suffix}\", f\"trend_dir_{suffix}\"]\n",
    "    \n",
    "    return df_htf[result_cols]\n",
    "\n",
    "\n",
    "print(\"  è®¡ç®— 15min æŒ‡æ ‡...\")\n",
    "df_15_features = calc_htf_features(df_15, \"15\", n_atr=14, n_er=14, l_back=14)\n",
    "\n",
    "print(\"  è®¡ç®— 60min æŒ‡æ ‡...\")\n",
    "df_60_features = calc_htf_features(df_60, \"60\", n_atr=14, n_er=14, l_back=14)\n",
    "\n",
    "# ========== 3. Floor å¯¹é½å¹¶ merge å› 5min ==========\n",
    "print(\"  å¯¹é½å¹¶åˆå¹¶åˆ° 5min æ•°æ®...\")\n",
    "\n",
    "# âš ï¸ é‡è¦ï¼šä½¿ç”¨å‰ä¸€ä¸ªå·²å®Œæˆçš„é«˜å‘¨æœŸ barï¼Œé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²\n",
    "# ä¾‹å¦‚ï¼šåœ¨ 9:35 æ—¶ï¼Œfloor(\"15T\") = 9:30ï¼Œä½† 9:30-9:45 çš„ 15min bar åŒ…å« 9:40 çš„æœªæ¥ä¿¡æ¯\n",
    "# æ­£ç¡®åšæ³•ï¼šå‡å»ä¸€ä¸ªå‘¨æœŸï¼Œä½¿ç”¨å‰ä¸€ä¸ªå·²å®Œæˆçš„ bar\n",
    "df_5_reset[\"t_15\"] = df_5_reset.index.floor(\"15T\") - pd.Timedelta(\"15min\")\n",
    "df_5_reset[\"t_60\"] = df_5_reset.index.floor(\"60T\") - pd.Timedelta(\"60min\")\n",
    "\n",
    "# Merge 15min features\n",
    "df_15_features = df_15_features.reset_index()\n",
    "df_15_features = df_15_features.rename(columns={\"timestamp_dt\": \"t_15\"})\n",
    "df_5_merged = df_5_reset.reset_index().merge(\n",
    "    df_15_features, on=\"t_15\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Merge 60min features\n",
    "df_60_features = df_60_features.reset_index()\n",
    "df_60_features = df_60_features.rename(columns={\"timestamp_dt\": \"t_60\"})\n",
    "df_5_merged = df_5_merged.merge(\n",
    "    df_60_features, on=\"t_60\", how=\"left\"\n",
    ")\n",
    "\n",
    "# å°†ç‰¹å¾æ·»åŠ å›åŸå§‹ df\n",
    "df_5_merged = df_5_merged.set_index(\"timestamp\")\n",
    "\n",
    "for col in [\"atr_15\", \"er_15\", \"slope_15_norm\", \"chop_15\", \"trend_dir_15\",\n",
    "            \"atr_60\", \"er_60\", \"slope_60_norm\", \"chop_60\", \"trend_dir_60\"]:\n",
    "    if col in df_5_merged.columns:\n",
    "        df[col] = df_5_merged[col]\n",
    "\n",
    "# ========== 4. å¤šå‘¨æœŸæ¯”ç‡ç‰¹å¾ ==========\n",
    "# ATR æ¯”ç‡ (æ³¢åŠ¨ç‡è·¨å‘¨æœŸæ¯”)\n",
    "df[\"atr_ratio_15_5\"] = df[\"atr_15\"] / df[\"atr\"].replace(0, np.nan)\n",
    "df[\"atr_ratio_60_5\"] = df[\"atr_60\"] / df[\"atr\"].replace(0, np.nan)\n",
    "\n",
    "# å¤šå‘¨æœŸè¶‹åŠ¿ä¸€è‡´æ€§\n",
    "df[\"mtf_trend_alignment\"] = (\n",
    "    np.sign(df[\"slope_norm\"]) + \n",
    "    df[\"trend_dir_15\"].fillna(0) + \n",
    "    df[\"trend_dir_60\"].fillna(0)\n",
    ")\n",
    "\n",
    "# ========== ç»Ÿè®¡ ==========\n",
    "print(\"\\n=== å¤šå‘¨æœŸç‰¹å¾ç»Ÿè®¡ ===\")\n",
    "print(f\"15min ER: mean={df['er_15'].mean():.4f}\")\n",
    "print(f\"60min ER: mean={df['er_60'].mean():.4f}\")\n",
    "print(f\"ATR ratio (15/5): mean={df['atr_ratio_15_5'].mean():.4f}\")\n",
    "print(f\"ATR ratio (60/5): mean={df['atr_ratio_60_5'].mean():.4f}\")\n",
    "\n",
    "mtf_counts = df[\"mtf_trend_alignment\"].value_counts().sort_index()\n",
    "print(f\"\\nå¤šå‘¨æœŸè¶‹åŠ¿ä¸€è‡´æ€§åˆ†å¸ƒ:\")\n",
    "for k, v in mtf_counts.items():\n",
    "    print(f\"  {int(k):+d}: {v:,} ({v/len(df)*100:.1f}%)\")\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Heiti TC', 'PingFang SC']\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# ER æ¯”è¾ƒ: 5min vs 15min vs 60min\n",
    "axes[0].hist(df[\"er\"].dropna(), bins=50, alpha=0.5, label=\"5min ER\")\n",
    "axes[0].hist(df[\"er_15\"].dropna(), bins=50, alpha=0.5, label=\"15min ER\")\n",
    "axes[0].hist(df[\"er_60\"].dropna(), bins=50, alpha=0.5, label=\"60min ER\")\n",
    "axes[0].set_xlabel(\"ER\")\n",
    "axes[0].set_ylabel(\"æ•°é‡\")\n",
    "axes[0].set_title(\"å¤šå‘¨æœŸ ER åˆ†å¸ƒæ¯”è¾ƒ\")\n",
    "axes[0].legend()\n",
    "\n",
    "# ATR æ¯”ç‡åˆ†å¸ƒ\n",
    "axes[1].hist(df[\"atr_ratio_15_5\"].dropna(), bins=50, alpha=0.7, label=\"15/5\")\n",
    "axes[1].hist(df[\"atr_ratio_60_5\"].dropna(), bins=50, alpha=0.5, label=\"60/5\")\n",
    "axes[1].set_xlabel(\"ATR æ¯”ç‡\")\n",
    "axes[1].set_ylabel(\"æ•°é‡\")\n",
    "axes[1].set_title(\"å¤šå‘¨æœŸ ATR æ¯”ç‡åˆ†å¸ƒ\")\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0, 5)\n",
    "\n",
    "# å¤šå‘¨æœŸè¶‹åŠ¿ä¸€è‡´æ€§\n",
    "mtf_counts.plot(kind=\"bar\", ax=axes[2], color=\"steelblue\", edgecolor=\"black\")\n",
    "axes[2].set_xlabel(\"MTF Trend Alignment\")\n",
    "axes[2].set_ylabel(\"æ•°é‡\")\n",
    "axes[2].set_title(\"å¤šå‘¨æœŸè¶‹åŠ¿ä¸€è‡´æ€§\\n(Â±3=å…¨å‘¨æœŸåŒå‘)\")\n",
    "axes[2].set_xticklabels([f\"{int(x)}\" for x in mtf_counts.index], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCell 24 å®Œæˆ: å¤šæ—¶é—´å‘¨æœŸç‰¹å¾ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 25: æ»åç‰¹å¾ (Lagged Features) ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ Section 3: åŠ å…¥å‰å‡ ä¸ª K çº¿çš„æ ‡ç­¾å’Œæ»åç‰¹å¾\n",
    "\n",
    "æ·»åŠ :\n",
    "1. å†å² regime æ ‡ç­¾: regime_lag1, regime_lag2, regime_lag3ï¼Œå¦‚æœè¿‡å»å‡ æ ¹éƒ½åœ¨è¶‹åŠ¿ï¼Œé‚£å½“å‰æ›´å¯èƒ½ä¹Ÿåœ¨è¶‹åŠ¿\n",
    "2. å†å²æ ¸å¿ƒç‰¹å¾çš„æ»åç‰ˆæœ¬\n",
    "3. çª—å£ç»Ÿè®¡ç‰¹å¾ (å¦‚: æœ€è¿‘ 10 æ ¹ä¸­ regime=Â±1 çš„æ¯”ä¾‹)\n",
    "\n",
    "è¿™äº›ç›´æ¥æ¨¡æ‹Ÿ \"ç›˜æ„Ÿ\"ï¼š\n",
    "- æ¨¡å‹çœ‹åˆ°\"è¿ç»­ä¸‰æ ¹ regime=+1 ä¸” slope_norm éƒ½æ”¾å¤§\"ï¼Œè‡ªç„¶ä¼šæŠŠå½“å‰ä¹Ÿåˆ¤ä¸ºè¶‹åŠ¿æ®µ\n",
    "\"\"\"\n",
    "print(\"è®¡ç®—æ»åç‰¹å¾...\")\n",
    "\n",
    "# ========== é…ç½® ==========\n",
    "LAG_K = 3  # æ»åæœŸæ•°\n",
    "WINDOW_N = 10  # ç»Ÿè®¡çª—å£\n",
    "\n",
    "# ========== 1. å†å² regime æ ‡ç­¾ ==========\n",
    "print(\"  æ·»åŠ  regime æ»åç‰¹å¾...\")\n",
    "for lag in range(1, LAG_K + 1):\n",
    "    df[f\"regime_lag{lag}\"] = df[\"regime\"].shift(lag)\n",
    "\n",
    "# ========== 2. æ ¸å¿ƒç‰¹å¾çš„æ»åç‰ˆæœ¬ ==========\n",
    "print(\"  æ·»åŠ æ ¸å¿ƒç‰¹å¾æ»åç‰ˆæœ¬...\")\n",
    "\n",
    "# é€‰æ‹©éœ€è¦åšæ»åçš„æ ¸å¿ƒç‰¹å¾\n",
    "lag_features = [\n",
    "    \"er\",\n",
    "    \"slope_norm\",\n",
    "    \"dist_norm\",\n",
    "    \"chop\",\n",
    "    \"r2\",\n",
    "    \"range_z\",\n",
    "    \"dir_vol_ratio\",\n",
    "    \"bull_bear_diff\",\n",
    "    \"avg_body_ratio\",\n",
    "]\n",
    "\n",
    "for col in lag_features:\n",
    "    if col in df.columns:\n",
    "        for lag in range(1, LAG_K + 1):\n",
    "            df[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "\n",
    "# ========== 3. çª—å£ç»Ÿè®¡ç‰¹å¾ ==========\n",
    "print(\"  æ·»åŠ çª—å£ç»Ÿè®¡ç‰¹å¾...\")\n",
    "\n",
    "# æœ€è¿‘ N æ ¹ä¸­ regime çš„ç»Ÿè®¡\n",
    "# è¶‹åŠ¿æ¯”ä¾‹ (regime = Â±1 çš„æ¯”ä¾‹)\n",
    "df[\"regime_trend_ratio\"] = df[\"regime\"].abs().rolling(WINDOW_N).mean()\n",
    "\n",
    "# ä¸Šæ¶¨è¶‹åŠ¿æ¯”ä¾‹ (regime = +1 çš„æ¯”ä¾‹)\n",
    "df[\"regime_up_ratio\"] = (df[\"regime\"] == 1).astype(int).rolling(WINDOW_N).mean()\n",
    "\n",
    "# ä¸‹è·Œè¶‹åŠ¿æ¯”ä¾‹ (regime = -1 çš„æ¯”ä¾‹)\n",
    "df[\"regime_down_ratio\"] = (df[\"regime\"] == -1).astype(int).rolling(WINDOW_N).mean()\n",
    "\n",
    "# regime å˜åŒ–æ¬¡æ•° (å¸‚åœºç»“æ„å˜åŒ–é¢‘ç‡)\n",
    "df[\"regime_change\"] = (df[\"regime\"] != df[\"regime\"].shift(1)).astype(int)\n",
    "df[\"regime_changes_N\"] = df[\"regime_change\"].rolling(WINDOW_N).sum()\n",
    "\n",
    "# è¿ç»­ regime æ ‡ç­¾æ•°é‡ (æ¨¡æ‹Ÿ\"è¶‹åŠ¿æŒç»­æ€§\")\n",
    "def calc_consecutive_regime(regime: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"è®¡ç®—è¿ç»­ç›¸åŒ regime çš„é•¿åº¦\"\"\"\n",
    "    n = len(regime)\n",
    "    consec = np.zeros(n, dtype=np.int32)\n",
    "    \n",
    "    for i in range(n):\n",
    "        if i == 0 or np.isnan(regime[i]) or np.isnan(regime[i-1]):\n",
    "            consec[i] = 1\n",
    "        elif regime[i] == regime[i-1]:\n",
    "            consec[i] = consec[i-1] + 1\n",
    "        else:\n",
    "            consec[i] = 1\n",
    "    \n",
    "    return consec\n",
    "\n",
    "df[\"regime_consec\"] = calc_consecutive_regime(df[\"regime\"].values.astype(np.float64))\n",
    "\n",
    "# ========== 4. ç‰¹å¾å˜åŒ–ç‡ (momentum of features) ==========\n",
    "print(\"  æ·»åŠ ç‰¹å¾å˜åŒ–ç‡...\")\n",
    "\n",
    "# æ ¸å¿ƒç‰¹å¾çš„å˜åŒ–ç‡\n",
    "momentum_features = [\"er\", \"slope_norm\", \"chop\", \"range_z\"]\n",
    "\n",
    "for col in momentum_features:\n",
    "    if col in df.columns:\n",
    "        # 1 æœŸå˜åŒ–\n",
    "        df[f\"{col}_diff1\"] = df[col].diff(1)\n",
    "        # 3 æœŸå˜åŒ–\n",
    "        df[f\"{col}_diff3\"] = df[col].diff(3)\n",
    "\n",
    "# ========== 5. è¶‹åŠ¿åŠ é€Ÿ/å‡é€ŸæŒ‡æ ‡ ==========\n",
    "# slope_norm çš„å˜åŒ– (è¶‹åŠ¿åŠ é€Ÿ)\n",
    "df[\"slope_accel\"] = df[\"slope_norm\"].diff(1)\n",
    "df[\"slope_accel_ma3\"] = df[\"slope_accel\"].rolling(3).mean()\n",
    "\n",
    "# ========== ç»Ÿè®¡ ==========\n",
    "print(\"\\n=== æ»åç‰¹å¾ç»Ÿè®¡ ===\")\n",
    "print(f\"regime_trend_ratio: mean={df['regime_trend_ratio'].mean():.4f} (è¶‹åŠ¿å æ¯”)\")\n",
    "print(f\"regime_changes_N: mean={df['regime_changes_N'].mean():.2f} (æ¯{WINDOW_N}barå˜åŒ–æ¬¡æ•°)\")\n",
    "print(f\"regime_consec: max={df['regime_consec'].max()}, mean={df['regime_consec'].mean():.2f}\")\n",
    "\n",
    "# æ£€æŸ¥æ»åç‰¹å¾æ•°é‡\n",
    "lag_cols = [c for c in df.columns if \"_lag\" in c or \"_diff\" in c]\n",
    "print(f\"\\næ–°å¢æ»å/å˜åŒ–ç‰¹å¾æ•°é‡: {len(lag_cols)}\")\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Heiti TC', 'PingFang SC']\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# regime_trend_ratio åˆ†å¸ƒ\n",
    "axes[0].hist(df[\"regime_trend_ratio\"].dropna(), bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].set_xlabel(\"regime_trend_ratio\")\n",
    "axes[0].set_ylabel(\"æ•°é‡\")\n",
    "axes[0].set_title(f\"è¶‹åŠ¿å æ¯”åˆ†å¸ƒ (çª—å£={WINDOW_N})\\n(é«˜=æ›´å¤šè¶‹åŠ¿)\")\n",
    "\n",
    "# regime_consec åˆ†å¸ƒ\n",
    "consec_counts = df[\"regime_consec\"].value_counts().sort_index()\n",
    "consec_counts_plot = consec_counts[consec_counts.index <= 30]  # é™åˆ¶æ˜¾ç¤ºèŒƒå›´\n",
    "axes[1].bar(consec_counts_plot.index, consec_counts_plot.values, edgecolor=\"black\", alpha=0.7)\n",
    "axes[1].set_xlabel(\"regime_consec\")\n",
    "axes[1].set_ylabel(\"æ•°é‡\")\n",
    "axes[1].set_title(\"è¿ç»­ç›¸åŒ Regime é•¿åº¦åˆ†å¸ƒ\")\n",
    "\n",
    "# slope_accel åˆ†å¸ƒ\n",
    "axes[2].hist(df[\"slope_accel\"].dropna(), bins=50, edgecolor=\"black\", alpha=0.7, color=\"purple\")\n",
    "axes[2].axvline(x=0, color=\"red\", linestyle=\"--\")\n",
    "axes[2].set_xlabel(\"slope_accel\")\n",
    "axes[2].set_ylabel(\"æ•°é‡\")\n",
    "axes[2].set_title(\"æ–œç‡åŠ é€Ÿåº¦åˆ†å¸ƒ\\n(æ­£=è¶‹åŠ¿åŠ é€Ÿ)\")\n",
    "axes[2].set_xlim(-0.02, 0.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCell 25 å®Œæˆ: æ»åç‰¹å¾ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 11: å¤šæ—¶é—´å‘¨æœŸè¾…åŠ© Regime æ ‡æ³¨ (MTF-Assisted Labeling) ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ ç›®æ ‡: åœ¨æ‰“æ ‡ç­¾æ—¶è®©é«˜å‘¨æœŸç»“æ„æˆä¸º\"é¢å¤–çš„è£åˆ¤\"\n",
    "  - 5m è¶‹åŠ¿å¦‚æœå’Œ 60m å®Œå…¨åå‘ â†’ é™æƒæˆ–æ‰“å› 0 (å¯èƒ½æ˜¯ pullback)\n",
    "  - 5m éœ‡è¡å¦‚æœåœ¨å¼º 60m è¶‹åŠ¿ä¸­ â†’ æ ‡è®°ä¸º \"range_inside_trend\"\n",
    "  \n",
    "ğŸ“Œ è®¾è®¡åŸåˆ™:\n",
    "  - 5min ç‰¹å¾å  ~80% å†³ç­–æƒé‡\n",
    "  - 15min/60min å  ~20%ï¼Œä»…èµ·è¾…åŠ©ä½œç”¨\n",
    "  - å¤šå‘¨æœŸåŒå‘æ—¶å¢åŠ ç½®ä¿¡åº¦\n",
    "  - é«˜å‘¨æœŸå¼ºåå‘æ—¶é™ä½è¶‹åŠ¿æ¦‚ç‡\n",
    "\"\"\"\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”„ å¤šæ—¶é—´å‘¨æœŸè¾…åŠ© Regime æ ‡æ³¨\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========== 1. å®šä¹‰é«˜å‘¨æœŸå¼ºè¶‹åŠ¿æ¡ä»¶ ==========\n",
    "# 15min å¼ºè¶‹åŠ¿\n",
    "STRONG_15_SLOPE_THR = 0.8\n",
    "STRONG_15_ER_THR = 0.4\n",
    "STRONG_15_CHOP_THR = 0.6\n",
    "\n",
    "# 60min å¼ºè¶‹åŠ¿\n",
    "STRONG_60_SLOPE_THR = 1.0\n",
    "STRONG_60_ER_THR = 0.4\n",
    "STRONG_60_CHOP_THR = 0.6\n",
    "\n",
    "# MTF æƒé‡\n",
    "W_15 = 0.15  # 15min æƒé‡\n",
    "W_60 = 0.15  # 60min æƒé‡\n",
    "THR_TREND_SCORE = 1.0  # è¶‹åŠ¿å¾—åˆ†é˜ˆå€¼\n",
    "\n",
    "# è®¡ç®—é«˜å‘¨æœŸå¼ºè¶‹åŠ¿æ ‡è®°\n",
    "df[\"strong_15\"] = (\n",
    "    (df[\"slope_15_norm\"].abs() > STRONG_15_SLOPE_THR) & \n",
    "    (df[\"er_15\"] > STRONG_15_ER_THR) & \n",
    "    (df[\"chop_15\"] < STRONG_15_CHOP_THR)\n",
    ").astype(int)\n",
    "\n",
    "df[\"strong_60\"] = (\n",
    "    (df[\"slope_60_norm\"].abs() > STRONG_60_SLOPE_THR) & \n",
    "    (df[\"er_60\"] > STRONG_60_ER_THR) & \n",
    "    (df[\"chop_60\"] < STRONG_60_CHOP_THR)\n",
    ").astype(int)\n",
    "\n",
    "print(f\"\\nå¼º 15min è¶‹åŠ¿ bar æ¯”ä¾‹: {df['strong_15'].mean()*100:.1f}%\")\n",
    "print(f\"å¼º 60min è¶‹åŠ¿ bar æ¯”ä¾‹: {df['strong_60'].mean()*100:.1f}%\")\n",
    "\n",
    "# ========== 2. å¸¦ MTF è¾…åŠ©çš„è¶‹åŠ¿è¯†åˆ« ==========\n",
    "def identify_trend_with_mtf(df: pd.DataFrame, is_range: np.ndarray,\n",
    "                             w_15: float = 0.15, w_60: float = 0.15,\n",
    "                             thr_score: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    å¸¦å¤šæ—¶é—´å‘¨æœŸè¾…åŠ©çš„è¶‹åŠ¿è¯†åˆ«\n",
    "    \n",
    "    è¶‹åŠ¿å¾—åˆ† = base_score + mtf_bonus - mtf_penalty\n",
    "    - base_score: åŸºäº 5min çš„åŸå§‹æ¡ä»¶ (0-1)\n",
    "    - mtf_bonus: é«˜å‘¨æœŸåŒå‘åŠ åˆ†\n",
    "    - mtf_penalty: é«˜å‘¨æœŸåå‘å‡åˆ†\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    regime = np.zeros(n, dtype=np.int32)\n",
    "    trend_scores = np.zeros(n, dtype=np.float64)\n",
    "    \n",
    "    # é¢„æå–æ•°ç»„ (åŠ é€Ÿ)\n",
    "    er = df[\"er\"].values\n",
    "    chop = df[\"chop\"].values\n",
    "    slope_norm = df[\"slope_norm\"].values\n",
    "    dist_norm = df[\"dist_norm\"].values\n",
    "    d_barrier = df[\"d_barrier\"].values\n",
    "    close = df[\"close\"].values\n",
    "    \n",
    "    # é«˜å‘¨æœŸä¿¡æ¯\n",
    "    trend_dir_15 = df[\"trend_dir_15\"].fillna(0).values\n",
    "    trend_dir_60 = df[\"trend_dir_60\"].fillna(0).values\n",
    "    strong_15 = df[\"strong_15\"].fillna(0).values\n",
    "    strong_60 = df[\"strong_60\"].fillna(0).values\n",
    "    er_15 = df[\"er_15\"].fillna(0).values\n",
    "    er_60 = df[\"er_60\"].fillna(0).values\n",
    "    \n",
    "    for t in range(n): \n",
    "        if is_range[t]: # éœ‡è¡ç›´æ¥æ ‡è®°ä¸º 0\n",
    "            regime[t] = 0\n",
    "            continue\n",
    "        \n",
    "        if np.isnan(slope_norm[t]) or np.isnan(er[t]):\n",
    "            continue\n",
    "        \n",
    "        # ========== 5min åŸºç¡€å¾—åˆ† ==========\n",
    "        dir_5 = np.sign(slope_norm[t])\n",
    "        if dir_5 == 0:\n",
    "            continue\n",
    "        \n",
    "        # æ–¹å‘ä¸€è‡´æ€§ (ä½¿ç”¨æœªæ¥ L_FWD çš„æ”¶ç›Š)\n",
    "        future_idx = min(t + L_FWD, n - 1)\n",
    "        future_direction = np.sign(close[future_idx] - close[t])\n",
    "        direction_consistent = (dir_5 == future_direction) and (dir_5 != 0)\n",
    "        \n",
    "        # æ„å»º base_score (0-1 åŒºé—´)\n",
    "        base_score = 0.0\n",
    "        \n",
    "        # æ–¹å‘ä¸€è‡´ +0.3\n",
    "        if direction_consistent:\n",
    "            base_score += 0.3\n",
    "        \n",
    "        # æ–œç‡å¼ºåº¦ +0.2\n",
    "        if abs(slope_norm[t]) > THR_SLOPE_NORM:\n",
    "            base_score += 0.2\n",
    "        \n",
    "        # è¶‹åŠ¿è´¨é‡ (ER é«˜æˆ– chop ä½) +0.25\n",
    "        if er[t] > THR_ER_HIGH or chop[t] < THR_CHOP_LOW:\n",
    "            base_score += 0.25\n",
    "        \n",
    "        # ä»·æ ¼åç¦» +0.15\n",
    "        if abs(dist_norm[t]) > THR_DIST_NORM:\n",
    "            base_score += 0.15\n",
    "        \n",
    "        # Triple barrier ç¡®è®¤ +0.1\n",
    "        if d_barrier[t] == dir_5:\n",
    "            base_score += 0.1\n",
    "        \n",
    "        # ========== MTF åŠ å‡åˆ† ==========\n",
    "        mtf_adjustment = 0.0\n",
    "        \n",
    "        # 15min åŒå‘åŠ åˆ†\n",
    "        if trend_dir_15[t] == dir_5 and trend_dir_15[t] != 0:\n",
    "            mtf_adjustment += w_15\n",
    "        \n",
    "        # 60min åŒå‘åŠ åˆ†\n",
    "        if trend_dir_60[t] == dir_5 and trend_dir_60[t] != 0:\n",
    "            mtf_adjustment += w_60\n",
    "        \n",
    "        # é«˜å‘¨æœŸå¼ºåå‘ â†’ å¤§å¹…å‡åˆ† (å¯èƒ½æ˜¯ counter-trend pullback)\n",
    "        if strong_60[t] and trend_dir_60[t] == -dir_5:\n",
    "            final_score *= 0.5\n",
    "        \n",
    "        # å¤šå‘¨æœŸå®Œå…¨æ··ä¹± â†’ å°å¹…é™æƒ\n",
    "        mtf_align = dir_5 + trend_dir_15[t] + trend_dir_60[t]\n",
    "        if abs(mtf_align) <= 1:\n",
    "            base_score *= 0.85\n",
    "        \n",
    "        # æœ€ç»ˆå¾—åˆ†\n",
    "        final_score = base_score + mtf_adjustment\n",
    "        trend_scores[t] = final_score\n",
    "        \n",
    "        # åˆ¤å®š\n",
    "        if final_score >= thr_score:\n",
    "            regime[t] = int(dir_5)\n",
    "    \n",
    "    return regime, trend_scores\n",
    "\n",
    "\n",
    "# ========== 3. å¸¦ MTF è¿‡æ»¤çš„éœ‡è¡è¯†åˆ« ==========\n",
    "def identify_range_with_mtf_filter(df: pd.DataFrame, is_range_5m: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    åœ¨ 5min éœ‡è¡åŸºç¡€ä¸Šï¼Œæ ‡è®°\"å¼ºè¶‹åŠ¿ä¸­çš„éœ‡è¡\"\n",
    "    \n",
    "    è¿”å›:\n",
    "        is_range_final: æœ€ç»ˆéœ‡è¡æ ‡è®°\n",
    "        is_range_in_trend: å¼ºè¶‹åŠ¿ä¸­çš„éœ‡è¡æ ‡è®° (ä»è®­ç»ƒé›†å‰”é™¤)\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    is_range_final = is_range_5m.copy()\n",
    "    is_range_in_trend = np.zeros(n, dtype=bool)\n",
    "    \n",
    "    strong_60 = df[\"strong_60\"].fillna(0).values\n",
    "    er_60 = df[\"er_60\"].fillna(0.5).values\n",
    "    chop_60 = df[\"chop_60\"].fillna(0.5).values\n",
    "    \n",
    "    for t in range(n):\n",
    "        if is_range_5m[t]:\n",
    "            # å¦‚æœ 60min æ˜¯å¼ºè¶‹åŠ¿ï¼Œæ ‡è®°ä¸º \"range_inside_trend\"\n",
    "            if strong_60[t] and er_60[t] > 0.45 and chop_60[t] < 0.55:\n",
    "                is_range_in_trend[t] = True\n",
    "                # å¯é€‰: ä¿æŒ is_range_final[t] = True (ä»ç„¶æ˜¯éœ‡è¡ï¼Œä½†è®­ç»ƒæ—¶å‰”é™¤)\n",
    "    \n",
    "    return is_range_final, is_range_in_trend\n",
    "\n",
    "\n",
    "# ========== 4. æ‰§è¡Œ MTF è¾…åŠ©æ ‡æ³¨ ==========\n",
    "print(\"\\næ‰§è¡Œ MTF è¾…åŠ©æ ‡æ³¨...\")\n",
    "\n",
    "# ä½¿ç”¨åŸå§‹çš„ 5m éœ‡è¡åˆ¤å®š\n",
    "is_range_5m = df[\"is_range_extended\"].values if \"is_range_extended\" in df.columns else df[\"is_range\"].values\n",
    "\n",
    "# MTF è¿‡æ»¤éœ‡è¡\n",
    "is_range_mtf, is_range_in_trend = identify_range_with_mtf_filter(df, is_range_5m)\n",
    "\n",
    "# MTF è¾…åŠ©è¶‹åŠ¿è¯†åˆ«\n",
    "regime_mtf_raw, trend_scores = identify_trend_with_mtf(\n",
    "    df, is_range_mtf, \n",
    "    w_15=W_15, w_60=W_60, thr_score=THR_TREND_SCORE\n",
    ")\n",
    "\n",
    "# åº”ç”¨å¹³æ»‘ (ä¸åŸæ–¹æ³•ç›¸åŒ)\n",
    "regime_mtf = smooth_regime_labels(regime_mtf_raw, MIN_TREND_LEN, SMOOTH_WINDOW)\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "df[\"regime_mtf\"] = regime_mtf\n",
    "df[\"trend_score_mtf\"] = trend_scores\n",
    "df[\"is_range_in_trend\"] = is_range_in_trend\n",
    "\n",
    "# ========== 5. ç»Ÿè®¡å¯¹æ¯” ==========\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“Š åŸå§‹ vs MTF è¾…åŠ© Regime åˆ†å¸ƒå¯¹æ¯”\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# åŸå§‹åˆ†å¸ƒ\n",
    "orig_counts = df[\"regime\"].value_counts().sort_index()\n",
    "mtf_counts = df[\"regime_mtf\"].value_counts().sort_index()\n",
    "\n",
    "print(\"\\nåŸå§‹æ–¹æ³•:\")\n",
    "for k, v in orig_counts.items():\n",
    "    label = {-1: \"DOWN\", 0: \"RANGE\", 1: \"UP\"}.get(k, str(k))\n",
    "    print(f\"  {label}: {v:,} ({v/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nMTF è¾…åŠ©æ–¹æ³•:\")\n",
    "for k, v in mtf_counts.items():\n",
    "    label = {-1: \"DOWN\", 0: \"RANGE\", 1: \"UP\"}.get(k, str(k))\n",
    "    print(f\"  {label}: {v:,} ({v/len(df)*100:.2f}%)\")\n",
    "\n",
    "# æ ‡ç­¾å˜åŒ–ç»Ÿè®¡\n",
    "changed = (df[\"regime\"] != df[\"regime_mtf\"]).sum()\n",
    "print(f\"\\næ ‡ç­¾å˜åŒ–æ•°é‡: {changed:,} ({changed/len(df)*100:.2f}%)\")\n",
    "\n",
    "# å¼ºè¶‹åŠ¿ä¸­çš„éœ‡è¡æ•°é‡\n",
    "range_in_trend_count = df[\"is_range_in_trend\"].sum()\n",
    "print(f\"å¼ºè¶‹åŠ¿ä¸­çš„éœ‡è¡ bar: {range_in_trend_count:,} ({range_in_trend_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nCell 11 å®Œæˆ: MTF è¾…åŠ©æ ‡æ³¨ âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell: åŸå§‹ vs MTF å¯¹æ¯”ï¼ˆä¿®å¤ç‰ˆï¼šreset_output + æ¯æ¬¡é‡å»ºæ¨¡å‹ï¼‰ ==========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from bokeh.layouts import row as bokeh_row, column\n",
    "from bokeh.io import output_file, save, show, reset_output, output_notebook\n",
    "from bokeh.models import BoxAnnotation, ColumnDataSource, HoverTool, Range1d\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "# ç¡®ä¿ Notebook è¾“å‡ºå·²å¯ç”¨\n",
    "output_notebook()\n",
    "\n",
    "print(\"ç”ŸæˆåŸå§‹ vs MTF å¯¹æ¯”å›¾è¡¨ï¼ˆä¿®æ­£ç‰ˆï¼Œä½¿ç”¨ reset_output()ï¼‰...\")\n",
    "\n",
    "# è¾“å‡ºç›®å½•å…œåº•\n",
    "if \"OUTPUT_DIR_CHARTS\" not in globals():\n",
    "    OUTPUT_DIR_CHARTS = Path(\"./charts\")\n",
    "OUTPUT_DIR_CHARTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# å‡†å¤‡ç»˜å›¾æ•°æ®ï¼ˆç¡®ä¿å˜é‡ df å·²åœ¨ç¯å¢ƒä¸­ï¼‰\n",
    "df_plot = df.reset_index().copy()\n",
    "df_plot[\"timestamp_dt\"] = pd.to_datetime(df_plot[\"timestamp\"].astype(str).str.slice(0, 19))\n",
    "df_plot[\"date\"] = df_plot[\"timestamp_dt\"].dt.date\n",
    "\n",
    "# å¿…è¦åˆ—æ£€æŸ¥\n",
    "required_cols = [\"open\", \"high\", \"low\", \"close\", \"regime\", \"regime_mtf\", \"date\", \"timestamp_dt\"]\n",
    "missing = [c for c in required_cols if c not in df_plot.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"ç¼ºå°‘å¿…è¦åˆ—: {missing}\")\n",
    "\n",
    "# æœ‰æ•ˆäº¤æ˜“æ—¥ï¼ˆè‡³å°‘ 50 æ ¹ barï¼‰\n",
    "day_counts = df_plot.groupby(\"date\").size()\n",
    "valid_dates = day_counts[day_counts >= 50].index.tolist()\n",
    "if len(valid_dates) == 0:\n",
    "    raise ValueError(\"æ²¡æœ‰æ»¡è¶³ >=50 bars çš„äº¤æ˜“æ—¥å¯ç»˜åˆ¶ï¼Œè¯·æ£€æŸ¥æ•°æ®æˆ–æ ‡ç­¾æ˜¯å¦å¤§é‡ NaNã€‚\")\n",
    "\n",
    "# éšæœºé€‰æ‹©è‹¥å¹²æ—¥æœŸ\n",
    "np.random.seed(None) # 42çš„æ„æ€æ˜¯éšæœºä½†å¯å¤ç°ï¼Œå¦‚æœéœ€è¦æ¯æ¬¡ä¸åŒå¯æ”¹ä¸º None\n",
    "sample_dates = sorted(np.random.choice(valid_dates, size=min(5, len(valid_dates)), replace=False))\n",
    "print(f\"é€‰æ‹© {len(sample_dates)} ä¸ªæ—¥æœŸè¿›è¡Œå¯¹æ¯”:\")\n",
    "for d in sample_dates:\n",
    "    print(f\"  - {d}\")\n",
    "\n",
    "# å•æ—¥ç»˜å›¾å‡½æ•°ï¼ˆæ¯æ¬¡è°ƒç”¨éƒ½ä¼šåˆ›å»ºå…¨æ–°æ¨¡å‹ï¼‰\n",
    "def plot_kline_comparison(df_day: pd.DataFrame, regime_col: str, title: str, width: int = 600, height: int = 350):\n",
    "    d = df_day.copy().reset_index(drop=True)\n",
    "    d[\"idx\"] = np.arange(len(d))\n",
    "    d[\"regime_filled\"] = d[regime_col].fillna(0).astype(int)\n",
    "    d[\"bar_color\"] = np.where(d[\"close\"] >= d[\"open\"], \"#26a69a\", \"#ef5350\")\n",
    "    d[\"top\"] = np.maximum(d[\"open\"], d[\"close\"])\n",
    "    d[\"bottom\"] = np.minimum(d[\"open\"], d[\"close\"])\n",
    "\n",
    "    source = ColumnDataSource(d)\n",
    "\n",
    "    y_min = float(d[\"low\"].min() * 0.9995)\n",
    "    y_max = float(d[\"high\"].max() * 1.0005)\n",
    "\n",
    "    p = figure(\n",
    "        width=width, height=height,\n",
    "        title=title,\n",
    "        x_range=Range1d(-1, len(d)),\n",
    "        y_range=Range1d(y_min, y_max),\n",
    "        tools=\"pan,wheel_zoom,box_zoom,reset,save,crosshair\",\n",
    "        background_fill_color=\"#fafafa\",\n",
    "    )\n",
    "\n",
    "    # regime èƒŒæ™¯è‰²æŒ‰è¿ç»­æ®µç»˜åˆ¶ï¼ˆBoxAnnotationï¼‰\n",
    "    regime_colors = {-1: \"#ffcccc\", 0: \"#ffffcc\", 1: \"#ccffcc\"}\n",
    "    d[\"regime_change\"] = (d[\"regime_filled\"] != d[\"regime_filled\"].shift()).cumsum()\n",
    "    groups = d.groupby(\"regime_change\").agg(start_idx=(\"idx\", \"first\"), end_idx=(\"idx\", \"last\"), regime=(\"regime_filled\", \"first\"))\n",
    "\n",
    "    for _, g in groups.iterrows():\n",
    "        r = int(g[\"regime\"])\n",
    "        box = BoxAnnotation(\n",
    "            left=float(g[\"start_idx\"]) - 0.5,\n",
    "            right=float(g[\"end_idx\"]) + 0.5,\n",
    "            fill_color=regime_colors.get(r, \"#ffffff\"),\n",
    "            fill_alpha=0.30,\n",
    "            level=\"underlay\",\n",
    "        )\n",
    "        p.add_layout(box)\n",
    "\n",
    "    # å½±çº¿ä¸å®ä½“ï¼ˆæ¯æ¬¡ä½¿ç”¨æ–°çš„ ColumnDataSourceï¼‰\n",
    "    p.segment(x0=\"idx\", y0=\"high\", x1=\"idx\", y1=\"low\", source=source, color=\"black\", line_width=1)\n",
    "    p.vbar(x=\"idx\", width=0.65, top=\"top\", bottom=\"bottom\", source=source, fill_color=\"bar_color\", line_color=\"black\", line_width=0.5)\n",
    "\n",
    "    hover = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"æ—¶é—´\", \"@timestamp_dt{%F %T}\"),\n",
    "            (\"å¼€\", \"@open{0.2f}\"),\n",
    "            (\"é«˜\", \"@high{0.2f}\"),\n",
    "            (\"ä½\", \"@low{0.2f}\"),\n",
    "            (\"æ”¶\", \"@close{0.2f}\"),\n",
    "            (f\"{regime_col}\", \"@regime_filled\"),\n",
    "        ],\n",
    "        formatters={\"@timestamp_dt\": \"datetime\"},\n",
    "        mode=\"vline\",\n",
    "    )\n",
    "    p.add_tools(hover)\n",
    "    p.xaxis.visible = False\n",
    "    return p\n",
    "\n",
    "# ç”Ÿæˆ layout çš„å‡½æ•°ï¼šæ¯æ¬¡è°ƒç”¨æ„å»ºå…¨æ–°æ¨¡å‹é›†åˆ\n",
    "def build_comparison_layout(df_plot: pd.DataFrame, sample_dates):\n",
    "    rows = []\n",
    "    for date in sample_dates:\n",
    "        df_day = df_plot[df_plot[\"date\"] == date].copy()\n",
    "        if len(df_day) < 30:\n",
    "            continue\n",
    "\n",
    "        orig_counts = df_day[\"regime\"].fillna(0).astype(int).value_counts().to_dict()\n",
    "        mtf_counts  = df_day[\"regime_mtf\"].fillna(0).astype(int).value_counts().to_dict()\n",
    "        changes = (df_day[\"regime\"].fillna(0).astype(int) != df_day[\"regime_mtf\"].fillna(0).astype(int)).sum()\n",
    "\n",
    "        title_orig = f\"{date} - åŸå§‹ | D:{orig_counts.get(-1,0)} R:{orig_counts.get(0,0)} U:{orig_counts.get(1,0)}\"\n",
    "        title_mtf  = f\"{date} - MTF  | D:{mtf_counts.get(-1,0)} R:{mtf_counts.get(0,0)} U:{mtf_counts.get(1,0)} | å˜åŒ–:{changes}\"\n",
    "\n",
    "        p_orig = plot_kline_comparison(df_day, \"regime\", title_orig, width=550)\n",
    "        p_mtf  = plot_kline_comparison(df_day, \"regime_mtf\", title_mtf, width=550)\n",
    "        rows.append(bokeh_row(p_orig, p_mtf))\n",
    "\n",
    "    return column(*rows) if rows else None\n",
    "\n",
    "# ---- æ¸…ç†å…ˆå‰ notebook stateï¼Œé˜²æ­¢æ®‹ç•™æ¨¡å‹å†²çª ----\n",
    "reset_output()\n",
    "\n",
    "# ---- ä½¿ç”¨ä¸€å¥— layout ç”¨äº showï¼ˆæ¯æ¬¡éƒ½é‡å»ºï¼‰ ----\n",
    "layout_show = build_comparison_layout(df_plot, sample_dates)\n",
    "if layout_show is not None:\n",
    "    try:\n",
    "        show(layout_show)\n",
    "    except RuntimeError as e:\n",
    "        print(\"ç¬¬ä¸€æ¬¡ show() å‡ºé”™ï¼Œå°è¯• reset_output() å¹¶é‡è¯•ï¼š\", e)\n",
    "        reset_output()\n",
    "        layout_show = build_comparison_layout(df_plot, sample_dates)\n",
    "        show(layout_show)\n",
    "else:\n",
    "    print(\"âš ï¸ æ²¡æœ‰ç”Ÿæˆä»»ä½•å›¾ï¼ˆå¯èƒ½ sample_dates å¯¹åº”æ—¥æ•°æ®ä¸è¶³ï¼‰ã€‚\")\n",
    "\n",
    "# ---- å†æ¬¡ reset å¹¶ä¸ºä¿å­˜é‡å»ºä¸€å¥—ç‹¬ç«‹æ¨¡å‹ï¼ˆå…³é”®ï¼šä¸èƒ½å¤ç”¨å·²ç”¨äº show çš„æ¨¡å‹ï¼‰ ----\n",
    "reset_output()\n",
    "layout_save = build_comparison_layout(df_plot, sample_dates)\n",
    "if layout_save is not None:\n",
    "    chart_path = OUTPUT_DIR_CHARTS / \"regime_comparison_orig_vs_mtf.html\"\n",
    "    output_file(str(chart_path), title=\"Regime Comparison: Original vs MTF\")\n",
    "    save(layout_save, filename=str(chart_path))\n",
    "    print(f\"\\nå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {chart_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ— å›¾å¯ä¿å­˜ã€‚\")\n",
    "\n",
    "# ---- æ±‡æ€»ç»Ÿè®¡ï¼ˆä¸æ¶‰åŠ Bokeh æ¨¡å‹ï¼‰ ----\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“Š æ–¹æ³•å¯¹æ¯”æ±‡æ€»\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "orig_reg = df[\"regime\"].fillna(0).astype(int)\n",
    "mtf_reg  = df[\"regime_mtf\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"\\næ ‡ç­¾å˜åŒ–çŸ©é˜µ (è¡Œ=åŸå§‹, åˆ—=MTF):\")\n",
    "change_matrix = pd.crosstab(orig_reg, mtf_reg, margins=True, margins_name=\"Total\")\n",
    "print(change_matrix)\n",
    "\n",
    "orig_trend_rate = (orig_reg != 0).mean() * 100\n",
    "mtf_trend_rate  = (mtf_reg  != 0).mean() * 100\n",
    "print(f\"\\nè¶‹åŠ¿è¯†åˆ«ç‡:\")\n",
    "print(f\"  åŸå§‹æ–¹æ³•: {orig_trend_rate:.2f}%\")\n",
    "print(f\"  MTF è¾…åŠ©: {mtf_trend_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 26: æœ€ç»ˆç‰¹å¾çŸ©é˜µ & æ•°æ®å¯¼å‡º (Final Feature Matrix) ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ æœ€ç»ˆå‡†å¤‡æœºå™¨å­¦ä¹ è®­ç»ƒæ•°æ®\n",
    "\n",
    "å…³é”®æ›´æ–°:\n",
    "1. ä½¿ç”¨ regime_mtf (å¤šå‘¨æœŸè¾…åŠ©æ ‡æ³¨) ä½œä¸ºæ ‡ç­¾ (å¯åˆ‡æ¢ä¸ºåŸå§‹ regime)\n",
    "2. æ’é™¤åŸå§‹å°ºåº¦ç‰¹å¾ (kama, atr, tr, bar_range)ï¼Œä½¿ç”¨å½’ä¸€åŒ–ç‰ˆæœ¬ (*_z)\n",
    "3. d_barrier åªç”¨äºæ ‡æ³¨ï¼Œä¸è¾“å…¥æ¨¡å‹è®­ç»ƒ (å®ƒä½¿ç”¨äº†æœªæ¥ä¿¡æ¯)\n",
    "\n",
    "æ³„éœ²æ£€æŸ¥æ¸…å•:\n",
    "âœ… æ— æ³„éœ²: ä½¿ç”¨ shift/rolling çš„è¿‡å»çª—å£ç‰¹å¾\n",
    "âœ… æ— æ³„éœ²: ä½¿ç”¨ expanding (ç´¯ç§¯åˆ°å½“å‰) çš„ç‰¹å¾\n",
    "âœ… æ— æ³„éœ²: å¤šå‘¨æœŸç‰¹å¾ä½¿ç”¨ floor - 1 å‘¨æœŸå¯¹é½\n",
    "âŒ æ³„éœ²: d_barrier (ä½¿ç”¨äº†æœªæ¥è·¯å¾„) â†’ æ’é™¤\n",
    "\"\"\"\n",
    "print(\"å‡†å¤‡æœ€ç»ˆç‰¹å¾çŸ©é˜µ...\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âš ï¸ é€‰æ‹©ä½¿ç”¨å“ªä¸ª regime ä½œä¸ºæ ‡ç­¾\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "USE_MTF_REGIME = True  # True: ä½¿ç”¨ MTF è¾…åŠ©æ ‡ç­¾, False: ä½¿ç”¨åŸå§‹æ ‡ç­¾ï¼Œå®æµ‹MTFä¸å¦‚åŸå§‹æ ‡ç­¾\n",
    "\n",
    "LABEL_COL = \"regime_mtf\" if USE_MTF_REGIME else \"regime\"\n",
    "print(f\"\\nä½¿ç”¨æ ‡ç­¾åˆ—: {LABEL_COL}\")\n",
    "\n",
    "# ========== 1. å®šä¹‰ç‰¹å¾åˆ—è¡¨ (å·²æ’é™¤åŸå§‹å°ºåº¦ç‰¹å¾) ==========\n",
    "\n",
    "# åŸºç¡€æ³¢åŠ¨ç‰¹å¾ (å·²å½’ä¸€åŒ–)\n",
    "volatility_features = [\n",
    "    \"log_return\",       # å½“å‰æ”¶ç›Š (æ— é‡çº²)\n",
    "    \"range_atr\",        # åŒºé—´/ATR (æ¯”ä¾‹)\n",
    "    \"range_z\",          # Range z-score\n",
    "    \"range_z_ma5\",      # Range z-score å‡å€¼\n",
    "    \"atr_z\",            # ATR å½’ä¸€åŒ–\n",
    "    \"tr_z\",             # TR å½’ä¸€åŒ– (å¦‚æœå­˜åœ¨)\n",
    "    \"bar_range_z\",      # Bar range å½’ä¸€åŒ– (å¦‚æœå­˜åœ¨)\n",
    "]\n",
    "\n",
    "# è¶‹åŠ¿ç‰¹å¾ (æ— æ³„éœ² - å…¨éƒ¨ä½¿ç”¨è¿‡å»çª—å£)\n",
    "trend_features = [\n",
    "    \"er\",               # æ•ˆç‡å› å­ [0,1]\n",
    "    \"slope_norm\",       # KAMA æ–œç‡æ ‡å‡†åŒ– (ç›¸å¯¹ ATR)\n",
    "    \"dist_norm\",        # ä»·æ ¼åç¦»åº¦ (ç›¸å¯¹ ATR)\n",
    "    \"kama_z\",           # KAMA å½’ä¸€åŒ– (å¦‚æœå­˜åœ¨)\n",
    "    \"beta\",             # å›å½’æ–œç‡\n",
    "    \"r2\",               # RÂ² [0,1]\n",
    "    \"slope_10_norm\",    # 10-bar æ–œç‡\n",
    "    \"slope_30_norm\",    # 30-bar æ–œç‡\n",
    "    \"slope_60_norm\",    # 60-bar æ–œç‡\n",
    "    \"trend_alignment\",  # å¤šå°ºåº¦ä¸€è‡´æ€§ [-3, +3]\n",
    "]\n",
    "\n",
    "# éœ‡è¡ç‰¹å¾ (æ— æ³„éœ²)\n",
    "range_features = [\n",
    "    \"chop\",             # Choppiness Index [0,1]\n",
    "    \"overlap_ratio\",    # é‡å åº¦ [0,1]\n",
    "    \"dir_vol_ratio\",    # æ–¹å‘æ€§æ³¢åŠ¨ [0,1]\n",
    "    \"vol_up_ratio\",     # ä¸Šæ¶¨æ³¢åŠ¨å æ¯” [0,1]\n",
    "]\n",
    "\n",
    "# Bar ç»“æ„ç‰¹å¾ (æ— æ³„éœ²)\n",
    "bar_structure_features = [\n",
    "    \"is_bull\",          # é˜³çº¿ {0,1}\n",
    "    \"is_bear\",          # é˜´çº¿ {0,1}\n",
    "    \"bull_ratio\",       # é˜³çº¿æ¯”ä¾‹ [0,1]\n",
    "    \"bear_ratio\",       # é˜´çº¿æ¯”ä¾‹ [0,1]\n",
    "    \"bull_bear_diff\",   # å¤šç©ºå·® [-1,1]\n",
    "    \"consec_bull\",      # è¿ç»­é˜³çº¿\n",
    "    \"consec_bear\",      # è¿ç»­é˜´çº¿\n",
    "    \"body_ratio\",       # å®ä½“æ¯”ä¾‹ [0,1]\n",
    "    \"upper_wick_ratio\", # ä¸Šå½±çº¿æ¯”ä¾‹ [0,1]\n",
    "    \"lower_wick_ratio\", # ä¸‹å½±çº¿æ¯”ä¾‹ [0,1]\n",
    "    \"total_wick_ratio\", # æ€»å½±çº¿æ¯”ä¾‹ [0,1]\n",
    "    \"avg_body_ratio\",   # çª—å£å¹³å‡å®ä½“æ¯”ä¾‹\n",
    "    \"avg_total_wick_ratio\",  # çª—å£å¹³å‡å½±çº¿æ¯”ä¾‹\n",
    "    \"doji_like_ratio\",  # å°å®ä½“å¤§å½±çº¿æ¯”ä¾‹ [0,1]\n",
    "]\n",
    "\n",
    "# è…¿ç»“æ„ç‰¹å¾ (æ— æ³„éœ² - å…¨éƒ¨åŸºäºè¿‡å»)\n",
    "leg_features = [\n",
    "    \"legA_dir\", \"legA_prev1_dir\", \"legA_prev2_dir\", \"legA_changes_10\",\n",
    "    \"legB_dir\", \"legB_prev1_dir\", \"legB_prev2_dir\", \"legB_changes_10\",\n",
    "    \"legC_dir\", \"legC_prev1_dir\", \"legC_prev2_dir\", \"legC_changes_10\",\n",
    "]\n",
    "\n",
    "# Phase ç‰¹å¾ (æ— æ³„éœ²)\n",
    "phase_features = [\n",
    "    \"spike_score\",      # Spike åˆ†æ•° [0,1]\n",
    "    \"channel_score\",    # Channel åˆ†æ•° [0,1]\n",
    "    \"phase\",            # Phase {0,1,2}\n",
    "]\n",
    "\n",
    "# æ—¥å†…ç‰¹å¾ (æ— æ³„éœ² - expanding)\n",
    "intraday_features = [\n",
    "    \"pos_in_day_range\", # æ—¥å†…ä½ç½® [0,1]\n",
    "    \"dist_to_open\",     # è·å¼€ç›˜ä»·\n",
    "    \"day_return\",       # æ—¥å†…æ”¶ç›Š\n",
    "    \"bar_of_day\",       # å½“æ—¥ç¬¬å‡ æ ¹ bar\n",
    "    \"time_of_day_norm\", # æ—¶é—´å½’ä¸€åŒ– [0,1]\n",
    "    \"near_day_high\",    # æ¥è¿‘æ—¥é«˜ {0,1}\n",
    "    \"near_day_low\",     # æ¥è¿‘æ—¥ä½ {0,1}\n",
    "    \"near_open\",        # æ¥è¿‘å¼€ç›˜ {0,1}\n",
    "    \"is_open_range\",    # å¼€ç›˜é˜¶æ®µ {0,1}\n",
    "    \"is_close_range\",   # æ”¶ç›˜é˜¶æ®µ {0,1}\n",
    "]\n",
    "\n",
    "# å¤šå‘¨æœŸç‰¹å¾ (æ— æ³„éœ² - ä½¿ç”¨ floor-1 å¯¹é½)\n",
    "mtf_features = [\n",
    "    \"er_15\", \"slope_15_norm\", \"chop_15\", \"trend_dir_15\",\n",
    "    \"er_60\", \"slope_60_norm\", \"chop_60\", \"trend_dir_60\",\n",
    "    \"atr_ratio_15_5\", \"atr_ratio_60_5\", \"mtf_trend_alignment\",\n",
    "]\n",
    "\n",
    "# æ»åç‰¹å¾ (æ— æ³„éœ² - shift)\n",
    "lag_features = []\n",
    "for lag in range(1, 4):\n",
    "    lag_features.extend([\n",
    "        f\"er_lag{lag}\", f\"slope_norm_lag{lag}\", f\"dist_norm_lag{lag}\",\n",
    "        f\"chop_lag{lag}\", f\"r2_lag{lag}\", f\"range_z_lag{lag}\",\n",
    "        f\"dir_vol_ratio_lag{lag}\", f\"bull_bear_diff_lag{lag}\",\n",
    "        f\"avg_body_ratio_lag{lag}\",\n",
    "    ])\n",
    "'''\n",
    "# ç»Ÿè®¡ç‰¹å¾ ï¼ˆæœ‰å¯èƒ½æ³„éœ²ï¼ï¼‰\n",
    "stat_features = [\n",
    "    \"regime_trend_ratio\", \"regime_up_ratio\", \"regime_down_ratio\",\n",
    "    \"regime_changes_N\", \"regime_consec\",\n",
    "    \"er_diff1\", \"er_diff3\", \"slope_norm_diff1\", \"slope_norm_diff3\",\n",
    "    \"chop_diff1\", \"chop_diff3\", \"range_z_diff1\", \"range_z_diff3\",\n",
    "    \"slope_accel\", \"slope_accel_ma3\",\n",
    "]\n",
    "'''\n",
    "\n",
    "\n",
    "stat_features = [\n",
    "    \"er_diff1\", \"er_diff3\", \"slope_norm_diff1\", \"slope_norm_diff3\",\n",
    "    \"chop_diff1\", \"chop_diff3\", \"range_z_diff1\", \"range_z_diff3\",\n",
    "    \"slope_accel\", \"slope_accel_ma3\",\n",
    "]\n",
    "\n",
    "# Volume ç‰¹å¾ (å¦‚æœå­˜åœ¨)\n",
    "volume_features = [\"volume_z\", \"log_volume_z\"]\n",
    "\n",
    "# ========== 2. åˆå¹¶æ‰€æœ‰ç‰¹å¾å¹¶è¿‡æ»¤å­˜åœ¨çš„åˆ— ==========\n",
    "all_features = (\n",
    "    volatility_features + trend_features + range_features +\n",
    "    bar_structure_features + leg_features + phase_features +\n",
    "    intraday_features + mtf_features + lag_features + stat_features +\n",
    "    volume_features\n",
    ")\n",
    "\n",
    "# æ˜ç¡®æ’é™¤çš„ç‰¹å¾ (åŸå§‹å°ºåº¦æˆ–æ³„éœ²)\n",
    "EXCLUDED_FEATURES = [\n",
    "    \"kama\", \"atr\", \"tr\", \"bar_range\", \"volume\",  # åŸå§‹å°ºåº¦\n",
    "    \"d_barrier\", \"regime_raw\", \"is_range\", \"is_range_extended\",  # æ³„éœ²æˆ–è¾…åŠ©æ ‡è®°\n",
    "    \"is_range_in_trend\", \"strong_15\", \"strong_60\", \"trend_score_mtf\",  # è¾…åŠ©å˜é‡\n",
    "    # âœ… stat_features é‡Œå‰”é™¤çš„æ³„éœ²åˆ—ï¼ˆç”±çœŸå® regime ç»Ÿè®¡å¾—æ¥ï¼‰\n",
    "    \"regime_trend_ratio\", \"regime_up_ratio\", \"regime_down_ratio\",\n",
    "    \"regime_changes_N\", \"regime_consec\", \"regime_change\",\n",
    "]\n",
    "\n",
    "# è¿‡æ»¤: åªä¿ç•™å­˜åœ¨ä¸”æœªæ’é™¤çš„ç‰¹å¾\n",
    "training_features = [\n",
    "    col for col in all_features \n",
    "    if col in df.columns and col not in EXCLUDED_FEATURES\n",
    "]\n",
    "\n",
    "print(f\"\\nå¯ç”¨ç‰¹å¾æ•°é‡: {len(training_features)}\")\n",
    "\n",
    "# ========== 3. æ„å»ºè®­ç»ƒæ•°æ® ==========\n",
    "# è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬ (é NaN)\n",
    "df_train = df.dropna(subset=[LABEL_COL]).copy()\n",
    "\n",
    "# æ„å»º X å’Œ y\n",
    "X = df_train[training_features].copy()\n",
    "y = df_train[LABEL_COL].astype(int).copy()\n",
    "\n",
    "# è¿‡æ»¤æ‰ç‰¹å¾ä¸­æœ‰ NaN çš„è¡Œ\n",
    "valid_mask = X.notna().all(axis=1)\n",
    "X = X[valid_mask]\n",
    "y = y[valid_mask]\n",
    "df_train = df_train[valid_mask]\n",
    "\n",
    "print(f\"æœ‰æ•ˆæ ·æœ¬æ•°: {len(X):,} / {len(df):,} ({len(X)/len(df)*100:.1f}%)\")\n",
    "print(f\"ç‰¹å¾æ•°: {X.shape[1]}\")\n",
    "\n",
    "# ========== 4. ç‰¹å¾ç»Ÿè®¡ ==========\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“Š ç‰¹å¾ç»Ÿè®¡\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# æŒ‰ç±»åˆ«ç»Ÿè®¡\n",
    "feature_categories = {\n",
    "    \"æ³¢åŠ¨\": volatility_features,\n",
    "    \"è¶‹åŠ¿\": trend_features,\n",
    "    \"éœ‡è¡\": range_features,\n",
    "    \"Barç»“æ„\": bar_structure_features,\n",
    "    \"è…¿ç»“æ„\": leg_features,\n",
    "    \"Phase\": phase_features,\n",
    "    \"æ—¥å†…\": intraday_features,\n",
    "    \"å¤šå‘¨æœŸ\": mtf_features,\n",
    "    \"æ»å\": lag_features,\n",
    "    \"ç»Ÿè®¡\": stat_features,\n",
    "}\n",
    "\n",
    "for cat, feats in feature_categories.items():\n",
    "    available = [f for f in feats if f in training_features]\n",
    "    print(f\"  {cat}: {len(available)} ä¸ªç‰¹å¾\")\n",
    "\n",
    "# ========== 5. æ ‡ç­¾åˆ†å¸ƒ ==========\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ ({LABEL_COL})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "label_counts = y.value_counts().sort_index()\n",
    "for k, v in label_counts.items():\n",
    "    label_name = {-1: \"DOWN (-1)\", 0: \"RANGE (0)\", 1: \"UP (+1)\"}.get(k, str(k))\n",
    "    print(f\"  {label_name}: {v:,} ({v/len(y)*100:.2f}%)\")\n",
    "\n",
    "# ========== 6. å¯¼å‡ºè®­ç»ƒæ•°æ® ==========\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ’¾ å¯¼å‡ºè®­ç»ƒæ•°æ®\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# å‡†å¤‡å¯¼å‡ºæ•°æ®\n",
    "df_train = df_train.reset_index()\n",
    "df_train[\"timestamp_str\"] = df_train[\"timestamp\"].astype(str)\n",
    "df_train[\"year\"] = pd.to_datetime(df_train[\"timestamp_str\"].str[:10]).dt.year\n",
    "df_train[\"month\"] = pd.to_datetime(df_train[\"timestamp_str\"].str[:10]).dt.month\n",
    "df_train[\"date\"] = df_train[\"timestamp_str\"].str[:10]\n",
    "\n",
    "# é€‰æ‹©è¦å¯¼å‡ºçš„åˆ—\n",
    "export_meta = [\"timestamp_str\", \"year\", \"month\", \"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "export_cols = export_meta + training_features + [LABEL_COL]\n",
    "export_cols = [c for c in export_cols if c in df_train.columns]\n",
    "\n",
    "# å»é‡\n",
    "export_cols = list(dict.fromkeys(export_cols))\n",
    "\n",
    "df_export = df_train[export_cols].copy()\n",
    "df_export = df_export.rename(columns={\"timestamp_str\": \"timestamp\", LABEL_COL: \"regime\"})\n",
    "\n",
    "# å¯¼å‡º\n",
    "OUTPUT_TRAIN_CSV = OUTPUT_DIR_FEATURES / \"market_cycle_train_data.csv\"\n",
    "OUTPUT_TRAIN_PARQUET = OUTPUT_DIR_FEATURES / \"market_cycle_train_data.parquet\"\n",
    "\n",
    "df_export.to_csv(OUTPUT_TRAIN_CSV, index=False)\n",
    "df_export.to_parquet(OUTPUT_TRAIN_PARQUET, index=False)\n",
    "\n",
    "print(f\"\\nğŸ“ è®­ç»ƒæ•°æ®å·²å¯¼å‡º:\")\n",
    "print(f\"   - CSV: {OUTPUT_TRAIN_CSV}\")\n",
    "print(f\"   - Parquet: {OUTPUT_TRAIN_PARQUET}\")\n",
    "print(f\"   - æ ·æœ¬æ•°: {len(df_export):,}\")\n",
    "print(f\"   - ç‰¹å¾æ•°: {len(training_features)}\")\n",
    "\n",
    "# ========== 7. æ•°æ®æ³„éœ²æœ€ç»ˆæ£€æŸ¥ ==========\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ” æ•°æ®æ³„éœ²æ£€æŸ¥\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰æœªæ¥ä¿¡æ¯ç‰¹å¾\n",
    "future_keywords = [\"_fwd\", \"future\", \"barrier\"]\n",
    "potential_leak = [c for c in training_features if any(k in c.lower() for k in future_keywords)]\n",
    "if potential_leak:\n",
    "    print(f\"âš ï¸ å‘ç°å¯èƒ½æ³„éœ²çš„ç‰¹å¾: {potential_leak}\")\n",
    "else:\n",
    "    print(\"âœ… æœªå‘ç°æ˜æ˜¾çš„æ•°æ®æ³„éœ²\")\n",
    "\n",
    "# æ£€æŸ¥åŸå§‹å°ºåº¦ç‰¹å¾\n",
    "raw_scale_in_train = [c for c in [\"kama\", \"atr\", \"tr\", \"bar_range\"] if c in training_features]\n",
    "if raw_scale_in_train:\n",
    "    print(f\"âš ï¸ è®­ç»ƒé›†ä¸­åŒ…å«åŸå§‹å°ºåº¦ç‰¹å¾: {raw_scale_in_train}\")\n",
    "else:\n",
    "    print(\"âœ… åŸå§‹å°ºåº¦ç‰¹å¾å·²æ’é™¤\")\n",
    "\n",
    "print(\"\\nCell 26 å®Œæˆ: æœ€ç»ˆç‰¹å¾çŸ©é˜µå‡†å¤‡å®Œæ¯• âœ…\")\n",
    "print(\"\\nğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆï¼å¯ä»¥å¼€å§‹æœºå™¨å­¦ä¹ è®­ç»ƒäº†ï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¸‚åœºå‘¨æœŸæ ‡æ³¨è§„åˆ™æ€»ç»“\n",
    "\n",
    "## ğŸ“Œ æ ‡ç­¾å®šä¹‰\n",
    "\n",
    "| æ ‡ç­¾ | å«ä¹‰ | Al Brooks å¯¹åº”æ¦‚å¿µ |\n",
    "|-----|------|-------------------|\n",
    "| `-1` | ä¸‹è·Œè¶‹åŠ¿ | Bear Trend / Sell-off |\n",
    "| `0` | éœ‡è¡/äº¤æ˜“åŒºé—´ | Trading Range / Breakout Mode |\n",
    "| `+1` | ä¸Šæ¶¨è¶‹åŠ¿ | Bull Trend / Rally |\n",
    "\n",
    "## ğŸ“Œ æ ‡ç­¾ç”Ÿæˆæµç¨‹ (ä¼ªä»£ç )\n",
    "\n",
    "```\n",
    "è¾“å…¥: 5åˆ†é’ŸKçº¿åºåˆ— (OHLCV)\n",
    "\n",
    "==== ç¬¬ä¸€é˜¶æ®µ: ç‰¹å¾è®¡ç®— ====\n",
    "FOR æ¯æ ¹ bar t:\n",
    "    # åŸºç¡€åºåˆ—\n",
    "    log_return = ln(close_t) - ln(close_{t-1})\n",
    "    ATR = EMA(TR, 20)\n",
    "    range_atr = (roll_max - roll_min) / ATR\n",
    "    \n",
    "    # KAMA åŠè¡ç”Ÿ\n",
    "    ER = |close_t - close_{t-20}| / sum(|close_i - close_{i-1}|)  # æ•ˆç‡å› å­\n",
    "    KAMA = è‡ªé€‚åº”ç§»åŠ¨å¹³å‡\n",
    "    slope_norm = (KAMA_t - KAMA_{t-5}) / (5 * ATR)  # æ ‡å‡†åŒ–æ–œç‡\n",
    "    dist_norm = (close_t - KAMA_t) / ATR  # ä»·æ ¼åç¦»åº¦\n",
    "    \n",
    "    # ç»“æ„ç‰¹å¾\n",
    "    RÂ² = çº¿æ€§å›å½’æ‹Ÿåˆåº¦ (çª—å£=20)\n",
    "    chop = Choppiness Index (åŸºäºATR)\n",
    "    overlap_ratio = ç›¸é‚»baré‡å åº¦\n",
    "    \n",
    "    # Triple Barrier (ä½¿ç”¨æœªæ¥ä¿¡æ¯ï¼Œä»…ç”¨äºæ ‡æ³¨)\n",
    "    d_barrier = å…ˆè§¦åŠ +1.5*ATR ä¸Šè½¨ â†’ +1\n",
    "               å…ˆè§¦åŠ -1.5*ATR ä¸‹è½¨ â†’ -1\n",
    "               æ—¶é—´æˆªæ­¢ (12 bar) â†’ sign(æ”¶ç›Š)\n",
    "\n",
    "==== ç¬¬äºŒé˜¶æ®µ: éœ‡è¡åŒºé—´è¯†åˆ« (Step 1) ====\n",
    "FOR æ¯æ ¹ bar t:\n",
    "    éœ‡è¡å€™é€‰æ¡ä»¶:\n",
    "        - ER < 0.25  (èµ°åŠ¿ä¸\"ç›´\")\n",
    "        - chop > 0.75  (éœ‡è¡åº¦é«˜)\n",
    "        - range_atr < 3.5 OR RÂ² < 0.25  (åŒºé—´çª„æˆ–æ— çº¿æ€§è¶‹åŠ¿)\n",
    "        - |future_move| < 1.2 * ATR  (æœªæ¥æ²¡æœ‰å¤§å¹…ç§»åŠ¨)\n",
    "    \n",
    "    IF æ»¡è¶³éœ‡è¡å€™é€‰:\n",
    "        is_range[t] = True\n",
    "    \n",
    "    # Brooks 20-bar rule (æœ€å°é•¿åº¦çº¦æŸ)\n",
    "    IF è¿ç»­éœ‡è¡æ®µé•¿åº¦ >= 15 bar:\n",
    "        ç¡®è®¤ä¸ºéœ‡è¡åŒºé—´\n",
    "    ELSE:\n",
    "        æ‰“å›ä¸ºééœ‡è¡\n",
    "    \n",
    "    # è¾¹ç¼˜æ‰©å±• (é˜²æ­¢é¢‘ç¹æŠ–åŠ¨)\n",
    "    éœ‡è¡åŒºé—´è¾¹ç¼˜å‰åå„æ‰©å±• 2 bar\n",
    "\n",
    "==== ç¬¬ä¸‰é˜¶æ®µ: è¶‹åŠ¿è¯†åˆ« (Step 2) ====\n",
    "FOR æ¯æ ¹ ééœ‡è¡åŒºé—´ çš„ bar t:\n",
    "    è¶‹åŠ¿å€™é€‰æ¡ä»¶:\n",
    "        æ–¹å‘ä¸€è‡´ = sign(slope_norm_t) == sign(close_{t+12} - close_t)\n",
    "        æ–œç‡è¶³å¤Ÿ = |slope_norm_t| > 0.015\n",
    "        è¶‹åŠ¿è´¨é‡ = (ER_t > 0.35) OR (chop_t < 0.55)\n",
    "        ä»·æ ¼åç¦» = |dist_norm_t| > 0.4\n",
    "    \n",
    "    IF æ–¹å‘ä¸€è‡´ AND æ–œç‡è¶³å¤Ÿ AND (è¶‹åŠ¿è´¨é‡ OR ä»·æ ¼åç¦»):\n",
    "        IF d_barrier_t ä¸ slope_norm_t æ–¹å‘ä¸€è‡´:\n",
    "            regime[t] = sign(slope_norm_t)  # Â±1 è¶‹åŠ¿\n",
    "        ELSE:\n",
    "            regime[t] = 0  # æ–¹å‘ä¸ä¸€è‡´ â†’ æ¨¡ç³Šæ®µ â†’ éœ‡è¡\n",
    "    ELSE:\n",
    "        regime[t] = 0  # ä¸æ»¡è¶³æ¡ä»¶ â†’ éœ‡è¡\n",
    "\n",
    "==== ç¬¬å››é˜¶æ®µ: æ ‡ç­¾å¹³æ»‘ (Step 3) ====\n",
    "# 1. åˆ é™¤çŸ­è¶‹åŠ¿æ®µ\n",
    "FOR æ¯ä¸ªè¿ç»­è¶‹åŠ¿æ®µ:\n",
    "    IF é•¿åº¦ < 4 bar:\n",
    "        â†’ æ”¹ä¸º 0 (éœ‡è¡)\n",
    "\n",
    "# 2. å¤šæ•°æŠ•ç¥¨\n",
    "FOR æ¯æ ¹ bar t:\n",
    "    çª—å£ = [t-2, t+2] å…± 5 bar\n",
    "    regime[t] = çª—å£å†…å‡ºç°æœ€å¤šçš„æ ‡ç­¾\n",
    "\n",
    "==== è¾“å‡º ====\n",
    "regime âˆˆ {-1, 0, +1}\n",
    "```\n",
    "\n",
    "## ğŸ“Œ æ ¸å¿ƒåˆ¤å®šè§„åˆ™å¯¹ç…§è¡¨\n",
    "\n",
    "| æŒ‡æ ‡ | éœ‡è¡åŒºé—´æ¡ä»¶ | è¶‹åŠ¿æ¡ä»¶ | è¯´æ˜ |\n",
    "|-----|-------------|---------|------|\n",
    "| **ER (æ•ˆç‡å› å­)** | < 0.25 | > 0.35 | ERâ†’1 èµ°ç›´çº¿, ERâ†’0 éœ‡è¡ |\n",
    "| **Chop (éœ‡è¡åº¦)** | > 0.75 | < 0.55 | åŸºäºATRçš„éœ‡è¡æŒ‡æ•° |\n",
    "| **RÂ² (çº¿æ€§æ‹Ÿåˆ)** | < 0.25 | - | å›å½’æ‹Ÿåˆåº¦ä½ â†’ æ— è¶‹åŠ¿ |\n",
    "| **slope_norm** | - | > 0.015 | KAMAæ–œç‡éœ€è¶³å¤Ÿå¼º |\n",
    "| **dist_norm** | - | > 0.4 | ä»·æ ¼éœ€åç¦»KAMA |\n",
    "| **æ–¹å‘ä¸€è‡´æ€§** | - | âœ“ | slope ä¸æœªæ¥æ”¶ç›ŠåŒå‘ |\n",
    "| **Triple Barrier** | - | âœ“ | æ–¹å‘ç¡®è®¤ |\n",
    "| **æœ€å°æ®µé•¿åº¦** | â‰¥ 15 bar | â‰¥ 4 bar | Brooks 20-bar rule |\n",
    "\n",
    "## ğŸ“Œ ç¬¦åˆ Al Brooks çš„æ ¸å¿ƒåŸåˆ™\n",
    "\n",
    "### 80/20 è§„åˆ™\n",
    "- **éœ‡è¡åŒºé—´å æ¯”**: çº¦ 70-80% (ç¬¦åˆ Brooks çš„ \"80% æ—¶é—´åœ¨äº¤æ˜“åŒºé—´\")\n",
    "- **è¶‹åŠ¿åŒºé—´å æ¯”**: çº¦ 20-30%\n",
    "\n",
    "### 20-bar ä¸­æ€§åŒ–è§„åˆ™\n",
    "- éœ‡è¡åŒºé—´è‡³å°‘æŒç»­ 15-20 æ ¹ bar æ‰ç¡®è®¤\n",
    "- çŸ­äºæ­¤çš„ \"å‡è¶‹åŠ¿\" è¢«è¿‡æ»¤\n",
    "\n",
    "### Triple Barrier æ–¹å‘ç¡®è®¤\n",
    "- åªæœ‰å½“ **ç»“æ„ç‰¹å¾** å’Œ **æœªæ¥èµ°åŠ¿** éƒ½æŒ‡å‘åŒä¸€æ–¹å‘æ—¶ï¼Œæ‰æ ‡è®°ä¸ºè¶‹åŠ¿\n",
    "- è¿™æ¨¡æ‹Ÿäº† Brooks æ‰€è¯´çš„ \"80% çš„çªç ´ä¼šå¤±è´¥è¢«å¸å›åŒºé—´\"\n",
    "\n",
    "## ğŸ“Œ æ‰©å±•ç‰¹å¾ (ç”¨äº ML è®­ç»ƒ)\n",
    "\n",
    "æ–°å¢ç‰¹å¾**ä¸å‚ä¸**æ ‡ç­¾ç”Ÿæˆï¼Œä»…ä½œä¸ºæ¨¡å‹è¾“å…¥:\n",
    "\n",
    "| ç±»åˆ« | ç‰¹å¾ | æ•°é‡ |\n",
    "|-----|------|-----|\n",
    "| å¤šå°ºåº¦è¶‹åŠ¿ | slope_10/30/60, trend_alignment | 7 |\n",
    "| æ–¹å‘æ€§æ³¢åŠ¨ | dir_vol_ratio, vol_up_ratio, range_z | 4 |\n",
    "| Bar ç»“æ„ | bull/bear_ratio, consec_bull/bear, body/wick_ratio | 14 |\n",
    "| è…¿ç»“æ„ | legA/B/C_dir, legA/B/C_changes_10 | 12 |\n",
    "| Spike/Channel | spike_score, channel_score, phase | 3 |\n",
    "| æ—¥å†…ä½ç½® | pos_in_day_range, dist_to_open, bar_of_day | 10 |\n",
    "| å¤šå‘¨æœŸ | er_15/60, slope_15/60_norm, chop_15/60 | 11 |\n",
    "| æ»åç‰¹å¾ | *_lag1/2/3, *_diff1/3, regime_consec | 40+ |\n",
    "\n",
    "**æ€»è®¡: 100+ ä¸ªæ— æ³„éœ²ç‰¹å¾**\n",
    "\n",
    "## ğŸ“Œ æ•°æ®æ³„éœ²è¯´æ˜\n",
    "\n",
    "| ç‰¹å¾ | æ˜¯å¦æ³„éœ² | å¤„ç† |\n",
    "|-----|---------|------|\n",
    "| `d_barrier` | âœ… æ˜¯ | ä»…ç”¨äºæ ‡æ³¨ï¼Œ**ä¸è¾“å…¥æ¨¡å‹** |\n",
    "| `regime` | âœ… æ˜¯ | ä½œä¸ºæ ‡ç­¾ yï¼Œ**ä¸ä½œä¸ºç‰¹å¾** |\n",
    "| å…¶ä»–ç‰¹å¾ | âŒ å¦ | åŸºäº rolling/shift/expanding |\n",
    "| å¤šå‘¨æœŸç‰¹å¾ | âŒ å¦ | ä½¿ç”¨ floor() å¯¹é½ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 26: æœ€ç»ˆç‰¹å¾çŸ©é˜µ & æ•°æ®å¯¼å‡º (Final Feature Matrix | No-Leak) ==========\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"å‡†å¤‡æœ€ç»ˆç‰¹å¾çŸ©é˜µï¼ˆæ–°ç­–ç•¥/éšå˜é‡ä¼˜å…ˆ + ä¸¥æ ¼æ— æ³„éœ²ï¼‰...\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 0) æ ‡ç­¾åˆ—é€‰æ‹©\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "USE_MTF_REGIME = True\n",
    "LABEL_COL = \"regime_mtf\" if USE_MTF_REGIME else \"regime\"\n",
    "print(f\"\\nä½¿ç”¨æ ‡ç­¾åˆ—: {LABEL_COL}\")\n",
    "\n",
    "if LABEL_COL not in df.columns:\n",
    "    raise ValueError(f\"ç¼ºå°‘æ ‡ç­¾åˆ— {LABEL_COL}ï¼Œè¯·å…ˆç”Ÿæˆå¹¶å†™å…¥ df['{LABEL_COL}'].\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1) æ–°ç­–ç•¥ï¼šéšå˜é‡ï¼ˆå­˜åœ¨å°±ç”¨ï¼‰\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "latent_features = [\n",
    "    \"impulse_push\",\n",
    "    \"impulse_control\",\n",
    "    \"coherence_strength\",\n",
    "    \"noise_regime\",\n",
    "    \"trend_env_score\",\n",
    "]\n",
    "\n",
    "# ä½ ä¹Ÿå¯ä»¥æŠŠâ€œæ–°ç­–ç•¥æ ¸å¿ƒç‰¹å¾â€è¡¥å……è¿›æ¥ï¼ˆå¯é€‰ï¼‰\n",
    "extra_core_features = [\n",
    "    # æ³¢åŠ¨/è¶‹åŠ¿ç¯å¢ƒï¼ˆä¸å«æœªæ¥ï¼‰\n",
    "    \"log_return\", \"er\", \"chop\", \"overlap_ratio\", \"dir_vol_ratio\", \"range_z\",\n",
    "    # å½’ä¸€åŒ–ç‰ˆæœ¬ï¼ˆè‹¥å­˜åœ¨ï¼‰\n",
    "    \"atr_z\", \"tr_z\", \"bar_range_z\", \"kama_z\", \"log_volume_z\", \"volume_z\",\n",
    "    # å¤šå°ºåº¦ï¼ˆä¸å«æœªæ¥ï¼‰\n",
    "    \"slope_10_norm\", \"slope_30_norm\", \"slope_60_norm\", \"trend_alignment\",\n",
    "    # MTFï¼ˆä¸å«æœªæ¥ï¼›ä½ å·²åš floor-1 å¯¹é½ï¼‰\n",
    "    \"er_15\", \"slope_15_norm\", \"chop_15\", \"trend_dir_15\",\n",
    "    \"er_60\", \"slope_60_norm\", \"chop_60\", \"trend_dir_60\",\n",
    "    \"atr_ratio_15_5\", \"atr_ratio_60_5\", \"mtf_trend_alignment\",\n",
    "]\n",
    "\n",
    "def pick_existing(cols):\n",
    "    return [c for c in cols if c in df.columns]\n",
    "\n",
    "# ä½ æƒ³â€œåªç”¨éšå˜é‡â€å°±æŠŠ USE_LATENT_ONLY=True\n",
    "USE_LATENT_ONLY = False\n",
    "\n",
    "candidate_features = pick_existing(latent_features)\n",
    "if not USE_LATENT_ONLY:\n",
    "    candidate_features += pick_existing(extra_core_features)\n",
    "\n",
    "# å»é‡ä¿æŒé¡ºåº\n",
    "candidate_features = list(dict.fromkeys(candidate_features))\n",
    "\n",
    "print(f\"å€™é€‰ç‰¹å¾æ•°(å»é‡å): {len(candidate_features)}\")\n",
    "if len(candidate_features) == 0:\n",
    "    raise ValueError(\"æ²¡æœ‰å¯ç”¨ç‰¹å¾åˆ—ï¼ˆlatent_features/extra_core_features åœ¨ df ä¸­éƒ½ä¸å­˜åœ¨ï¼‰ã€‚\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2) æ˜ç¡®æ’é™¤ï¼šæ³„éœ²/æ ‡ç­¾æ´¾ç”Ÿ/è¾…åŠ©å˜é‡/åŸå§‹å°ºåº¦\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "EXCLUDED_FEATURES = set([\n",
    "    # åŸå§‹å°ºåº¦ï¼ˆè®­ç»ƒåªç”¨ *_z / æ¯”ä¾‹/æ— é‡çº²ï¼‰\n",
    "    \"kama\", \"atr\", \"tr\", \"bar_range\", \"volume\",\n",
    "\n",
    "    # æ˜ç¡®æ³„éœ²ï¼šç”¨æœªæ¥è·¯å¾„\n",
    "    \"d_barrier\",\n",
    "\n",
    "    # æ ‡ç­¾/è¾…åŠ©æ ‡è®°ï¼ˆä»¥åŠä½ å·²æœ‰çš„ï¼‰\n",
    "    \"regime_raw\", \"is_range\", \"is_range_extended\", \"is_range_in_trend\",\n",
    "    \"strong_15\", \"strong_60\", \"trend_score_mtf\",\n",
    "\n",
    "    # å…³é”®ï¼šphase æ˜¯ç”¨ regime åˆ¤çš„ â†’ å¿…é¡»æ’é™¤\n",
    "    \"phase\",\n",
    "])\n",
    "\n",
    "# å…³é”®ï¼šä»»ä½•â€œç›´æ¥/é—´æ¥åŸºäº regime çœŸå®æ ‡ç­¾æ»šåŠ¨ç®—å‡ºæ¥â€çš„åˆ—éƒ½æ’é™¤\n",
    "# ï¼ˆå¦‚æœä½ æœªæ¥åšâ€œé¢„æµ‹ä¸‹ä¸€æ ¹â€ï¼Œå¯ä»¥æŠŠå®ƒä»¬ shift(1) åå†çº³å…¥ï¼‰\n",
    "def is_label_derived(col: str) -> bool:\n",
    "    if col.startswith(\"regime_\"):\n",
    "        return True\n",
    "    if col.startswith(\"regime\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# å…³é”®ï¼šåå­—é‡Œåƒæœªæ¥ä¿¡æ¯çš„ä¹Ÿæ’é™¤ï¼ˆåŒä¿é™©ï¼‰\n",
    "LEAK_KEYWORDS = [\"future\", \"_fwd\", \"barrier\"]\n",
    "\n",
    "training_features = []\n",
    "for c in candidate_features:\n",
    "    if c in EXCLUDED_FEATURES:\n",
    "        continue\n",
    "    if is_label_derived(c):\n",
    "        continue\n",
    "    if any(k in c.lower() for k in LEAK_KEYWORDS):\n",
    "        continue\n",
    "    training_features.append(c)\n",
    "\n",
    "print(f\"\\næœ€ç»ˆå¯ç”¨ç‰¹å¾æ•°é‡: {len(training_features)}\")\n",
    "if len(training_features) == 0:\n",
    "    raise ValueError(\"è¿‡æ»¤åæ²¡æœ‰å¯ç”¨ç‰¹å¾äº†ï¼šè¯·æ£€æŸ¥ä½ çš„ç‰¹å¾åˆ—åæˆ–æ’é™¤è§„åˆ™ã€‚\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3) æ„å»ºè®­ç»ƒæ•°æ®ï¼ˆä¸¥æ ¼ï¼šlabel éç©º + ç‰¹å¾å…¨éç©ºï¼‰\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "df_train = df.dropna(subset=[LABEL_COL]).copy()\n",
    "X = df_train[training_features].copy()\n",
    "y = df_train[LABEL_COL].astype(int).copy()\n",
    "\n",
    "valid_mask = X.notna().all(axis=1)\n",
    "X = X[valid_mask]\n",
    "y = y[valid_mask]\n",
    "df_train = df_train[valid_mask]\n",
    "\n",
    "print(f\"æœ‰æ•ˆæ ·æœ¬æ•°: {len(X):,} / {len(df):,} ({len(X)/len(df)*100:.1f}%)\")\n",
    "print(f\"ç‰¹å¾æ•°: {X.shape[1]}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4) æ ‡ç­¾åˆ†å¸ƒ\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ ({LABEL_COL})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "label_counts = y.value_counts().sort_index()\n",
    "for k, v in label_counts.items():\n",
    "    label_name = {-1: \"DOWN (-1)\", 0: \"RANGE (0)\", 1: \"UP (+1)\"}.get(k, str(k))\n",
    "    print(f\"  {label_name}: {v:,} ({v/len(y)*100:.2f}%)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5) å¯¼å‡º\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ’¾ å¯¼å‡ºè®­ç»ƒæ•°æ®\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df_out = df_train.copy().reset_index()\n",
    "\n",
    "# ç»Ÿä¸€ timestamp è¾“å‡º\n",
    "df_out[\"timestamp_str\"] = df_out[\"timestamp\"].astype(str)\n",
    "# æ›´ç¨³ï¼šä» timestamp_dt å–å¹´æœˆæ—¥ï¼ˆå¦‚æœä½ æœ‰ timestamp_dtï¼‰\n",
    "if \"timestamp_dt\" in df_out.columns:\n",
    "    dt = pd.to_datetime(df_out[\"timestamp_dt\"])\n",
    "else:\n",
    "    dt = pd.to_datetime(df_out[\"timestamp_str\"].str.slice(0, 19), errors=\"coerce\")\n",
    "\n",
    "df_out[\"year\"] = dt.dt.year\n",
    "df_out[\"month\"] = dt.dt.month\n",
    "df_out[\"date\"] = dt.dt.date.astype(str)\n",
    "\n",
    "export_meta = [c for c in [\"timestamp_str\", \"year\", \"month\", \"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"] if c in df_out.columns]\n",
    "export_cols = export_meta + training_features + [LABEL_COL]\n",
    "export_cols = list(dict.fromkeys(export_cols))  # å»é‡\n",
    "\n",
    "df_export = df_out[export_cols].copy()\n",
    "df_export = df_export.rename(columns={\"timestamp_str\": \"timestamp\", LABEL_COL: \"regime\"})\n",
    "\n",
    "# è¾“å‡ºç›®å½•å…œåº•\n",
    "out_dir = None\n",
    "if \"OUTPUT_DIR_FEATURES\" in globals():\n",
    "    out_dir = OUTPUT_DIR_FEATURES\n",
    "elif \"OUTPUT_DIR_DATA\" in globals():\n",
    "    out_dir = OUTPUT_DIR_DATA\n",
    "else:\n",
    "    out_dir = \"./features_out\"\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "OUTPUT_TRAIN_CSV = os.path.join(out_dir, \"market_cycle_train_data.csv\")\n",
    "OUTPUT_TRAIN_PARQUET = os.path.join(out_dir, \"market_cycle_train_data.parquet\")\n",
    "\n",
    "df_export.to_csv(OUTPUT_TRAIN_CSV, index=False)\n",
    "df_export.to_parquet(OUTPUT_TRAIN_PARQUET, index=False)\n",
    "\n",
    "print(f\"\\nğŸ“ è®­ç»ƒæ•°æ®å·²å¯¼å‡º:\")\n",
    "print(f\"   - CSV: {OUTPUT_TRAIN_CSV}\")\n",
    "print(f\"   - Parquet: {OUTPUT_TRAIN_PARQUET}\")\n",
    "print(f\"   - æ ·æœ¬æ•°: {len(df_export):,}\")\n",
    "print(f\"   - ç‰¹å¾æ•°: {len(training_features)}\")\n",
    "\n",
    "print(\"\\nCell 26 å®Œæˆ âœ…ï¼ˆæ–°ç­–ç•¥/éšå˜é‡ä¼˜å…ˆ + ä¸¥æ ¼æ— æ³„éœ²ï¼‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 31.1: Transformer å¸‚åœºå‘¨æœŸåˆ†ç±» (PyTorch + MPS | Latent-First) ==========\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------------- 0) è®¾å¤‡é€‰æ‹© ----------------\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"âœ… ä½¿ç”¨è®¾å¤‡: MPS (Apple GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"âœ… ä½¿ç”¨è®¾å¤‡: CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"âš ï¸ æœªæ£€æµ‹åˆ° GPU, ä½¿ç”¨ CPU\")\n",
    "\n",
    "# ---------------- 1) è¯»å–è®­ç»ƒæ•°æ® ----------------\n",
    "OUTPUT_DIR = Path(\"market_cycle\")\n",
    "OUTPUT_DIR_FEATURES = OUTPUT_DIR / \"features\"\n",
    "train_parquet_path = OUTPUT_DIR_FEATURES / \"market_cycle_train_data.parquet\"\n",
    "train_csv_path = OUTPUT_DIR_FEATURES / \"market_cycle_train_data.csv\"\n",
    "\n",
    "if train_parquet_path.exists():\n",
    "    print(f\"\\nä» Parquet è¯»å–è®­ç»ƒæ•°æ®: {train_parquet_path}\")\n",
    "    df_train = pd.read_parquet(train_parquet_path)\n",
    "elif train_csv_path.exists():\n",
    "    print(f\"\\nä» CSV è¯»å–è®­ç»ƒæ•°æ®: {train_csv_path}\")\n",
    "    df_train = pd.read_csv(train_csv_path)\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè·‘å®Œç‰¹å¾å¯¼å‡º Cellã€‚\\n\"\n",
    "        f\"æœŸæœ›è·¯å¾„: \\n  {train_parquet_path}\\n  æˆ– {train_csv_path}\"\n",
    "    )\n",
    "\n",
    "print(f\"æ•°æ®å½¢çŠ¶: {df_train.shape}\")\n",
    "\n",
    "# ---------------- 2) ç‰¹å¾é€‰æ‹©ï¼šéšå˜é‡ä¼˜å…ˆ + è‡ªåŠ¨è¿‡æ»¤æ³„éœ² ----------------\n",
    "meta_cols = [\"timestamp\", \"year\", \"month\", \"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"regime\"]\n",
    "\n",
    "# âœ… ä½ çš„â€œæ–°ç­–ç•¥éšå˜é‡â€ï¼ˆå­˜åœ¨å°±ç”¨ï¼Œä¸å­˜åœ¨è‡ªåŠ¨è·³è¿‡ï¼‰\n",
    "LATENT_COLS = [\n",
    "    \"impulse_push\",\n",
    "    \"impulse_control\",\n",
    "    \"coherence_strength\",\n",
    "    \"noise_regime\",\n",
    "    \"trend_env_score\",   # å¦‚æœä½ åšæŒåªæœ‰4ä¸ªï¼Œå°±æŠŠè¿™ä¸€è¡Œåˆ æ‰\n",
    "]\n",
    "\n",
    "# âœ… è‡ªåŠ¨æ’é™¤ï¼ˆé˜²æ­¢æŠŠæ ‡ç­¾/æ³„éœ²åˆ—å–‚è¿›å»ï¼‰\n",
    "EXCLUDED = set([\n",
    "    \"phase\",                    # ä½ ä¹‹å‰ phase ç›´æ¥ç”¨ regime åˆ¤è¿‡ â†’ å¼ºæ³„éœ²\n",
    "    \"d_barrier\",                # æœªæ¥è·¯å¾„ â†’ æ³„éœ²\n",
    "    \"trend_score_mtf\",\n",
    "    \"strong_15\", \"strong_60\",\n",
    "    \"regime_raw\", \"is_range\", \"is_range_extended\", \"is_range_in_trend\",\n",
    "])\n",
    "\n",
    "LEAK_KEYWORDS = [\"future\", \"_fwd\", \"barrier\"]  # åŒä¿é™©\n",
    "def is_bad_feature(c: str) -> bool:\n",
    "    if c in EXCLUDED:\n",
    "        return True\n",
    "    if c.startswith(\"regime\"):              # ä»»ä½• regime_* æ´¾ç”Ÿåˆ—éƒ½å…ˆæ’æ‰ï¼ˆå°¤å…¶ rolling ç»Ÿè®¡ï¼‰\n",
    "        return True\n",
    "    if any(k in c.lower() for k in LEAK_KEYWORDS):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# é€‰æ‹©æ¨¡å¼ï¼šåªç”¨éšå˜é‡ or éšå˜é‡ + å…¶ä»–ç‰¹å¾\n",
    "FEATURE_MODE = \"latent_only\"   # \"latent_only\" / \"latent_plus_all\"\n",
    "\n",
    "all_candidate = [c for c in df_train.columns if c not in meta_cols]\n",
    "all_candidate = [c for c in all_candidate if not is_bad_feature(c)]\n",
    "\n",
    "latent_exist = [c for c in LATENT_COLS if c in df_train.columns]\n",
    "if len(latent_exist) == 0:\n",
    "    print(\"âš ï¸ parquet ä¸­æ‰¾ä¸åˆ° LATENT_COLSï¼Œå›é€€ä¸ºä½¿ç”¨è¿‡æ»¤åçš„å…¨éƒ¨ç‰¹å¾ã€‚\")\n",
    "    feature_cols = all_candidate\n",
    "else:\n",
    "    if FEATURE_MODE == \"latent_only\":\n",
    "        feature_cols = latent_exist\n",
    "    else:\n",
    "        # latent + å…¶å®ƒï¼ˆå…ˆæ”¾ latentï¼Œå†åŠ å…¶å®ƒï¼‰\n",
    "        others = [c for c in all_candidate if c not in latent_exist]\n",
    "        feature_cols = latent_exist + others\n",
    "\n",
    "print(f\"\\nç‰¹å¾é€‰æ‹©æ¨¡å¼: {FEATURE_MODE}\")\n",
    "print(f\"æœ€ç»ˆç‰¹å¾æ•°é‡: {len(feature_cols)}\")\n",
    "print(\"ç‰¹å¾åˆ—ç¤ºä¾‹:\", feature_cols[:12])\n",
    "\n",
    "# ---------------- 3) æ—¶é—´æ’åº + æ¸…ç† NaN ----------------\n",
    "if \"timestamp\" in df_train.columns:\n",
    "    # å­—ç¬¦ä¸²ä¹Ÿèƒ½æ’åºï¼Œä½†æ›´ç¨³ï¼šè½¬ datetime å†æ’åº\n",
    "    ts = pd.to_datetime(df_train[\"timestamp\"].astype(str).str.slice(0, 19), errors=\"coerce\")\n",
    "    df_train = df_train.assign(_ts=ts).sort_values(\"_ts\").drop(columns=[\"_ts\"]).reset_index(drop=True)\n",
    "\n",
    "df_train = df_train.dropna(subset=feature_cols + [\"regime\"]).reset_index(drop=True)\n",
    "\n",
    "# æ ‡ç­¾: -1,0,1 â†’ 0,1,2\n",
    "label_raw = df_train[\"regime\"].astype(int).values\n",
    "y_all = label_raw + 1\n",
    "X_all = df_train[feature_cols].astype(\"float32\").values\n",
    "\n",
    "print(f\"æ¸…ç† NaN åå½¢çŠ¶: X={X_all.shape}, y={y_all.shape}\")\n",
    "\n",
    "# ---------------- 4) æŒ‰æ—¶é—´åˆ‡åˆ† Train / Val ----------------\n",
    "train_ratio = 0.8\n",
    "n_total = len(df_train)\n",
    "split_idx = int(n_total * train_ratio)\n",
    "\n",
    "X_train_all = X_all[:split_idx]\n",
    "y_train_all = y_all[:split_idx]\n",
    "X_val_all   = X_all[split_idx:]\n",
    "y_val_all   = y_all[split_idx:]\n",
    "\n",
    "print(f\"\\næ—¶é—´åˆ‡åˆ†:\")\n",
    "print(f\"  Train: {X_train_all.shape[0]:,} è¡Œ\")\n",
    "print(f\"  Val  : {X_val_all.shape[0]:,} è¡Œ\")\n",
    "\n",
    "# ---------------- 5) å½’ä¸€åŒ–ï¼ˆåªåœ¨ Train ä¸Š fitï¼‰ ----------------\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_all)\n",
    "\n",
    "X_train_all = scaler.transform(X_train_all).astype(\"float32\")\n",
    "X_val_all   = scaler.transform(X_val_all).astype(\"float32\")\n",
    "\n",
    "# ---------------- 6) æ„å»ºåºåˆ—æ ·æœ¬ ----------------\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 30\n",
    "LR = 1e-4\n",
    "\n",
    "# é¢„æµ‹ç›®æ ‡ï¼šå½“å‰barï¼ˆåºåˆ—æœ«ç«¯ï¼‰ or ä¸‹ä¸€æ ¹bar\n",
    "PREDICT_HORIZON = \"current\"   # \"current\" / \"next\"\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len, horizon=\"current\"):\n",
    "        assert len(X) == len(y)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seq_len = seq_len\n",
    "        self.horizon = horizon\n",
    "\n",
    "        if horizon == \"current\":\n",
    "            self.n_samples = len(X) - seq_len + 1\n",
    "        elif horizon == \"next\":\n",
    "            self.n_samples = len(X) - seq_len  # éœ€è¦ y[end] å­˜åœ¨\n",
    "        else:\n",
    "            raise ValueError(\"horizon must be 'current' or 'next'\")\n",
    "\n",
    "        if self.n_samples <= 0:\n",
    "            raise ValueError(f\"æ•°æ®å¤ªçŸ­ï¼Œæ— æ³•æ„é€ é•¿åº¦ä¸º {seq_len} çš„åºåˆ—ã€‚Xé•¿åº¦={len(X)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx\n",
    "        end = idx + self.seq_len\n",
    "        x_seq = self.X[start:end]  # (T, F)\n",
    "\n",
    "        if self.horizon == \"current\":\n",
    "            y_label = self.y[end - 1]\n",
    "        else:  # next\n",
    "            y_label = self.y[end]\n",
    "\n",
    "        return torch.from_numpy(x_seq).float(), torch.tensor(y_label, dtype=torch.long)\n",
    "\n",
    "train_dataset = SequenceDataset(X_train_all, y_train_all, SEQ_LEN, horizon=PREDICT_HORIZON)\n",
    "val_dataset   = SequenceDataset(X_val_all,   y_val_all,   SEQ_LEN, horizon=PREDICT_HORIZON)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"\\næ„é€ åºåˆ—åï¼š\")\n",
    "print(f\"  Train åºåˆ—æ•°: {len(train_dataset):,}\")\n",
    "print(f\"  Val   åºåˆ—æ•°: {len(val_dataset):,}\")\n",
    "\n",
    "# ---------------- 7) Transformer æ¨¡å‹ ----------------\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        d_model: int = 128,\n",
    "        nhead: int = 4,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "        num_classes: int = 3,\n",
    "        max_seq_len: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(num_features, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_seq_len, d_model))\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        B, T, F = x.shape\n",
    "        x = self.input_proj(x)                 # (B, T, D)\n",
    "        x = x + self.pos_embedding[:, :T, :]   # (B, T, D)\n",
    "        x = self.encoder(x)                    # (B, T, D)\n",
    "        last = x[:, -1, :]                     # (B, D)\n",
    "        return self.head(last)                 # (B, C)\n",
    "\n",
    "model = TimeSeriesTransformer(\n",
    "    num_features=len(feature_cols),\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.1,\n",
    "    num_classes=3,\n",
    "    max_seq_len=SEQ_LEN,\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\næ¨¡å‹ç»“æ„:\\n{model}\")\n",
    "\n",
    "# ---------------- 8) è®­ç»ƒå‡†å¤‡ ----------------\n",
    "# ç±»åˆ«ä¸å¹³è¡¡æ—¶å»ºè®®åŠ æƒï¼ˆå¯é€‰ï¼‰\n",
    "class_counts = np.bincount(y_train_all.astype(int), minlength=3)\n",
    "class_weights = (class_counts.sum() / np.maximum(class_counts, 1)).astype(np.float32)\n",
    "class_weights = class_weights / class_weights.sum() * 3.0\n",
    "class_weights_t = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_t)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_true, all_pred = [], []\n",
    "    total_loss, n_batches = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            logits = model(Xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "            pred = logits.argmax(dim=1)\n",
    "\n",
    "            all_true.append(yb.cpu().numpy())\n",
    "            all_pred.append(pred.cpu().numpy())\n",
    "\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_pred = np.concatenate(all_pred)\n",
    "\n",
    "    avg_loss = total_loss / max(n_batches, 1)\n",
    "    acc = accuracy_score(all_true, all_pred)\n",
    "    f1_macro = f1_score(all_true, all_pred, average=\"macro\")\n",
    "    f1_weighted = f1_score(all_true, all_pred, average=\"weighted\")\n",
    "    return avg_loss, acc, f1_macro, f1_weighted, all_true, all_pred\n",
    "\n",
    "# ---------------- 9) è®­ç»ƒå¾ªç¯ ----------------\n",
    "print(\"\\nå¼€å§‹ Transformer è®­ç»ƒ...\")\n",
    "\n",
    "best_val_f1 = -1e9\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss, n_batches = 0.0, 0\n",
    "\n",
    "    for Xb, yb in train_loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # ç¨³ä¸€ç‚¹\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    train_loss = total_loss / max(n_batches, 1)\n",
    "    val_loss, val_acc, val_f1_macro, val_f1_weighted, _, _ = evaluate(model, val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{N_EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.4f} | \"\n",
    "        f\"Val F1-macro: {val_f1_macro:.4f} | \"\n",
    "        f\"Val F1-weighted: {val_f1_weighted:.4f}\"\n",
    "    )\n",
    "\n",
    "    if val_f1_macro > best_val_f1:\n",
    "        best_val_f1 = val_f1_macro\n",
    "        best_state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optim\": optimizer.state_dict(),\n",
    "            \"val_f1_macro\": val_f1_macro,\n",
    "        }\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state[\"model\"])\n",
    "    print(f\"\\nâœ… åŠ è½½æœ€ä¼˜æ¨¡å‹ï¼šEpoch {best_state['epoch']} | best Val F1-macro={best_state['val_f1_macro']:.4f}\")\n",
    "\n",
    "# ---------------- 10) æœ€ç»ˆè¯„ä¼° ----------------\n",
    "val_loss, val_acc, val_f1_macro, val_f1_weighted, y_true, y_pred = evaluate(model, val_loader)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“Š æœ€ç»ˆéªŒè¯é›†è¡¨ç° (Transformer)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Val Loss       : {val_loss:.4f}\")\n",
    "print(f\"Val Accuracy   : {val_acc:.4f}\")\n",
    "print(f\"Val F1-macro   : {val_f1_macro:.4f}\")\n",
    "print(f\"Val F1-weighted: {val_f1_weighted:.4f}\")\n",
    "\n",
    "label_names = [\"DOWN (-1)\", \"RANGE (0)\", \"UP (+1)\"]\n",
    "print(\"\\nåˆ†ç±»æŠ¥å‘Šï¼š\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_names))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_norm = cm.astype(\"float32\") / np.maximum(cm.sum(axis=1, keepdims=True), 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=label_names, yticklabels=label_names, ax=axes[0])\n",
    "axes[0].set_xlabel(\"é¢„æµ‹ç±»åˆ«\")\n",
    "axes[0].set_ylabel(\"çœŸå®ç±»åˆ«\")\n",
    "axes[0].set_title(\"æ··æ·†çŸ©é˜µ (æ•°é‡)\")\n",
    "\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "            xticklabels=label_names, yticklabels=label_names, ax=axes[1])\n",
    "axes[1].set_xlabel(\"é¢„æµ‹ç±»åˆ«\")\n",
    "axes[1].set_ylabel(\"çœŸå®ç±»åˆ«\")\n",
    "axes[1].set_title(\"æ··æ·†çŸ©é˜µ (è¡Œå½’ä¸€åŒ–)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Transformer è®­ç»ƒå®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 31.2: æ”¹è¿›ç‰ˆ Transformer å¸‚åœºå‘¨æœŸåˆ†ç±» (PyTorch + MPS) ==========\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------- 0. è®¾å¤‡é€‰æ‹©ï¼šä¼˜å…ˆ MPSï¼Œå…¶æ¬¡ CUDAï¼Œæœ€å CPU --------\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"âœ… ä½¿ç”¨è®¾å¤‡: MPS (Apple GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"âœ… ä½¿ç”¨è®¾å¤‡: CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"âš ï¸ æœªæ£€æµ‹åˆ° GPU, ä½¿ç”¨ CPU\")\n",
    "\n",
    "# -------- 1. è¯»å–è®­ç»ƒæ•°æ®ï¼ˆå’Œ CatBoost åŒä¸€ä»½ï¼‰ --------\n",
    "OUTPUT_DIR = Path(\"market_cycle\")\n",
    "OUTPUT_DIR_FEATURES = OUTPUT_DIR / \"features\"\n",
    "train_parquet_path = OUTPUT_DIR_FEATURES / \"market_cycle_train_data.parquet\"\n",
    "train_csv_path = OUTPUT_DIR_FEATURES / \"market_cycle_train_data.csv\"\n",
    "\n",
    "if train_parquet_path.exists():\n",
    "    print(f\"\\nä» Parquet è¯»å–è®­ç»ƒæ•°æ®: {train_parquet_path}\")\n",
    "    df_train = pd.read_parquet(train_parquet_path)\n",
    "elif train_csv_path.exists():\n",
    "    print(f\"\\nä» CSV è¯»å–è®­ç»ƒæ•°æ®: {train_csv_path}\")\n",
    "    df_train = pd.read_csv(train_csv_path)\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè·‘å®Œç‰¹å¾å¯¼å‡º Cellã€‚\\n\"\n",
    "        f\"æœŸæœ›è·¯å¾„: \\n  {train_parquet_path}\\n  æˆ– {train_csv_path}\"\n",
    "    )\n",
    "\n",
    "print(f\"æ•°æ®å½¢çŠ¶: {df_train.shape}\")\n",
    "\n",
    "# -------- 2. ç‰¹å¾å’Œæ ‡ç­¾åˆ—å®šä¹‰ï¼ˆä¿æŒå’Œ CatBoost Cell å®Œå…¨ä¸€è‡´ï¼‰ --------\n",
    "meta_cols = [\n",
    "    \"timestamp\", \"year\", \"month\", \"date\",\n",
    "    \"open\", \"high\", \"low\", \"close\", \"volume\", \"regime\"\n",
    "]\n",
    "feature_cols = [c for c in df_train.columns if c not in meta_cols]\n",
    "\n",
    "print(f\"ç‰¹å¾æ•°é‡: {len(feature_cols)}\")\n",
    "print(f\"æ ·æœ¬æ•°é‡(è¡Œæ•°): {len(df_train):,}\")\n",
    "\n",
    "# æ—¶é—´æ’åºï¼Œä¿è¯æ—¶åºæ­£ç¡®\n",
    "if \"timestamp\" in df_train.columns:\n",
    "    df_train = df_train.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# å»æ‰å« NaN çš„è¡Œï¼ˆç‰¹å¾ + æ ‡ç­¾ï¼‰\n",
    "df_train = df_train.dropna(subset=feature_cols + [\"regime\"]).reset_index(drop=True)\n",
    "\n",
    "label_raw = df_train[\"regime\"].astype(int).values          # -1,0,1\n",
    "y_all = label_raw + 1                                      # â†’ 0,1,2\n",
    "X_all = df_train[feature_cols].astype(\"float32\").values    # (N, F)\n",
    "\n",
    "print(f\"æ¸…ç† NaN åå½¢çŠ¶: X={X_all.shape}, y={y_all.shape}\")\n",
    "\n",
    "# -------- 3. æŒ‰æ—¶é—´åˆ‡åˆ† Train / Val --------\n",
    "train_ratio = 0.8\n",
    "n_total = len(df_train)\n",
    "split_idx = int(n_total * train_ratio)\n",
    "\n",
    "X_train_all = X_all[:split_idx]\n",
    "y_train_all = y_all[:split_idx]\n",
    "X_val_all   = X_all[split_idx:]\n",
    "y_val_all   = y_all[split_idx:]\n",
    "\n",
    "print(f\"\\næ—¶é—´åˆ‡åˆ†:\")\n",
    "print(f\"  Train: {X_train_all.shape[0]:,} è¡Œ\")\n",
    "print(f\"  Val  : {X_val_all.shape[0]:,} è¡Œ\")\n",
    "\n",
    "# -------- 4. å½’ä¸€åŒ–ï¼ˆStandardScalerï¼Œåªåœ¨ Train ä¸Š fitï¼‰ --------\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_all)\n",
    "\n",
    "X_train_all = scaler.transform(X_train_all).astype(\"float32\")\n",
    "X_val_all   = scaler.transform(X_val_all).astype(\"float32\")\n",
    "\n",
    "# -------- 5. æ„å»ºæ—¶åºæ ·æœ¬ï¼šæ»‘åŠ¨çª—å£åºåˆ— --------\n",
    "SEQ_LEN = 64      # åºåˆ—é•¿åº¦ï¼ˆä¿æŒå’ŒåŸå§‹ç‰ˆæœ¬ä¸€è‡´ï¼›ä½ ä¹Ÿå¯ä»¥æ”¹æˆ 96 å†è¯•ï¼‰\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 15\n",
    "LR = 3e-4         # ç•¥å¤§ä¸€ç‚¹çš„å­¦ä¹ ç‡ï¼Œé…åˆ scheduler ä½¿ç”¨\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    æŒ‰æ»‘åŠ¨çª—å£æ„é€  (seq_len, num_features) â†’ label çš„æ ·æœ¬\n",
    "    ç”¨åºåˆ—æœ€åä¸€ä¸ª time step çš„æ ‡ç­¾ä½œä¸º y\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, seq_len):\n",
    "        assert len(X) == len(y)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seq_len = seq_len\n",
    "        self.n_samples = len(X) - seq_len + 1\n",
    "        if self.n_samples <= 0:\n",
    "            raise ValueError(\n",
    "                f\"æ•°æ®å¤ªçŸ­ï¼Œæ— æ³•æ„é€ é•¿åº¦ä¸º {seq_len} çš„åºåˆ—ï¼Œ\"\n",
    "                f\"å½“å‰ X é•¿åº¦ä¸º {len(X)}\"\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx\n",
    "        end = idx + self.seq_len\n",
    "        x_seq = self.X[start:end]           # (T, F)\n",
    "        y_label = self.y[end - 1]           # åºåˆ—æœ€åä¸€ä¸ªç‚¹çš„æ ‡ç­¾\n",
    "        x_tensor = torch.from_numpy(x_seq).float()\n",
    "        y_tensor = torch.tensor(y_label, dtype=torch.long)\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "train_dataset = SequenceDataset(X_train_all, y_train_all, SEQ_LEN)\n",
    "val_dataset   = SequenceDataset(X_val_all,   y_val_all,   SEQ_LEN)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,   # æŒ‰åºåˆ—æ‰“ä¹±\n",
    "    drop_last=False,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(f\"\\næ„é€ åºåˆ—åï¼š\")\n",
    "print(f\"  Train åºåˆ—æ•°: {len(train_dataset):,}\")\n",
    "print(f\"  Val   åºåˆ—æ•°: {len(val_dataset):,}\")\n",
    "\n",
    "# -------- 6. å®šä¹‰ æ”¹è¿›ç‰ˆ Transformer æ¨¡å‹ --------\n",
    "class TimeSeriesTransformerV2(nn.Module):\n",
    "    \"\"\"\n",
    "    æ”¹è¿›ç‚¹ï¼š\n",
    "    1ï¼‰ä¿æŒå’ŒåŸç‰ˆä¸€æ ·çš„ encoder ç»“æ„ï¼ˆd_model=128, 2 å±‚ encoderï¼‰\n",
    "    2ï¼‰ä½¿ç”¨ learnable positional embedding\n",
    "    3ï¼‰åŠ ä¸€ä¸ª attention poolingï¼Œè€Œä¸æ˜¯åªç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        d_model: int = 128,\n",
    "        nhead: int = 4,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "        num_classes: int = 3,\n",
    "        max_seq_len: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # è¾“å…¥ç‰¹å¾ â†’ d_model ç»´\n",
    "        self.input_proj = nn.Linear(num_features, d_model)\n",
    "\n",
    "        # å¯å­¦ä¹ ä½ç½®ç¼–ç \n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.zeros(1, max_seq_len, d_model)\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,  # (B, T, D)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # attention poolingï¼šä»æ•´ä¸ªåºåˆ—åšä¸€ä¸ªåŠ æƒå¹³å‡\n",
    "        self.attn = nn.Linear(d_model, 1)\n",
    "\n",
    "        # åˆ†ç±»å¤´\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, F)\n",
    "        \"\"\"\n",
    "        B, T, F = x.shape\n",
    "\n",
    "        # æŠ•å½±\n",
    "        x = self.input_proj(x)  # (B, T, D)\n",
    "\n",
    "        # åŠ ä½ç½®ç¼–ç \n",
    "        pos = self.pos_embedding[:, :T, :]  # (1, T, D)\n",
    "        x = x + pos\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.encoder(x)  # (B, T, D)\n",
    "\n",
    "        # attention pooling\n",
    "        # attn_scores: (B, T, 1) -> (B, T)\n",
    "        attn_scores = self.attn(x).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)  # (B, T)\n",
    "        # åŠ æƒæ±‚å’Œï¼š (B, T, D) * (B, T, 1)\n",
    "        context = torch.sum(x * attn_weights.unsqueeze(-1), dim=1)  # (B, D)\n",
    "\n",
    "        logits = self.fc(context)  # (B, num_classes)\n",
    "        return logits\n",
    "\n",
    "num_features = len(feature_cols)\n",
    "num_classes = 3\n",
    "\n",
    "model = TimeSeriesTransformerV2(\n",
    "    num_features=num_features,\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.1,\n",
    "    num_classes=num_classes,\n",
    "    max_seq_len=SEQ_LEN,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"\\næ¨¡å‹ç»“æ„:\\n{model}\")\n",
    "\n",
    "# -------- 7. æŸå¤±å‡½æ•°ï¼ˆåŠ å…¥ç±»åˆ«æƒé‡ï¼‰ï¼Œä¼˜åŒ–å™¨ & Scheduler --------\n",
    "# è®­ç»ƒé›†ä¸Šçš„ç±»åˆ«æƒé‡ï¼ˆæ ‡ç­¾æ˜¯ 0,1,2ï¼‰\n",
    "class_counts = np.bincount(y_train_all, minlength=num_classes)\n",
    "print(\"\\nè®­ç»ƒé›†æ ‡ç­¾è®¡æ•°:\", class_counts)\n",
    "\n",
    "total = class_counts.sum()\n",
    "class_weights = total / (num_classes * class_counts.astype(np.float32))\n",
    "class_weights = class_weights / class_weights.mean()  # å½’ä¸€åŒ–åˆ°å‡å€¼=1ï¼Œæ–¹ä¾¿è°ƒå‚\n",
    "print(\"ç±»åˆ«æƒé‡:\", class_weights)\n",
    "\n",
    "class_weights_t = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_t)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "# ä½¿ç”¨ ReduceLROnPlateauï¼Œæ ¹æ®éªŒè¯é›† F1-macro è°ƒæ•´å­¦ä¹ ç‡\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_true.extend(y_batch.cpu().numpy())\n",
    "            all_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / max(n_batches, 1)\n",
    "    acc = accuracy_score(all_true, all_pred)\n",
    "    f1_macro = f1_score(all_true, all_pred, average=\"macro\")\n",
    "    f1_weighted = f1_score(all_true, all_pred, average=\"weighted\")\n",
    "\n",
    "    return avg_loss, acc, f1_macro, f1_weighted, np.array(all_true), np.array(all_pred)\n",
    "\n",
    "# -------- 8. è®­ç»ƒå¾ªç¯ --------\n",
    "print(\"\\nå¼€å§‹ æ”¹è¿›ç‰ˆ Transformer è®­ç»ƒ...\")\n",
    "\n",
    "best_val_f1 = -np.inf\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    n_train_batches = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        n_train_batches += 1\n",
    "\n",
    "    train_loss = total_train_loss / max(n_train_batches, 1)\n",
    "\n",
    "    val_loss, val_acc, val_f1_macro, val_f1_weighted, y_true_val, y_pred_val = evaluate(model, val_loader)\n",
    "\n",
    "    # æ›´æ–° schedulerï¼ˆä½¿ç”¨ F1-macroï¼‰\n",
    "    scheduler.step(val_f1_macro)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{N_EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.4f} | \"\n",
    "        f\"Val F1-macro: {val_f1_macro:.4f} | \"\n",
    "        f\"Val F1-weighted: {val_f1_weighted:.4f}\"\n",
    "    )\n",
    "\n",
    "    # ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆæŒ‰ macro F1ï¼‰\n",
    "    if val_f1_macro > best_val_f1:\n",
    "        best_val_f1 = val_f1_macro\n",
    "        best_state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"val_f1_macro\": val_f1_macro,\n",
    "        }\n",
    "\n",
    "# åŠ è½½æœ€å¥½çš„ä¸€æ¬¡\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state[\"model_state_dict\"])\n",
    "    print(f\"\\nâœ… åŠ è½½éªŒè¯é›† F1-macro æœ€ä¼˜æ¨¡å‹ (Epoch {best_state['epoch']})ï¼Œæœ€ä½³ F1-macro = {best_state['val_f1_macro']:.4f}\")\n",
    "\n",
    "# -------- 9. æœ€ç»ˆåœ¨éªŒè¯é›†ä¸Šè¯„ä¼° --------\n",
    "val_loss, val_acc, val_f1_macro, val_f1_weighted, y_true, y_pred = evaluate(model, val_loader)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“Š æœ€ç»ˆéªŒè¯é›†è¡¨ç° (æ”¹è¿›ç‰ˆ Transformer)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Val Loss       : {val_loss:.4f}\")\n",
    "print(f\"Val Accuracy   : {val_acc:.4f}\")\n",
    "print(f\"Val F1-macro   : {val_f1_macro:.4f}\")\n",
    "print(f\"Val F1-weighted: {val_f1_weighted:.4f}\")\n",
    "\n",
    "label_names = [\"DOWN (-1)\", \"RANGE (0)\", \"UP (+1)\"]\n",
    "print(\"\\nåˆ†ç±»æŠ¥å‘Šï¼š\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_names))\n",
    "\n",
    "# -------- 10. æ··æ·†çŸ©é˜µå¯è§†åŒ– --------\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_norm = cm.astype(\"float32\") / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=label_names,\n",
    "    yticklabels=label_names,\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_xlabel(\"é¢„æµ‹ç±»åˆ«\")\n",
    "axes[0].set_ylabel(\"çœŸå®ç±»åˆ«\")\n",
    "axes[0].set_title(\"æ··æ·†çŸ©é˜µ (æ•°é‡)\")\n",
    "\n",
    "sns.heatmap(\n",
    "    cm_norm,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=label_names,\n",
    "    yticklabels=label_names,\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_xlabel(\"é¢„æµ‹ç±»åˆ«\")\n",
    "axes[1].set_ylabel(\"çœŸå®ç±»åˆ«\")\n",
    "axes[1].set_title(\"æ··æ·†çŸ©é˜µ (è¡Œå½’ä¸€åŒ–)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… æ”¹è¿›ç‰ˆ Transformer è®­ç»ƒå®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 31: é¢„æµ‹ vs çœŸå®æ ‡ç­¾ Kçº¿å›¾å¯¹æ¯” ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ å¯è§†åŒ–æ¨¡å‹é¢„æµ‹æ•ˆæœ\n",
    "   - å·¦ä¾§: çœŸå®æ ‡ç­¾ (Ground Truth)\n",
    "   - å³ä¾§: æ¨¡å‹é¢„æµ‹æ ‡ç­¾ (Prediction)\n",
    "   - èƒŒæ™¯è‰²: çº¢è‰²=DOWN, é»„è‰²=RANGE, ç»¿è‰²=UP\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, BoxAnnotation, Span, Label\n",
    "from bokeh.layouts import row as bokeh_row, column as bokeh_column\n",
    "from bokeh.io import output_file, save\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“Š é¢„æµ‹ vs çœŸå®æ ‡ç­¾ Kçº¿å›¾å¯¹æ¯”\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========== 1. å‡†å¤‡é¢„æµ‹æ•°æ® ==========\n",
    "# ä½¿ç”¨æœ€åä¸€æŠ˜çš„éªŒè¯é›†è¿›è¡Œå¯è§†åŒ–\n",
    "print(\"\\nå‡†å¤‡å¯è§†åŒ–æ•°æ®...\")\n",
    "\n",
    "# è·å–æœ€åä¸€æŠ˜çš„ç´¢å¼•\n",
    "last_fold_train_idx, last_fold_val_idx = list(tscv.split(X))[-1]\n",
    "\n",
    "# è·å–éªŒè¯é›†çš„åŸå§‹æ•°æ®\n",
    "df_val = df_train.iloc[last_fold_val_idx].copy()\n",
    "df_val['y_true'] = y[last_fold_val_idx]  # åŸå§‹æ ‡ç­¾ {-1, 0, 1}\n",
    "\n",
    "# é¢„æµ‹ (ä½¿ç”¨æœ€åä¸€æŠ˜è®­ç»ƒçš„æ¨¡å‹)\n",
    "X_val = X[last_fold_val_idx]\n",
    "y_pred_mapped = model.predict(X_val).flatten().astype(int)\n",
    "df_val['y_pred'] = y_pred_mapped - 1  # è½¬å› {-1, 0, 1}\n",
    "\n",
    "# è§£ææ—¥æœŸ (timestamp æ˜¯å­—ç¬¦ä¸²æ ¼å¼)\n",
    "df_val['date'] = df_val['timestamp'].astype(str).str[:10]\n",
    "\n",
    "print(f\"éªŒè¯é›†æ ·æœ¬æ•°: {len(df_val):,}\")\n",
    "print(f\"éªŒè¯é›†æ—¥æœŸèŒƒå›´: {df_val['date'].min()} ~ {df_val['date'].max()}\")\n",
    "\n",
    "# ========== 2. Kçº¿ç»˜å›¾å‡½æ•° ==========\n",
    "def plot_kline_with_regime(df_day, regime_col, title, width=600):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶å¸¦ regime èƒŒæ™¯è‰²çš„ K çº¿å›¾\n",
    "    \"\"\"\n",
    "    df_day = df_day.copy().reset_index(drop=True)\n",
    "    df_day['idx'] = range(len(df_day))\n",
    "    \n",
    "    # åˆ›å»ºå›¾è¡¨\n",
    "    p = figure(\n",
    "        width=width, height=300,\n",
    "        title=title,\n",
    "        x_axis_label=\"Bar Index\",\n",
    "        y_axis_label=\"Price\",\n",
    "        tools=\"pan,wheel_zoom,box_zoom,reset\",\n",
    "    )\n",
    "    \n",
    "    # Regime èƒŒæ™¯è‰²\n",
    "    regime_colors = {-1: \"#FFCCCC\", 0: \"#FFFFCC\", 1: \"#CCFFCC\"}\n",
    "    regime_names = {-1: \"DOWN\", 0: \"RANGE\", 1: \"UP\"}\n",
    "    \n",
    "    # åˆ†æ®µæ·»åŠ èƒŒæ™¯\n",
    "    current_regime = None\n",
    "    start_idx = 0\n",
    "    \n",
    "    for i in range(len(df_day)):\n",
    "        r = int(df_day[regime_col].iloc[i]) if not pd.isna(df_day[regime_col].iloc[i]) else 0\n",
    "        \n",
    "        if current_regime is None:\n",
    "            current_regime = r\n",
    "            start_idx = i\n",
    "        elif r != current_regime or i == len(df_day) - 1:\n",
    "            end_idx = i if r != current_regime else i + 1\n",
    "            \n",
    "            box = BoxAnnotation(\n",
    "                left=start_idx - 0.5, right=end_idx - 0.5,\n",
    "                fill_color=regime_colors.get(current_regime, \"#FFFFFF\"),\n",
    "                fill_alpha=0.4, level=\"underlay\"\n",
    "            )\n",
    "            p.add_layout(box)\n",
    "            \n",
    "            current_regime = r\n",
    "            start_idx = i\n",
    "    \n",
    "    # ç»˜åˆ¶ K çº¿\n",
    "    bull_mask = df_day['close'] >= df_day['open']\n",
    "    bear_mask = ~bull_mask\n",
    "    \n",
    "    # é˜³çº¿\n",
    "    bull_idx = df_day[bull_mask]['idx']\n",
    "    if len(bull_idx) > 0:\n",
    "        p.segment(x0=bull_idx, y0=df_day[bull_mask]['low'], \n",
    "                  x1=bull_idx, y1=df_day[bull_mask]['high'], color=\"green\", line_width=1)\n",
    "        p.vbar(x=bull_idx, width=0.6, top=df_day[bull_mask]['close'], \n",
    "               bottom=df_day[bull_mask]['open'], fill_color=\"green\", line_color=\"green\")\n",
    "    \n",
    "    # é˜´çº¿\n",
    "    bear_idx = df_day[bear_mask]['idx']\n",
    "    if len(bear_idx) > 0:\n",
    "        p.segment(x0=bear_idx, y0=df_day[bear_mask]['low'], \n",
    "                  x1=bear_idx, y1=df_day[bear_mask]['high'], color=\"red\", line_width=1)\n",
    "        p.vbar(x=bear_idx, width=0.6, top=df_day[bear_mask]['open'], \n",
    "               bottom=df_day[bear_mask]['close'], fill_color=\"red\", line_color=\"red\")\n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "# ========== 3. é€‰æ‹©ä»£è¡¨æ€§æ—¥æœŸ ==========\n",
    "# é€‰æ‹©é¢„æµ‹æ•ˆæœå·®å¼‚æ˜æ˜¾çš„æ—¥æœŸè¿›è¡Œå±•ç¤º\n",
    "print(\"\\né€‰æ‹©ä»£è¡¨æ€§æ—¥æœŸ...\")\n",
    "\n",
    "# æŒ‰æ—¥è®¡ç®—å‡†ç¡®ç‡\n",
    "daily_accuracy = df_val.groupby('date').apply(\n",
    "    lambda x: (x['y_true'] == x['y_pred']).mean()\n",
    ").sort_values()\n",
    "\n",
    "# é€‰æ‹©: 2ä¸ªä½å‡†ç¡®ç‡æ—¥ + 2ä¸ªä¸­ç­‰å‡†ç¡®ç‡æ—¥ + 2ä¸ªé«˜å‡†ç¡®ç‡æ—¥\n",
    "n_days = len(daily_accuracy)\n",
    "sample_dates = []\n",
    "\n",
    "# ä½å‡†ç¡®ç‡ (æ¨¡å‹è¡¨ç°å·®çš„æ—¥å­)\n",
    "sample_dates.extend(daily_accuracy.head(2).index.tolist())\n",
    "# ä¸­ç­‰å‡†ç¡®ç‡\n",
    "mid_idx = n_days // 2\n",
    "sample_dates.extend(daily_accuracy.iloc[mid_idx:mid_idx+2].index.tolist())\n",
    "# é«˜å‡†ç¡®ç‡ (æ¨¡å‹è¡¨ç°å¥½çš„æ—¥å­)\n",
    "sample_dates.extend(daily_accuracy.tail(2).index.tolist())\n",
    "\n",
    "print(f\"é€‰æ‹©çš„æ—¥æœŸåŠå…¶å‡†ç¡®ç‡:\")\n",
    "for d in sample_dates:\n",
    "    acc = daily_accuracy[d]\n",
    "    print(f\"  {d}: {acc:.2%}\")\n",
    "\n",
    "# ========== 4. ç”Ÿæˆå¹¶æ’å¯¹æ¯”å›¾ ==========\n",
    "print(\"\\nç”Ÿæˆå¯¹æ¯”å›¾...\")\n",
    "\n",
    "comparison_plots = []\n",
    "\n",
    "for date in sample_dates:\n",
    "    df_day = df_val[df_val['date'] == date].copy()\n",
    "    \n",
    "    if len(df_day) < 20:\n",
    "        continue\n",
    "    \n",
    "    # è®¡ç®—è¯¥æ—¥ç»Ÿè®¡\n",
    "    day_acc = (df_day['y_true'] == df_day['y_pred']).mean()\n",
    "    \n",
    "    # çœŸå®æ ‡ç­¾åˆ†å¸ƒ\n",
    "    true_dist = df_day['y_true'].value_counts().to_dict()\n",
    "    pred_dist = df_day['y_pred'].value_counts().to_dict()\n",
    "    \n",
    "    # å·¦å›¾: çœŸå®æ ‡ç­¾\n",
    "    title_true = f\"{date} çœŸå®æ ‡ç­¾ | D:{true_dist.get(-1,0)} R:{true_dist.get(0,0)} U:{true_dist.get(1,0)}\"\n",
    "    p_true = plot_kline_with_regime(df_day, 'y_true', title_true, width=550)\n",
    "    \n",
    "    # å³å›¾: é¢„æµ‹æ ‡ç­¾\n",
    "    title_pred = f\"{date} é¢„æµ‹æ ‡ç­¾ | D:{pred_dist.get(-1,0)} R:{pred_dist.get(0,0)} U:{pred_dist.get(1,0)} | Acc:{day_acc:.1%}\"\n",
    "    p_pred = plot_kline_with_regime(df_day, 'y_pred', title_pred, width=550)\n",
    "    \n",
    "    # å¹¶æ’\n",
    "    comparison_plots.append(bokeh_row(p_true, p_pred))\n",
    "\n",
    "if comparison_plots:\n",
    "    comparison_layout = bokeh_column(*comparison_plots)\n",
    "    show(comparison_layout)\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    chart_path = OUTPUT_DIR_CHARTS / \"prediction_vs_truth_comparison.html\"\n",
    "    output_file(str(chart_path))\n",
    "    save(comparison_layout, filename=str(chart_path), title=\"Prediction vs Truth\")\n",
    "    print(f\"\\nå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {chart_path}\")\n",
    "\n",
    "# ========== 5. é”™è¯¯åˆ†æç»Ÿè®¡ ==========\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“ˆ é”™è¯¯åˆ†æ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# æ··æ·†æƒ…å†µç»Ÿè®¡\n",
    "df_val['correct'] = df_val['y_true'] == df_val['y_pred']\n",
    "df_val['error_type'] = df_val.apply(\n",
    "    lambda r: f\"{['DOWN','RANGE','UP'][int(r['y_true'])+1]}â†’{['DOWN','RANGE','UP'][int(r['y_pred'])+1]}\" \n",
    "    if not r['correct'] else 'CORRECT', axis=1\n",
    ")\n",
    "\n",
    "error_counts = df_val[~df_val['correct']]['error_type'].value_counts()\n",
    "print(\"\\næœ€å¸¸è§çš„é¢„æµ‹é”™è¯¯:\")\n",
    "for err, count in error_counts.head(10).items():\n",
    "    pct = count / len(df_val) * 100\n",
    "    print(f\"  {err}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# æŒ‰çœŸå®æ ‡ç­¾ç»Ÿè®¡å‡†ç¡®ç‡\n",
    "print(\"\\nå„ç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡:\")\n",
    "for label in [-1, 0, 1]:\n",
    "    mask = df_val['y_true'] == label\n",
    "    acc = df_val[mask]['correct'].mean()\n",
    "    label_name = {-1: 'DOWN', 0: 'RANGE', 1: 'UP'}[label]\n",
    "    print(f\"  {label_name}: {acc:.2%} (n={mask.sum():,})\")\n",
    "\n",
    "# ========== 6. è¿ç»­é”™è¯¯åˆ†æ ==========\n",
    "print(\"\\nè¿ç»­é¢„æµ‹é”™è¯¯åˆ†æ:\")\n",
    "\n",
    "# æ‰¾å‡ºè¿ç»­é”™è¯¯çš„æ®µè½\n",
    "df_val['error_run'] = (~df_val['correct']).astype(int)\n",
    "df_val['error_run_id'] = (df_val['error_run'] != df_val['error_run'].shift()).cumsum()\n",
    "\n",
    "error_runs = df_val[~df_val['correct']].groupby('error_run_id').size()\n",
    "if len(error_runs) > 0:\n",
    "    print(f\"  å¹³å‡è¿ç»­é”™è¯¯é•¿åº¦: {error_runs.mean():.1f} bars\")\n",
    "    print(f\"  æœ€å¤§è¿ç»­é”™è¯¯é•¿åº¦: {error_runs.max()} bars\")\n",
    "    print(f\"  è¿ç»­é”™è¯¯æ®µæ•°: {len(error_runs)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… é¢„æµ‹ vs çœŸå®æ ‡ç­¾å¯¹æ¯”å®Œæˆ!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell: Transformer é¢„æµ‹ vs çœŸå®æ ‡ç­¾ Kçº¿å›¾å¯¹æ¯” ==========\n",
    "\n",
    "\"\"\"\n",
    "ğŸ“Œ å¯è§†åŒ– Transformer æ¨¡å‹é¢„æµ‹æ•ˆæœ\n",
    "   - ä½¿ç”¨ Cell 31.1 è®­ç»ƒå¥½çš„ Transformer æ¨¡å‹\n",
    "   - æ•°æ®æ¥æºï¼šåŒä¸€ä»½ df_trainï¼ŒéªŒè¯é›†ä¸ºæ—¶é—´åºåˆ—å°¾éƒ¨ 20%\n",
    "   - å·¦ä¾§: çœŸå®æ ‡ç­¾ (Ground Truth)\n",
    "   - å³ä¾§: æ¨¡å‹é¢„æµ‹æ ‡ç­¾ (Prediction)\n",
    "   - èƒŒæ™¯è‰²: çº¢è‰²=DOWN, é»„è‰²=RANGE, ç»¿è‰²=UP\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, BoxAnnotation\n",
    "from bokeh.layouts import row as bokeh_row, column as bokeh_column\n",
    "from bokeh.io import output_file, save\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# å¦‚æœæ²¡å®šä¹‰ OUTPUT_DIR_CHARTSï¼Œè¿™é‡Œè¡¥ä¸€ä¸‹\n",
    "OUTPUT_DIR = Path(\"market_cycle\")\n",
    "OUTPUT_DIR_CHARTS = OUTPUT_DIR / \"charts\"\n",
    "OUTPUT_DIR_CHARTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“Š Transformer é¢„æµ‹ vs çœŸå®æ ‡ç­¾ Kçº¿å›¾å¯¹æ¯”\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========== 1. é‡æ–°åœ¨éªŒè¯é›†ä¸Šè·‘ä¸€é evaluateï¼ˆä¿è¯ y_true / y_pred åœ¨å†…å­˜é‡Œï¼‰ ==========\n",
    "\n",
    "print(\"\\nåœ¨éªŒè¯é›†ä¸Šé‡æ–°è¯„ä¼° Transformer...\")\n",
    "\n",
    "val_loss, val_acc, val_f1_macro, val_f1_weighted, y_true_seq, y_pred_seq = evaluate(model, val_loader)\n",
    "\n",
    "print(f\"Val Loss       : {val_loss:.4f}\")\n",
    "print(f\"Val Accuracy   : {val_acc:.4f}\")\n",
    "print(f\"Val F1-macro   : {val_f1_macro:.4f}\")\n",
    "print(f\"Val F1-weighted: {val_f1_weighted:.4f}\")\n",
    "\n",
    "# y_true_seq / y_pred_seq æ˜¯åºåˆ—çº§çš„æ ‡ç­¾ (0,1,2)\n",
    "# å¯¹åº”çš„å…¨å±€ bar ç´¢å¼•è®¡ç®—æ–¹å¼ï¼š\n",
    "#   - éªŒè¯éƒ¨åˆ†åœ¨ df_train ä¸­ä» split_idx å¼€å§‹\n",
    "#   - SequenceDataset åœ¨éªŒè¯é›†ä¸Šï¼Œä»¥æ»‘åŠ¨çª—å£æ„é€ ï¼Œé•¿åº¦ = len(X_val_all) - SEQ_LEN + 1\n",
    "#   - æ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾å¯¹åº”çª—å£æœ€åä¸€ä¸ª bar:\n",
    "#         å…¨å±€ç´¢å¼• = split_idx + (æ ·æœ¬èµ·å§‹ index) + (SEQ_LEN - 1)\n",
    "#   - æ ·æœ¬èµ·å§‹ index ä¾æ¬¡ä¸º 0,1,2,... â†’ å…¨å±€ç´¢å¼•ä» split_idx + SEQ_LEN -1 ç›´åˆ°æœ€åä¸€è¡Œ\n",
    "\n",
    "n_val_seq = len(y_true_seq)\n",
    "val_start_global = split_idx + SEQ_LEN - 1\n",
    "val_indices_global = np.arange(val_start_global, val_start_global + n_val_seq)\n",
    "\n",
    "# å®‰å…¨æ£€æŸ¥\n",
    "assert val_indices_global.max() < len(df_train), \"ç´¢å¼•è¶Šç•Œï¼Œè¯·æ£€æŸ¥ split_idx / SEQ_LEN è®¾å®šã€‚\"\n",
    "\n",
    "# ========== 2. æ„é€ éªŒè¯é›† DataFrameï¼Œé™„ä¸ŠçœŸ / é¢„æµ‹æ ‡ç­¾ ==========\n",
    "print(\"\\næ„é€ å¯è§†åŒ–ç”¨ df_val...\")\n",
    "\n",
    "df_val = df_train.iloc[val_indices_global].copy()\n",
    "\n",
    "# å°† 0,1,2 æ˜ å°„å› -1,0,1\n",
    "df_val[\"y_true\"] = (y_true_seq - 1).astype(int)\n",
    "df_val[\"y_pred\"] = (y_pred_seq - 1).astype(int)\n",
    "\n",
    "# è§£ææ—¥æœŸ\n",
    "df_val[\"date\"] = df_val[\"timestamp\"].astype(str).str[:10]\n",
    "\n",
    "print(f\"éªŒè¯é›†æ ·æœ¬æ•° (æœ‰æ ‡ç­¾çš„ bar): {len(df_val):,}\")\n",
    "print(f\"éªŒè¯é›†æ—¥æœŸèŒƒå›´: {df_val['date'].min()} ~ {df_val['date'].max()}\")\n",
    "\n",
    "# ========== 3. å®šä¹‰å¸¦ regime èƒŒæ™¯è‰²çš„ K çº¿å›¾ç»˜åˆ¶å‡½æ•° (Bokeh) ==========\n",
    "\n",
    "def plot_kline_with_regime(df_day, regime_col, title, width=600):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶å¸¦ regime èƒŒæ™¯è‰²çš„ K çº¿å›¾ (Bokeh)\n",
    "    regime_col: 'y_true' æˆ– 'y_pred'\n",
    "    \"\"\"\n",
    "    df_day = df_day.copy().reset_index(drop=True)\n",
    "    df_day[\"idx\"] = range(len(df_day))\n",
    "    \n",
    "    p = figure(\n",
    "        width=width, height=300,\n",
    "        title=title,\n",
    "        x_axis_label=\"Bar Index\",\n",
    "        y_axis_label=\"Price\",\n",
    "        tools=\"pan,wheel_zoom,box_zoom,reset\",\n",
    "    )\n",
    "    \n",
    "    # èƒŒæ™¯è‰²: -1=çº¢,0=é»„,1=ç»¿\n",
    "    regime_colors = {-1: \"#FFCCCC\", 0: \"#FFFFCC\", 1: \"#CCFFCC\"}\n",
    "    \n",
    "    current_regime = None\n",
    "    start_idx = 0\n",
    "    \n",
    "    for i in range(len(df_day)):\n",
    "        val = df_day[regime_col].iloc[i]\n",
    "        r = int(val) if not pd.isna(val) else 0\n",
    "        \n",
    "        if current_regime is None:\n",
    "            current_regime = r\n",
    "            start_idx = i\n",
    "        elif r != current_regime or i == len(df_day) - 1:\n",
    "            end_idx = i if r != current_regime else i + 1\n",
    "            \n",
    "            box = BoxAnnotation(\n",
    "                left=start_idx - 0.5, right=end_idx - 0.5,\n",
    "                fill_color=regime_colors.get(current_regime, \"#FFFFFF\"),\n",
    "                fill_alpha=0.4, level=\"underlay\"\n",
    "            )\n",
    "            p.add_layout(box)\n",
    "            \n",
    "            current_regime = r\n",
    "            start_idx = i\n",
    "    \n",
    "    # ç”» K çº¿\n",
    "    bull_mask = df_day[\"close\"] >= df_day[\"open\"]\n",
    "    bear_mask = ~bull_mask\n",
    "    \n",
    "    bull_idx = df_day[bull_mask][\"idx\"]\n",
    "    if len(bull_idx) > 0:\n",
    "        p.segment(\n",
    "            x0=bull_idx, y0=df_day[bull_mask][\"low\"],\n",
    "            x1=bull_idx, y1=df_day[bull_mask][\"high\"],\n",
    "            color=\"green\", line_width=1\n",
    "        )\n",
    "        p.vbar(\n",
    "            x=bull_idx, width=0.6,\n",
    "            top=df_day[bull_mask][\"close\"],\n",
    "            bottom=df_day[bull_mask][\"open\"],\n",
    "            fill_color=\"green\", line_color=\"green\"\n",
    "        )\n",
    "    \n",
    "    bear_idx = df_day[bear_mask][\"idx\"]\n",
    "    if len(bear_idx) > 0:\n",
    "        p.segment(\n",
    "            x0=bear_idx, y0=df_day[bear_mask][\"low\"],\n",
    "            x1=bear_idx, y1=df_day[bear_mask][\"high\"],\n",
    "            color=\"red\", line_width=1\n",
    "        )\n",
    "        p.vbar(\n",
    "            x=bear_idx, width=0.6,\n",
    "            top=df_day[bear_mask][\"open\"],\n",
    "            bottom=df_day[bear_mask][\"close\"],\n",
    "            fill_color=\"red\", line_color=\"red\"\n",
    "        )\n",
    "    \n",
    "    return p\n",
    "\n",
    "# ========== 4. æŒ‰æ—¥ç»Ÿè®¡å‡†ç¡®ç‡ï¼Œé€‰ä»£è¡¨æ€§æ—¥æœŸå±•ç¤º ==========\n",
    "print(\"\\næŒ‰æ—¥ç»Ÿè®¡å‡†ç¡®ç‡ï¼Œé€‰æ‹©ä»£è¡¨æ€§æ—¥æœŸ...\")\n",
    "\n",
    "daily_accuracy = df_val.groupby(\"date\").apply(\n",
    "    lambda x: (x[\"y_true\"] == x[\"y_pred\"]).mean()\n",
    ").sort_values()\n",
    "\n",
    "n_days = len(daily_accuracy)\n",
    "sample_dates = []\n",
    "\n",
    "# 2 å¤©ä½å‡†ç¡®ç‡ + 2 å¤©ä¸­ç­‰ + 2 å¤©é«˜å‡†ç¡®ç‡\n",
    "if n_days >= 6:\n",
    "    sample_dates.extend(daily_accuracy.head(2).index.tolist())\n",
    "    mid_idx = n_days // 2\n",
    "    sample_dates.extend(daily_accuracy.iloc[mid_idx:mid_idx+2].index.tolist())\n",
    "    sample_dates.extend(daily_accuracy.tail(2).index.tolist())\n",
    "else:\n",
    "    sample_dates = daily_accuracy.index.tolist()\n",
    "\n",
    "print(\"é€‰æ‹©çš„æ—¥æœŸåŠå…¶æ—¥å†…å‡†ç¡®ç‡ï¼š\")\n",
    "for d in sample_dates:\n",
    "    acc = daily_accuracy[d]\n",
    "    print(f\"  {d}: {acc:.2%}\")\n",
    "\n",
    "# ========== 5. ç”Ÿæˆå¹¶æ’å¯¹æ¯”å›¾ (çœŸå® vs é¢„æµ‹) ==========\n",
    "\n",
    "print(\"\\nç”Ÿæˆ K çº¿å¯¹æ¯”å›¾...\")\n",
    "\n",
    "comparison_plots = []\n",
    "\n",
    "for date in sample_dates:\n",
    "    df_day = df_val[df_val[\"date\"] == date].copy()\n",
    "    if len(df_day) < 20:\n",
    "        continue\n",
    "    \n",
    "    day_acc = (df_day[\"y_true\"] == df_day[\"y_pred\"]).mean()\n",
    "    \n",
    "    true_dist = df_day[\"y_true\"].value_counts().to_dict()\n",
    "    pred_dist = df_day[\"y_pred\"].value_counts().to_dict()\n",
    "    \n",
    "    title_true = (\n",
    "        f\"{date} çœŸå®æ ‡ç­¾ | \"\n",
    "        f\"D:{true_dist.get(-1,0)} R:{true_dist.get(0,0)} U:{true_dist.get(1,0)}\"\n",
    "    )\n",
    "    p_true = plot_kline_with_regime(df_day, \"y_true\", title_true, width=550)\n",
    "    \n",
    "    title_pred = (\n",
    "        f\"{date} é¢„æµ‹æ ‡ç­¾ | \"\n",
    "        f\"D:{pred_dist.get(-1,0)} R:{pred_dist.get(0,0)} U:{pred_dist.get(1,0)} \"\n",
    "        f\"| Acc:{day_acc:.1%}\"\n",
    "    )\n",
    "    p_pred = plot_kline_with_regime(df_day, \"y_pred\", title_pred, width=550)\n",
    "    \n",
    "    comparison_plots.append(bokeh_row(p_true, p_pred))\n",
    "\n",
    "if comparison_plots:\n",
    "    comparison_layout = bokeh_column(*comparison_plots)\n",
    "    show(comparison_layout)\n",
    "    \n",
    "    chart_path = OUTPUT_DIR_CHARTS / \"transformer_prediction_vs_truth.html\"\n",
    "    output_file(str(chart_path))\n",
    "    save(comparison_layout, filename=str(chart_path), title=\"Transformer Prediction vs Truth\")\n",
    "    print(f\"\\nå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {chart_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ²¡æœ‰è¶³å¤Ÿæ ·æœ¬ç”Ÿæˆå¯¹æ¯”å›¾ã€‚\")\n",
    "\n",
    "# ========== 6. é”™è¯¯åˆ†æ ==========\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“ˆ Transformer é”™è¯¯åˆ†æ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df_val[\"correct\"] = df_val[\"y_true\"] == df_val[\"y_pred\"]\n",
    "\n",
    "df_val[\"error_type\"] = df_val.apply(\n",
    "    lambda r: (\n",
    "        f\"{['DOWN','RANGE','UP'][int(r['y_true'])+1]}â†’\"\n",
    "        f\"{['DOWN','RANGE','UP'][int(r['y_pred'])+1]}\"\n",
    "        if not r[\"correct\"] else \"CORRECT\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "error_counts = df_val[~df_val[\"correct\"]][\"error_type\"].value_counts()\n",
    "print(\"\\næœ€å¸¸è§çš„é¢„æµ‹é”™è¯¯ï¼š\")\n",
    "for err, count in error_counts.head(10).items():\n",
    "    pct = count / len(df_val) * 100\n",
    "    print(f\"  {err}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "print(\"\\nå„ç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡ï¼š\")\n",
    "for label in [-1, 0, 1]:\n",
    "    mask = df_val[\"y_true\"] == label\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    acc_lbl = df_val[mask][\"correct\"].mean()\n",
    "    label_name = {-1: \"DOWN\", 0: \"RANGE\", 1: \"UP\"}[label]\n",
    "    print(f\"  {label_name}: {acc_lbl:.2%} (n={mask.sum():,})\")\n",
    "\n",
    "print(\"\\nè¿ç»­é¢„æµ‹é”™è¯¯åˆ†æ:\")\n",
    "\n",
    "df_val[\"error_run\"] = (~df_val[\"correct\"]).astype(int)\n",
    "df_val[\"error_run_id\"] = (df_val[\"error_run\"] != df_val[\"error_run\"].shift()).cumsum()\n",
    "\n",
    "error_runs = df_val[~df_val[\"correct\"]].groupby(\"error_run_id\").size()\n",
    "if len(error_runs) > 0:\n",
    "    print(f\"  å¹³å‡è¿ç»­é”™è¯¯é•¿åº¦: {error_runs.mean():.1f} bars\")\n",
    "    print(f\"  æœ€å¤§è¿ç»­é”™è¯¯é•¿åº¦: {error_runs.max()} bars\")\n",
    "    print(f\"  è¿ç»­é”™è¯¯æ®µæ•°: {len(error_runs)}\")\n",
    "else:\n",
    "    print(\"  æ²¡æœ‰é”™è¯¯æ ·æœ¬ã€‚\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… Transformer é¢„æµ‹ vs çœŸå®æ ‡ç­¾å¯¹æ¯”å®Œæˆ!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 32: å¤šæ¨¡å‹å¯¹æ¯” - é…ç½®ä¸æ•°æ®å‡†å¤‡ ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ ä½¿ç”¨å¤šä¸ªæ ‘æ¨¡å‹è¿›è¡Œå¯¹æ¯”:\n",
    "   1. CatBoost (æ›´å¤æ‚é…ç½®)\n",
    "   2. LightGBM\n",
    "   3. XGBoost\n",
    "   4. Random Forest\n",
    "   5. Stacking Ensemble (Logistic Regression ä½œä¸º meta-learner)\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸš€ å¤šæ¨¡å‹å¯¹æ¯” & Stacking é›†æˆå­¦ä¹ \")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========== 1. å®‰è£…ä¾èµ– ==========\n",
    "print(\"\\næ£€æŸ¥å¹¶å®‰è£…ä¾èµ–...\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "    print(\"  âœ… CatBoost\")\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"catboost\", \"-q\"])\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "    print(\"  âœ… CatBoost (å·²å®‰è£…)\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(\"  âœ… LightGBM\")\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"lightgbm\", \"-q\"])\n",
    "    import lightgbm as lgb\n",
    "    print(\"  âœ… LightGBM (å·²å®‰è£…)\")\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"  âœ… XGBoost\")\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"xgboost\", \"-q\"])\n",
    "    import xgboost as xgb\n",
    "    print(\"  âœ… XGBoost (å·²å®‰è£…)\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# ========== 2. å‡†å¤‡æ•°æ® ==========\n",
    "print(\"\\nå‡†å¤‡è®­ç»ƒæ•°æ®...\")\n",
    "\n",
    "df_train_models = pd.read_parquet(OUTPUT_DIR_FEATURES / \"market_cycle_train_data.parquet\")\n",
    "\n",
    "feature_cols_models = [c for c in df_train_models.columns if c not in [\n",
    "    'timestamp', 'year', 'month', 'date', 'open', 'high', 'low', 'close', 'volume', 'regime'\n",
    "]]\n",
    "\n",
    "X_models = df_train_models[feature_cols_models].values\n",
    "y_models = df_train_models['regime'].values\n",
    "\n",
    "# æ ‡ç­¾æ˜ å°„: {-1, 0, 1} â†’ {0, 1, 2}\n",
    "y_models_mapped = y_models + 1\n",
    "label_names_models = ['DOWN', 'RANGE', 'UP']\n",
    "\n",
    "print(f\"ç‰¹å¾æ•°: {len(feature_cols_models)}, æ ·æœ¬æ•°: {len(X_models):,}\")\n",
    "\n",
    "# ========== 3. å®šä¹‰æ¨¡å‹å‚æ•° ==========\n",
    "print(\"\\né…ç½®æ¨¡å‹...\")\n",
    "\n",
    "# CatBoost (æ›´å¤æ‚)\n",
    "catboost_params = {\n",
    "    'iterations': 1000,\n",
    "    'depth': 8,\n",
    "    'learning_rate': 0.03,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'eval_metric': 'Accuracy',\n",
    "    'random_seed': 42,\n",
    "    'verbose': False,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'task_type': 'CPU',\n",
    "}\n",
    "\n",
    "# LightGBM\n",
    "lgb_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 127,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators': 1000,\n",
    "    'min_child_samples': 20,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'random_state': 42,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "# XGBoost\n",
    "xgb_params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators': 1000,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "# Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 15,\n",
    "    'min_samples_split': 10,\n",
    "    'min_samples_leaf': 5,\n",
    "    'max_features': 'sqrt',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'class_weight': 'balanced',\n",
    "}\n",
    "\n",
    "print(\"  CatBoost: iterations=1000, depth=8, lr=0.03\")\n",
    "print(\"  LightGBM: n_estimators=1000, num_leaves=127, lr=0.03\")\n",
    "print(\"  XGBoost: n_estimators=1000, max_depth=8, lr=0.03\")\n",
    "print(\"  RandomForest: n_estimators=500, max_depth=15\")\n",
    "\n",
    "# ========== 4. åˆå§‹åŒ–å­˜å‚¨ ==========\n",
    "N_SPLITS = 5\n",
    "tscv_models = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "\n",
    "model_results = {\n",
    "    'CatBoost': {'accuracy': [], 'f1_macro': [], 'time': []},\n",
    "    'LightGBM': {'accuracy': [], 'f1_macro': [], 'time': []},\n",
    "    'XGBoost': {'accuracy': [], 'f1_macro': [], 'time': []},\n",
    "    'RandomForest': {'accuracy': [], 'f1_macro': [], 'time': []},\n",
    "    'Stacking': {'accuracy': [], 'f1_macro': [], 'time': []},\n",
    "}\n",
    "\n",
    "final_y_true_models = []\n",
    "final_predictions_models = {name: [] for name in model_results.keys()}\n",
    "\n",
    "print(\"\\nâœ… é…ç½®å®Œæˆï¼Œå‡†å¤‡å¼€å§‹è®­ç»ƒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 33: å¤šæ¨¡å‹è®­ç»ƒå¾ªç¯ ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ 5-Fold æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è®­ç»ƒ\n",
    "   æ¯æŠ˜è®­ç»ƒ: CatBoost, LightGBM, XGBoost, RandomForest, Stacking\n",
    "\"\"\"\n",
    "print(\"=\" * 70)\n",
    "print(\"å¼€å§‹äº¤å‰éªŒè¯è®­ç»ƒ...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv_models.split(X_models)):\n",
    "    print(f\"\\n{'='*20} Fold {fold+1}/{N_SPLITS} {'='*20}\")\n",
    "    \n",
    "    X_train, X_val = X_models[train_idx], X_models[val_idx]\n",
    "    y_train, y_val = y_models_mapped[train_idx], y_models_mapped[val_idx]\n",
    "    \n",
    "    # ç”¨äº Stacking çš„ meta-features\n",
    "    meta_val = np.zeros((len(X_val), 4 * 3))  # 4ä¸ªæ¨¡å‹ x 3ç±»æ¦‚ç‡\n",
    "    \n",
    "    # ----- CatBoost -----\n",
    "    print(\"\\n  [1/4] Training CatBoost...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    train_pool = Pool(X_train, y_train)\n",
    "    val_pool = Pool(X_val, y_val)\n",
    "    \n",
    "    cat_model = CatBoostClassifier(**catboost_params)\n",
    "    cat_model.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "    \n",
    "    cat_pred = cat_model.predict(X_val).flatten().astype(int)\n",
    "    cat_proba = cat_model.predict_proba(X_val)\n",
    "    \n",
    "    cat_time = time.time() - start\n",
    "    cat_acc = accuracy_score(y_val, cat_pred)\n",
    "    cat_f1 = f1_score(y_val, cat_pred, average='macro')\n",
    "    \n",
    "    model_results['CatBoost']['accuracy'].append(cat_acc)\n",
    "    model_results['CatBoost']['f1_macro'].append(cat_f1)\n",
    "    model_results['CatBoost']['time'].append(cat_time)\n",
    "    \n",
    "    meta_val[:, 0:3] = cat_proba\n",
    "    print(f\"       Acc: {cat_acc:.4f}, F1: {cat_f1:.4f}, Time: {cat_time:.1f}s\")\n",
    "    \n",
    "    # ----- LightGBM -----\n",
    "    print(\"  [2/4] Training LightGBM...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "    lgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    lgb_pred = lgb_model.predict(X_val)\n",
    "    lgb_proba = lgb_model.predict_proba(X_val)\n",
    "    \n",
    "    lgb_time = time.time() - start\n",
    "    lgb_acc = accuracy_score(y_val, lgb_pred)\n",
    "    lgb_f1 = f1_score(y_val, lgb_pred, average='macro')\n",
    "    \n",
    "    model_results['LightGBM']['accuracy'].append(lgb_acc)\n",
    "    model_results['LightGBM']['f1_macro'].append(lgb_f1)\n",
    "    model_results['LightGBM']['time'].append(lgb_time)\n",
    "    \n",
    "    meta_val[:, 3:6] = lgb_proba\n",
    "    print(f\"       Acc: {lgb_acc:.4f}, F1: {lgb_f1:.4f}, Time: {lgb_time:.1f}s\")\n",
    "    \n",
    "    # ----- XGBoost -----\n",
    "    print(\"  [3/4] Training XGBoost...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "    xgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    xgb_pred = xgb_model.predict(X_val)\n",
    "    xgb_proba = xgb_model.predict_proba(X_val)\n",
    "    \n",
    "    xgb_time = time.time() - start\n",
    "    xgb_acc = accuracy_score(y_val, xgb_pred)\n",
    "    xgb_f1 = f1_score(y_val, xgb_pred, average='macro')\n",
    "    \n",
    "    model_results['XGBoost']['accuracy'].append(xgb_acc)\n",
    "    model_results['XGBoost']['f1_macro'].append(xgb_f1)\n",
    "    model_results['XGBoost']['time'].append(xgb_time)\n",
    "    \n",
    "    meta_val[:, 6:9] = xgb_proba\n",
    "    print(f\"       Acc: {xgb_acc:.4f}, F1: {xgb_f1:.4f}, Time: {xgb_time:.1f}s\")\n",
    "    \n",
    "    # ----- Random Forest -----\n",
    "    print(\"  [4/4] Training RandomForest...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    rf_model = RandomForestClassifier(**rf_params)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    rf_pred = rf_model.predict(X_val)\n",
    "    rf_proba = rf_model.predict_proba(X_val)\n",
    "    \n",
    "    rf_time = time.time() - start\n",
    "    rf_acc = accuracy_score(y_val, rf_pred)\n",
    "    rf_f1 = f1_score(y_val, rf_pred, average='macro')\n",
    "    \n",
    "    model_results['RandomForest']['accuracy'].append(rf_acc)\n",
    "    model_results['RandomForest']['f1_macro'].append(rf_f1)\n",
    "    model_results['RandomForest']['time'].append(rf_time)\n",
    "    \n",
    "    meta_val[:, 9:12] = rf_proba\n",
    "    print(f\"       Acc: {rf_acc:.4f}, F1: {rf_f1:.4f}, Time: {rf_time:.1f}s\")\n",
    "    \n",
    "    # ----- Stacking: Logistic Regression -----\n",
    "    print(\"  [Stacking] Training Meta-Learner...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # è·å–è®­ç»ƒé›†çš„ meta-features\n",
    "    cat_train_proba = cat_model.predict_proba(X_train)\n",
    "    lgb_train_proba = lgb_model.predict_proba(X_train)\n",
    "    xgb_train_proba = xgb_model.predict_proba(X_train)\n",
    "    rf_train_proba = rf_model.predict_proba(X_train)\n",
    "    \n",
    "    meta_train = np.hstack([cat_train_proba, lgb_train_proba, xgb_train_proba, rf_train_proba])\n",
    "    \n",
    "    # æ ‡å‡†åŒ– meta-features\n",
    "    scaler = StandardScaler()\n",
    "    meta_train_scaled = scaler.fit_transform(meta_train)\n",
    "    meta_val_scaled = scaler.transform(meta_val)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    stack_model = LogisticRegression(\n",
    "        C=1.0, max_iter=1000, multi_class='multinomial',\n",
    "        solver='lbfgs', random_state=42\n",
    "    )\n",
    "    stack_model.fit(meta_train_scaled, y_train)\n",
    "    \n",
    "    stack_pred = stack_model.predict(meta_val_scaled)\n",
    "    \n",
    "    stack_time = time.time() - start\n",
    "    stack_acc = accuracy_score(y_val, stack_pred)\n",
    "    stack_f1 = f1_score(y_val, stack_pred, average='macro')\n",
    "    \n",
    "    model_results['Stacking']['accuracy'].append(stack_acc)\n",
    "    model_results['Stacking']['f1_macro'].append(stack_f1)\n",
    "    model_results['Stacking']['time'].append(stack_time)\n",
    "    \n",
    "    print(f\"       Acc: {stack_acc:.4f}, F1: {stack_f1:.4f}, Time: {stack_time:.1f}s\")\n",
    "    \n",
    "    # æ”¶é›†æœ€ç»ˆé¢„æµ‹ (ç”¨äºæœ€åä¸€æŠ˜çš„å¯è§†åŒ–)\n",
    "    if fold == N_SPLITS - 1:\n",
    "        final_y_true_models = y_val\n",
    "        final_predictions_models['CatBoost'] = cat_pred\n",
    "        final_predictions_models['LightGBM'] = lgb_pred\n",
    "        final_predictions_models['XGBoost'] = xgb_pred\n",
    "        final_predictions_models['RandomForest'] = rf_pred\n",
    "        final_predictions_models['Stacking'] = stack_pred\n",
    "\n",
    "print(\"\\nâœ… è®­ç»ƒå®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 34: æ¨¡å‹å¯¹æ¯”æ±‡æ€» ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ æ±‡æ€»å„æ¨¡å‹çš„äº¤å‰éªŒè¯ç»“æœ\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“Š æ¨¡å‹å¯¹æ¯”æ±‡æ€»\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_data = []\n",
    "for name in model_results.keys():\n",
    "    acc_mean = np.mean(model_results[name]['accuracy'])\n",
    "    acc_std = np.std(model_results[name]['accuracy'])\n",
    "    f1_mean = np.mean(model_results[name]['f1_macro'])\n",
    "    f1_std = np.std(model_results[name]['f1_macro'])\n",
    "    time_total = np.sum(model_results[name]['time'])\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': f\"{acc_mean:.4f} Â± {acc_std:.4f}\",\n",
    "        'F1-Macro': f\"{f1_mean:.4f} Â± {f1_std:.4f}\",\n",
    "        'Acc_mean': acc_mean,\n",
    "        'F1_mean': f1_mean,\n",
    "        'Total Time (s)': f\"{time_total:.1f}\",\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).sort_values('Acc_mean', ascending=False)\n",
    "print(\"\\n\" + summary_df[['Model', 'Accuracy', 'F1-Macro', 'Total Time (s)']].to_string(index=False))\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³æ¨¡å‹\n",
    "best_model_name = summary_df.iloc[0]['Model']\n",
    "print(f\"\\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}\")\n",
    "\n",
    "# ========== å¯è§†åŒ–å¯¹æ¯” ==========\n",
    "print(\"\\nç»˜åˆ¶å¯¹æ¯”å›¾...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "models_list = list(model_results.keys())\n",
    "acc_means = [np.mean(model_results[m]['accuracy']) for m in models_list]\n",
    "acc_stds = [np.std(model_results[m]['accuracy']) for m in models_list]\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6', '#f39c12']\n",
    "\n",
    "# Accuracy å¯¹æ¯”\n",
    "bars1 = axes[0].bar(models_list, acc_means, yerr=acc_stds, capsize=5, color=colors, edgecolor='black')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”')\n",
    "axes[0].set_ylim([min(acc_means) - 0.05, max(acc_means) + 0.05])\n",
    "for bar, val in zip(bars1, acc_means):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                 f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# F1-Macro å¯¹æ¯”\n",
    "f1_means = [np.mean(model_results[m]['f1_macro']) for m in models_list]\n",
    "f1_stds = [np.std(model_results[m]['f1_macro']) for m in models_list]\n",
    "\n",
    "bars2 = axes[1].bar(models_list, f1_means, yerr=f1_stds, capsize=5, color=colors, edgecolor='black')\n",
    "axes[1].set_ylabel('F1-Macro')\n",
    "axes[1].set_title('æ¨¡å‹ F1-Macro å¯¹æ¯”')\n",
    "axes[1].set_ylim([min(f1_means) - 0.05, max(f1_means) + 0.05])\n",
    "for bar, val in zip(bars2, f1_means):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                 f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 35: æ··æ·†çŸ©é˜µå¯¹æ¯” ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ å¯è§†åŒ–å„æ¨¡å‹åœ¨æœ€åä¸€æŠ˜éªŒè¯é›†ä¸Šçš„æ··æ·†çŸ©é˜µ\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"ç»˜åˆ¶æ··æ·†çŸ©é˜µå¯¹æ¯”...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, preds) in enumerate(final_predictions_models.items()):\n",
    "    cm = confusion_matrix(final_y_true_models, preds)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=label_names_models, yticklabels=label_names_models)\n",
    "    axes[idx].set_xlabel('é¢„æµ‹')\n",
    "    axes[idx].set_ylabel('çœŸå®')\n",
    "    \n",
    "    acc = accuracy_score(final_y_true_models, preds)\n",
    "    axes[idx].set_title(f'{name}\\nAcc: {acc:.4f}')\n",
    "\n",
    "# éšè—ç¬¬6ä¸ªå­å›¾\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 36: å„ç±»åˆ« Recall åˆ†æ ==========\n",
    "\"\"\"\n",
    "ğŸ“Œ åˆ†æå„æ¨¡å‹åœ¨ä¸åŒç±»åˆ«ä¸Šçš„å¬å›ç‡\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“ˆ å„ç±»åˆ« Recall å¯¹æ¯”\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "recall_data = []\n",
    "for name, preds in final_predictions_models.items():\n",
    "    for i, label in enumerate(label_names_models):\n",
    "        mask = final_y_true_models == i\n",
    "        recall = (preds[mask] == i).mean() if mask.sum() > 0 else 0\n",
    "        recall_data.append({'Model': name, 'Class': label, 'Recall': recall})\n",
    "\n",
    "recall_df = pd.DataFrame(recall_data)\n",
    "recall_pivot = recall_df.pivot(index='Model', columns='Class', values='Recall')\n",
    "print(recall_pivot.round(4).to_string())\n",
    "\n",
    "# å¯è§†åŒ–å„ç±»åˆ« Recall\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "models_list = list(final_predictions_models.keys())\n",
    "x = np.arange(len(models_list))\n",
    "width = 0.25\n",
    "\n",
    "for i, label in enumerate(label_names_models):\n",
    "    recalls = [recall_pivot.loc[m, label] for m in models_list]\n",
    "    ax.bar(x + i*width, recalls, width, label=label)\n",
    "\n",
    "ax.set_ylabel('Recall')\n",
    "ax.set_title('å„æ¨¡å‹åœ¨ä¸åŒç±»åˆ«ä¸Šçš„ Recall')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(models_list)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… å¤šæ¨¡å‹å¯¹æ¯” & Stacking é›†æˆå®Œæˆ!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ€»ç»“\n",
    "\n",
    "## æ ‡æ³¨æµç¨‹\n",
    "\n",
    "1. **æ•°æ®é¢„å¤„ç†** (Cell 2-3)\n",
    "   - åŠ è½½ 5min Kçº¿æ•°æ®\n",
    "   - è®¡ç®—å¯¹æ•°æ”¶ç›Šã€ATRã€æ»šåŠ¨é«˜ä½åŒºé—´\n",
    "\n",
    "2. **è¾…åŠ©ç‰¹å¾è®¡ç®—** (Cell 4-5)\n",
    "   - KAMA åŠå…¶æ•ˆç‡å› å­ ER\n",
    "   - KAMA æ–œç‡æ ‡å‡†åŒ–ã€ä»·æ ¼åç¦»åº¦æ ‡å‡†åŒ–\n",
    "   - æ»šåŠ¨çº¿æ€§å›å½’ (æ–œç‡ + RÂ²)\n",
    "   - éœ‡è¡åº¦ (Choppiness)\n",
    "\n",
    "3. **Triple Barrier æ–¹å‘æ ‡ç­¾** (Cell 6)\n",
    "   - åŸºäº ATR è‡ªé€‚åº”çš„ä¸‰é‡éšœç¢\n",
    "   - åˆ¤æ–­å…ˆè§¦åŠä¸Šè½¨/ä¸‹è½¨/æ—¶é—´æˆªæ­¢\n",
    "\n",
    "4. **å¸‚åœºå‘¨æœŸæ ‡ç­¾** (Cell 7-9)\n",
    "   - Step 1: è¯†åˆ«éœ‡è¡/äº¤æ˜“åŒºé—´ (æ»¡è¶³ ERä½ + chopé«˜ + åŒºé—´çª„ + æœªæ¥ç§»åŠ¨å°)\n",
    "   - Step 2: è¯†åˆ«è¶‹åŠ¿ (æ–¹å‘ä¸€è‡´ + æ–œç‡å¼º + è¶‹åŠ¿è´¨é‡ + barrierç¡®è®¤)\n",
    "   - Step 3: æ ‡ç­¾å¹³æ»‘ (åˆ é™¤çŸ­è¶‹åŠ¿æ®µ + å¤šæ•°æŠ•ç¥¨)\n",
    "\n",
    "5. **å¯è§†åŒ–** (Cell 10-13)\n",
    "   - å•æ—¥ Kçº¿å›¾ + Regime èƒŒæ™¯\n",
    "   - è¿ç»­å¤šæ—¥ Kçº¿å›¾ (æ— ä¼‘ç›˜ç©ºéš™)\n",
    "   - è‡ªå®šä¹‰æ—¥æœŸæŸ¥çœ‹\n",
    "\n",
    "6. **æ•°æ®å¯¼å‡º** (Cell 14-15)\n",
    "   - å…¨é‡æ ‡æ³¨æ•°æ®\n",
    "   - é«˜ç½®ä¿¡åº¦æ ·æœ¬ç­›é€‰\n",
    "\n",
    "## è¾“å‡ºæ–‡ä»¶\n",
    "\n",
    "- `market_cycle_labeled_data.csv/parquet` - å…¨é‡æ ‡æ³¨æ•°æ®\n",
    "- `market_cycle_high_confidence.csv/parquet` - é«˜ç½®ä¿¡åº¦æ ·æœ¬\n",
    "- `market_cycle_random_days.html` - éšæœºå•æ—¥å¯è§†åŒ–\n",
    "- `market_cycle_multiday.html` - è¿ç»­å¤šæ—¥å¯è§†åŒ–\n",
    "- `market_cycle_custom_dates.html` - è‡ªå®šä¹‰æ—¥æœŸå¯è§†åŒ–\n",
    "\n",
    "## å‚æ•°è°ƒä¼˜å»ºè®®\n",
    "\n",
    "æ ¹æ®å®é™…æ•ˆæœå¯è°ƒæ•´ä»¥ä¸‹å‚æ•°:\n",
    "- `THR_ER_LOW/HIGH`: ER éœ‡è¡/è¶‹åŠ¿é˜ˆå€¼\n",
    "- `THR_CHOP_HIGH/LOW`: éœ‡è¡åº¦é˜ˆå€¼\n",
    "- `PT_MULT/SL_MULT`: Triple barrier å€æ•°\n",
    "- `MIN_TREND_LEN/MIN_RANGE_LEN`: æœ€å°æ®µé•¿åº¦\n",
    "- `SMOOTH_WINDOW`: å¹³æ»‘çª—å£\n",
    "\n",
    "## åç»­å»ºæ¨¡\n",
    "\n",
    "æ ‡æ³¨å¥½çš„æ•°æ®å¯ç”¨äºè®­ç»ƒ:\n",
    "- Transformer (FT-Transformer, TabTransformer)\n",
    "- LSTM / GRU\n",
    "- XGBoost / LightGBM / CatBoost\n",
    "- æˆ–ç»„åˆæ¨¡å‹\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
